{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch_CNN_ben-mal.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpnQSJRhkgpg",
        "colab_type": "text"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EMVVj99jqJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UmdzgPpj_yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the dataset from Google Drive to local\n",
        "\n",
        "!cp \"/content/gdrive/My Drive/CBIS_DDSM.zip\" .\n",
        "!unzip -qq CBIS_DDSM.zip\n",
        "!rm CBIS_DDSM.zip\n",
        "cbis_path = 'CBIS_DDSM'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO0kNtfWkY3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-r1HGS2kk2v",
        "colab_type": "text"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYzNC3cokcJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_training():\n",
        "    \"\"\"\n",
        "    Load the training set (excluding baseline patches)\n",
        "    \"\"\"\n",
        "    images = np.load(os.path.join(cbis_path, 'numpy data', 'train_tensor.npy'))[1::2]\n",
        "    labels = np.load(os.path.join(cbis_path, 'numpy data', 'train_labels.npy'))[1::2]\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def load_testing():\n",
        "    \"\"\"\n",
        "    Load the test set (abnormalities patches and labels, no baseline)\n",
        "    \"\"\"\n",
        "    images = np.load(os.path.join(cbis_path, 'numpy data', 'public_test_tensor.npy'))[1::2]\n",
        "    labels = np.load(os.path.join(cbis_path, 'numpy data', 'public_test_labels.npy'))[1::2]\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def remap_label(l):\n",
        "    \"\"\"\n",
        "    Remap the labels to 0->benign 1->malignant\n",
        "    \"\"\"\n",
        "    if l == 1 or l == 3:\n",
        "        return 0\n",
        "    elif l == 2 or l == 4:\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"[WARN] Unrecognized label (%d)\" % l)\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOiZAi64k3YR",
        "colab_type": "text"
      },
      "source": [
        "The data is prepared following these steps:\n",
        "\n",
        "\n",
        "1.   Import the training and testing data from numpy arrays\n",
        "2.   Remove the images and labels related to baseline patches (even indices in the arrays)\n",
        "3.   Adjust the labels for the binary classification problem, so that 0 corresponds to 'benign' and 1 maps to 'malignant'\n",
        "4.   Normalize the pixels to be in the range (0-1) floating point\n",
        "5.   Shuffle the training set (and labels accordingly, of course)\n",
        "6.   Split the training data into 'training' and 'validation' subsets\n",
        "7.   Build Keras generators for training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijUiOZtyk6Bf",
        "colab_type": "code",
        "outputId": "45590be1-8851-4bd2-981c-c73826c16325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load training and test images (abnormalities only, no baseline)\n",
        "train_images, train_labels= load_training()\n",
        "test_images, test_labels= load_testing()\n",
        "\n",
        "# Number of images\n",
        "n_train_img = train_images.shape[0]\n",
        "n_test_img = test_images.shape[0]\n",
        "print(\"Train size: %d \\t Test size: %d\" % (n_train_img, n_test_img))\n",
        "\n",
        "# Compute width and height of images\n",
        "img_w = train_images.shape[1]\n",
        "img_h = train_images.shape[2]\n",
        "print(\"Image size: %dx%d\" % (img_w, img_h))\n",
        "\n",
        "# Remap labels\n",
        "train_labels = np.array([remap_label(l) for l in train_labels])\n",
        "test_labels = np.array([remap_label(l) for l in test_labels])\n",
        "\n",
        "# Create a new dimension for color in the images arrays\n",
        "train_images = train_images.reshape((n_train_img, img_w, img_h, 1))\n",
        "test_images = test_images.reshape((n_test_img, img_w, img_h, 1))\n",
        "\n",
        "# Convert from 16-bit (0-65535) to float (0-1)\n",
        "train_images = train_images.astype('uint16') / 65535\n",
        "test_images = test_images.astype('uint16') / 65535\n",
        "\n",
        "# Shuffle the training set (originally sorted by label)\n",
        "perm = np.random.permutation(n_train_img)\n",
        "train_images = train_images[perm]\n",
        "train_labels = train_labels[perm]\n",
        "\n",
        "# Create a generator for training images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    validation_split=0.2,\n",
        "    rotation_range=180,\n",
        "    shear_range=10,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='reflect'\n",
        ")\n",
        "\n",
        "# Fit the generator with some images\n",
        "train_datagen.fit(train_images)\n",
        "\n",
        "# Split train images into actual training and validation\n",
        "train_generator = train_datagen.flow(train_images, train_labels, batch_size=128, subset='training')\n",
        "validation_generator = train_datagen.flow(train_images, train_labels, batch_size=128, subset='validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 2676 \t Test size: 336\n",
            "Image size: 150x150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1hVOhGRm9QT",
        "colab_type": "code",
        "outputId": "df8333f4-0da3-47e1-9ac8-0e6c66981be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Visualize one image from the dataset and its label, just to make sure the data format is correct\n",
        "\n",
        "idx = 0\n",
        "\n",
        "plt.imshow(train_images[idx][:,:,0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(\"Label: \" + str(train_labels[idx]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN9z+L+FQ1gWtmeUCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWeraE25EJJ",
        "colab_type": "text"
      },
      "source": [
        "## Run 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv4EVXkRnaf5",
        "colab_type": "code",
        "outputId": "689b2268-20ae-415f-c224-603baa7c7164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# Model 1\n",
        "\n",
        "model_1 = models.Sequential()\n",
        "model_1.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
        "model_1.add(layers.MaxPooling2D((2, 2)))\n",
        "model_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_1.add(layers.MaxPooling2D((2, 2)))\n",
        "model_1.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model_1.add(layers.MaxPooling2D((2, 2)))\n",
        "model_1.add(layers.Flatten())\n",
        "model_1.add(layers.Dense(64, activation='relu'))\n",
        "model_1.add(layers.Dropout(0.5))\n",
        "model_1.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_1.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 73984)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4735040   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,104,769\n",
            "Trainable params: 5,104,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg_ymphMnlOZ",
        "colab_type": "code",
        "outputId": "6a8db1ab-7e27-42a3-d957-47dcf2c3a89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Early stopping (stop training after the validation loss reaches the minimum)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=80, verbose=1)\n",
        "\n",
        "# Callback for checkpointing\n",
        "checkpoint = ModelCheckpoint('model_1_benmal_best.h5', \n",
        "        monitor='val_loss', mode='min', verbose=1, \n",
        "        save_best_only=True, save_freq='epoch'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(optimizer=RMSprop(learning_rate=0.001, decay=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history_1 = model_1.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=int(0.8*n_train_img) // 128,\n",
        "        epochs=500,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint],\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        initial_epoch=0\n",
        ")\n",
        "\n",
        "# Save\n",
        "models.save_model(model_1, 'model_1_benmal_end.h5')\n",
        "!cp model* \"/content/gdrive/My Drive/models/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.2233 - acc: 0.5607Epoch 1/500\n",
            " 5/16 [========>.....................] - ETA: 2s - loss: 0.6958 - acc: 0.4449\n",
            "Epoch 00001: val_loss improved from inf to 0.69575, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 15s 966ms/step - loss: 1.1912 - acc: 0.5544 - val_loss: 0.6958 - val_acc: 0.4449\n",
            "Epoch 2/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6883 - acc: 0.5883Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6817 - acc: 0.5645\n",
            "Epoch 00002: val_loss improved from 0.69575 to 0.68203, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6879 - acc: 0.5877 - val_loss: 0.6820 - val_acc: 0.5664\n",
            "Epoch 3/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6920 - acc: 0.5899Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6842 - acc: 0.5625\n",
            "Epoch 00003: val_loss improved from 0.68203 to 0.68104, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6927 - acc: 0.5902 - val_loss: 0.6810 - val_acc: 0.5664\n",
            "Epoch 4/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6831 - acc: 0.5782Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6797 - acc: 0.5625\n",
            "Epoch 00004: val_loss improved from 0.68104 to 0.67785, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6834 - acc: 0.5772 - val_loss: 0.6778 - val_acc: 0.5664\n",
            "Epoch 5/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6849 - acc: 0.6069Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6868 - acc: 0.5645\n",
            "Epoch 00005: val_loss did not improve from 0.67785\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6935 - acc: 0.5976 - val_loss: 0.6855 - val_acc: 0.5664\n",
            "Epoch 6/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6830 - acc: 0.5724Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6817 - acc: 0.5645\n",
            "Epoch 00006: val_loss did not improve from 0.67785\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6815 - acc: 0.5767 - val_loss: 0.6785 - val_acc: 0.5664\n",
            "Epoch 7/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6797 - acc: 0.5876Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6801 - acc: 0.5645\n",
            "Epoch 00007: val_loss did not improve from 0.67785\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6799 - acc: 0.5895 - val_loss: 0.6792 - val_acc: 0.5664\n",
            "Epoch 8/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6764 - acc: 0.5849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6803 - acc: 0.6406\n",
            "Epoch 00008: val_loss did not improve from 0.67785\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6766 - acc: 0.5869 - val_loss: 0.6793 - val_acc: 0.6411\n",
            "Epoch 9/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6768 - acc: 0.5788Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6811 - acc: 0.5820\n",
            "Epoch 00009: val_loss did not improve from 0.67785\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6774 - acc: 0.5789 - val_loss: 0.6813 - val_acc: 0.5832\n",
            "Epoch 10/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6664 - acc: 0.5846Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6699 - acc: 0.6387\n",
            "Epoch 00010: val_loss improved from 0.67785 to 0.66938, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6676 - acc: 0.5797 - val_loss: 0.6694 - val_acc: 0.6393\n",
            "Epoch 11/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6634 - acc: 0.6053Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6629 - acc: 0.6406\n",
            "Epoch 00011: val_loss improved from 0.66938 to 0.66387, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6636 - acc: 0.6041 - val_loss: 0.6639 - val_acc: 0.6393\n",
            "Epoch 12/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6697 - acc: 0.6010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6740 - acc: 0.5664\n",
            "Epoch 00012: val_loss did not improve from 0.66387\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6669 - acc: 0.6035 - val_loss: 0.6754 - val_acc: 0.5664\n",
            "Epoch 13/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6564 - acc: 0.6154Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6488 - acc: 0.6484\n",
            "Epoch 00013: val_loss improved from 0.66387 to 0.64683, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6564 - acc: 0.6135 - val_loss: 0.6468 - val_acc: 0.6505\n",
            "Epoch 14/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6544 - acc: 0.6212Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6475 - acc: 0.6328\n",
            "Epoch 00014: val_loss improved from 0.64683 to 0.64613, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6531 - acc: 0.6215 - val_loss: 0.6461 - val_acc: 0.6318\n",
            "Epoch 15/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6649 - acc: 0.6106Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6494 - acc: 0.6406\n",
            "Epoch 00015: val_loss did not improve from 0.64613\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.6650 - acc: 0.6095 - val_loss: 0.6522 - val_acc: 0.6393\n",
            "Epoch 16/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6624 - acc: 0.6175Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6416 - acc: 0.6074\n",
            "Epoch 00016: val_loss did not improve from 0.64613\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 0.6600 - acc: 0.6170 - val_loss: 0.6540 - val_acc: 0.6056\n",
            "Epoch 17/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6503 - acc: 0.6260Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6355 - acc: 0.6543\n",
            "Epoch 00017: val_loss improved from 0.64613 to 0.64506, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 5s 342ms/step - loss: 0.6504 - acc: 0.6244 - val_loss: 0.6451 - val_acc: 0.6486\n",
            "Epoch 18/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6450 - acc: 0.6198Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6395 - acc: 0.6348\n",
            "Epoch 00018: val_loss did not improve from 0.64506\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.6469 - acc: 0.6165 - val_loss: 0.6468 - val_acc: 0.6280\n",
            "Epoch 19/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6457 - acc: 0.6191Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6211 - acc: 0.6660\n",
            "Epoch 00019: val_loss improved from 0.64506 to 0.63273, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.6443 - acc: 0.6195 - val_loss: 0.6327 - val_acc: 0.6598\n",
            "Epoch 20/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6559 - acc: 0.6339Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6515 - acc: 0.5840\n",
            "Epoch 00020: val_loss did not improve from 0.63273\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6546 - acc: 0.6354 - val_loss: 0.6425 - val_acc: 0.5869\n",
            "Epoch 21/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6349 - acc: 0.6387Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6230 - acc: 0.6309\n",
            "Epoch 00021: val_loss improved from 0.63273 to 0.60960, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6353 - acc: 0.6369 - val_loss: 0.6096 - val_acc: 0.6355\n",
            "Epoch 22/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6259 - acc: 0.6334Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6438 - acc: 0.6230\n",
            "Epoch 00022: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.6223 - acc: 0.6374 - val_loss: 0.6588 - val_acc: 0.6187\n",
            "Epoch 23/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6436 - acc: 0.6156Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6205 - acc: 0.6387\n",
            "Epoch 00023: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6434 - acc: 0.6177 - val_loss: 0.6356 - val_acc: 0.6355\n",
            "Epoch 24/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6338 - acc: 0.6416Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6114 - acc: 0.6289\n",
            "Epoch 00024: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6365 - acc: 0.6395 - val_loss: 0.6257 - val_acc: 0.6299\n",
            "Epoch 25/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6260 - acc: 0.6297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6301 - acc: 0.6270\n",
            "Epoch 00025: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6248 - acc: 0.6353 - val_loss: 0.6388 - val_acc: 0.6262\n",
            "Epoch 26/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6291 - acc: 0.6271Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6337 - acc: 0.6250\n",
            "Epoch 00026: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6277 - acc: 0.6269 - val_loss: 0.6393 - val_acc: 0.6262\n",
            "Epoch 27/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6281 - acc: 0.6389Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6088 - acc: 0.6387\n",
            "Epoch 00027: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6274 - acc: 0.6395 - val_loss: 0.6302 - val_acc: 0.6393\n",
            "Epoch 28/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6435 - acc: 0.6245Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6128 - acc: 0.6328\n",
            "Epoch 00028: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6412 - acc: 0.6294 - val_loss: 0.6256 - val_acc: 0.6280\n",
            "Epoch 29/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6227 - acc: 0.6238Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6010 - acc: 0.6445\n",
            "Epoch 00029: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.6193 - acc: 0.6299 - val_loss: 0.6231 - val_acc: 0.6430\n",
            "Epoch 30/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6299 - acc: 0.6255Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6132 - acc: 0.6387\n",
            "Epoch 00030: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.6280 - acc: 0.6274 - val_loss: 0.6266 - val_acc: 0.6374\n",
            "Epoch 31/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6303 - acc: 0.6141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6083 - acc: 0.6484\n",
            "Epoch 00031: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 344ms/step - loss: 0.6313 - acc: 0.6127 - val_loss: 0.6181 - val_acc: 0.6449\n",
            "Epoch 32/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6287 - acc: 0.6313Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6246 - acc: 0.5957\n",
            "Epoch 00032: val_loss did not improve from 0.60960\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6305 - acc: 0.6304 - val_loss: 0.6434 - val_acc: 0.5944\n",
            "Epoch 33/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6203 - acc: 0.6377Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6072 - acc: 0.6348\n",
            "Epoch 00033: val_loss improved from 0.60960 to 0.60574, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 345ms/step - loss: 0.6199 - acc: 0.6390 - val_loss: 0.6057 - val_acc: 0.6355\n",
            "Epoch 34/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6194 - acc: 0.6234Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6165 - acc: 0.6348\n",
            "Epoch 00034: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.6203 - acc: 0.6240 - val_loss: 0.6310 - val_acc: 0.6280\n",
            "Epoch 35/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6249 - acc: 0.6228Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6154 - acc: 0.6328\n",
            "Epoch 00035: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.6252 - acc: 0.6244 - val_loss: 0.6226 - val_acc: 0.6299\n",
            "Epoch 36/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6302 - acc: 0.6334Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6028 - acc: 0.6387\n",
            "Epoch 00036: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6273 - acc: 0.6359 - val_loss: 0.6114 - val_acc: 0.6355\n",
            "Epoch 37/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6192 - acc: 0.6467Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6086 - acc: 0.6445\n",
            "Epoch 00037: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.6225 - acc: 0.6408 - val_loss: 0.6186 - val_acc: 0.6393\n",
            "Epoch 38/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6271 - acc: 0.6345Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6198 - acc: 0.6406\n",
            "Epoch 00038: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6265 - acc: 0.6334 - val_loss: 0.6338 - val_acc: 0.6374\n",
            "Epoch 39/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6211 - acc: 0.6605Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6089 - acc: 0.6504\n",
            "Epoch 00039: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6216 - acc: 0.6542 - val_loss: 0.6189 - val_acc: 0.6449\n",
            "Epoch 40/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6221 - acc: 0.6344Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6058 - acc: 0.6484\n",
            "Epoch 00040: val_loss did not improve from 0.60574\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6218 - acc: 0.6328 - val_loss: 0.6099 - val_acc: 0.6449\n",
            "Epoch 41/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6141 - acc: 0.6519Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5926 - acc: 0.6543\n",
            "Epoch 00041: val_loss improved from 0.60574 to 0.60517, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6137 - acc: 0.6537 - val_loss: 0.6052 - val_acc: 0.6486\n",
            "Epoch 42/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6232 - acc: 0.6344Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5928 - acc: 0.6582\n",
            "Epoch 00042: val_loss improved from 0.60517 to 0.59954, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6213 - acc: 0.6377 - val_loss: 0.5995 - val_acc: 0.6561\n",
            "Epoch 43/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6176 - acc: 0.6504Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5929 - acc: 0.6543\n",
            "Epoch 00043: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.6169 - acc: 0.6493 - val_loss: 0.6059 - val_acc: 0.6486\n",
            "Epoch 44/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6176 - acc: 0.6440Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6155 - acc: 0.6230\n",
            "Epoch 00044: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.6186 - acc: 0.6421 - val_loss: 0.6201 - val_acc: 0.6243\n",
            "Epoch 45/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6109 - acc: 0.6521Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6053 - acc: 0.6328\n",
            "Epoch 00045: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6125 - acc: 0.6475 - val_loss: 0.6138 - val_acc: 0.6299\n",
            "Epoch 46/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6157 - acc: 0.6361Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5997 - acc: 0.6445\n",
            "Epoch 00046: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6146 - acc: 0.6374 - val_loss: 0.6145 - val_acc: 0.6393\n",
            "Epoch 47/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6032 - acc: 0.6508Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6109 - acc: 0.6504\n",
            "Epoch 00047: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6025 - acc: 0.6507 - val_loss: 0.6204 - val_acc: 0.6467\n",
            "Epoch 48/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6150 - acc: 0.6292Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5979 - acc: 0.6465\n",
            "Epoch 00048: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.6127 - acc: 0.6279 - val_loss: 0.6162 - val_acc: 0.6393\n",
            "Epoch 49/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6101 - acc: 0.6497Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6112 - acc: 0.6387\n",
            "Epoch 00049: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.6092 - acc: 0.6502 - val_loss: 0.6210 - val_acc: 0.6355\n",
            "Epoch 50/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6070 - acc: 0.6562Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5995 - acc: 0.6523\n",
            "Epoch 00050: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.6084 - acc: 0.6538 - val_loss: 0.6056 - val_acc: 0.6505\n",
            "Epoch 51/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6152 - acc: 0.6515Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6086 - acc: 0.6504\n",
            "Epoch 00051: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 5s 344ms/step - loss: 0.6143 - acc: 0.6488 - val_loss: 0.6128 - val_acc: 0.6449\n",
            "Epoch 52/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6132 - acc: 0.6398Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6089 - acc: 0.6504\n",
            "Epoch 00052: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.6108 - acc: 0.6413 - val_loss: 0.6175 - val_acc: 0.6449\n",
            "Epoch 53/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6011 - acc: 0.6403Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6062 - acc: 0.6426\n",
            "Epoch 00053: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 5s 342ms/step - loss: 0.6006 - acc: 0.6413 - val_loss: 0.6189 - val_acc: 0.6411\n",
            "Epoch 54/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6138 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6166 - acc: 0.6309\n",
            "Epoch 00054: val_loss did not improve from 0.59954\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6154 - acc: 0.6523 - val_loss: 0.6312 - val_acc: 0.6280\n",
            "Epoch 55/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6000 - acc: 0.6465Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6015 - acc: 0.6562\n",
            "Epoch 00055: val_loss improved from 0.59954 to 0.59755, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6000 - acc: 0.6486 - val_loss: 0.5976 - val_acc: 0.6523\n",
            "Epoch 56/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6060 - acc: 0.6370Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5959 - acc: 0.6484\n",
            "Epoch 00056: val_loss improved from 0.59755 to 0.59287, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.6072 - acc: 0.6357 - val_loss: 0.5929 - val_acc: 0.6505\n",
            "Epoch 57/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6009 - acc: 0.6546Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6103 - acc: 0.6309\n",
            "Epoch 00057: val_loss did not improve from 0.59287\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6015 - acc: 0.6522 - val_loss: 0.6142 - val_acc: 0.6299\n",
            "Epoch 58/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6154 - acc: 0.6520Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5968 - acc: 0.6484\n",
            "Epoch 00058: val_loss did not improve from 0.59287\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6151 - acc: 0.6503 - val_loss: 0.5974 - val_acc: 0.6430\n",
            "Epoch 59/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6082 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6109 - acc: 0.6426\n",
            "Epoch 00059: val_loss did not improve from 0.59287\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6081 - acc: 0.6602 - val_loss: 0.6041 - val_acc: 0.6393\n",
            "Epoch 60/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6072 - acc: 0.6422Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5963 - acc: 0.6582\n",
            "Epoch 00060: val_loss improved from 0.59287 to 0.59101, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6085 - acc: 0.6436 - val_loss: 0.5910 - val_acc: 0.6561\n",
            "Epoch 61/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5987 - acc: 0.6578Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6039 - acc: 0.6660\n",
            "Epoch 00061: val_loss did not improve from 0.59101\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.6005 - acc: 0.6552 - val_loss: 0.6015 - val_acc: 0.6617\n",
            "Epoch 62/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6111 - acc: 0.6422Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5978 - acc: 0.6426\n",
            "Epoch 00062: val_loss did not improve from 0.59101\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6098 - acc: 0.6445 - val_loss: 0.5930 - val_acc: 0.6393\n",
            "Epoch 63/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6121 - acc: 0.6509Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5926 - acc: 0.6602\n",
            "Epoch 00063: val_loss improved from 0.59101 to 0.58752, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6133 - acc: 0.6523 - val_loss: 0.5875 - val_acc: 0.6598\n",
            "Epoch 64/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6156 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6006 - acc: 0.6523\n",
            "Epoch 00064: val_loss did not improve from 0.58752\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6129 - acc: 0.6587 - val_loss: 0.5979 - val_acc: 0.6542\n",
            "Epoch 65/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6129 - acc: 0.6313Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5927 - acc: 0.6484\n",
            "Epoch 00065: val_loss did not improve from 0.58752\n",
            "16/16 [==============================] - 5s 343ms/step - loss: 0.6100 - acc: 0.6345 - val_loss: 0.5906 - val_acc: 0.6449\n",
            "Epoch 66/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6104 - acc: 0.6583Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5980 - acc: 0.6406\n",
            "Epoch 00066: val_loss did not improve from 0.58752\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.6086 - acc: 0.6606 - val_loss: 0.5897 - val_acc: 0.6449\n",
            "Epoch 67/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5998 - acc: 0.6536Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6115 - acc: 0.6426\n",
            "Epoch 00067: val_loss did not improve from 0.58752\n",
            "16/16 [==============================] - 5s 342ms/step - loss: 0.6013 - acc: 0.6528 - val_loss: 0.6062 - val_acc: 0.6430\n",
            "Epoch 68/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6062 - acc: 0.6589Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5945 - acc: 0.6348\n",
            "Epoch 00068: val_loss improved from 0.58752 to 0.58712, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 350ms/step - loss: 0.6043 - acc: 0.6607 - val_loss: 0.5871 - val_acc: 0.6355\n",
            "Epoch 69/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6014 - acc: 0.6552Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5989 - acc: 0.6543\n",
            "Epoch 00069: val_loss did not improve from 0.58712\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.6032 - acc: 0.6533 - val_loss: 0.5920 - val_acc: 0.6561\n",
            "Epoch 70/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6099 - acc: 0.6492Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6053 - acc: 0.6621\n",
            "Epoch 00070: val_loss did not improve from 0.58712\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 0.6098 - acc: 0.6502 - val_loss: 0.5931 - val_acc: 0.6636\n",
            "Epoch 71/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6035 - acc: 0.6589Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5941 - acc: 0.6562\n",
            "Epoch 00071: val_loss improved from 0.58712 to 0.58579, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6030 - acc: 0.6582 - val_loss: 0.5858 - val_acc: 0.6561\n",
            "Epoch 72/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6137 - acc: 0.6377Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6147 - acc: 0.6543\n",
            "Epoch 00072: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6150 - acc: 0.6349 - val_loss: 0.6044 - val_acc: 0.6561\n",
            "Epoch 73/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6043 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5943 - acc: 0.6504\n",
            "Epoch 00073: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6028 - acc: 0.6637 - val_loss: 0.5871 - val_acc: 0.6486\n",
            "Epoch 74/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6159 - acc: 0.6462Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6011 - acc: 0.6523\n",
            "Epoch 00074: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6174 - acc: 0.6448 - val_loss: 0.5981 - val_acc: 0.6542\n",
            "Epoch 75/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6009 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5947 - acc: 0.6504\n",
            "Epoch 00075: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5973 - acc: 0.6650 - val_loss: 0.5873 - val_acc: 0.6486\n",
            "Epoch 76/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6074 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5985 - acc: 0.6602\n",
            "Epoch 00076: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6043 - acc: 0.6572 - val_loss: 0.5927 - val_acc: 0.6617\n",
            "Epoch 77/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6049 - acc: 0.6462Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6060 - acc: 0.6523\n",
            "Epoch 00077: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6056 - acc: 0.6413 - val_loss: 0.5982 - val_acc: 0.6579\n",
            "Epoch 78/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5975 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6003 - acc: 0.6582\n",
            "Epoch 00078: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5966 - acc: 0.6557 - val_loss: 0.6219 - val_acc: 0.6561\n",
            "Epoch 79/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6103 - acc: 0.6456Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5816 - acc: 0.6582\n",
            "Epoch 00079: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6099 - acc: 0.6448 - val_loss: 0.5977 - val_acc: 0.6561\n",
            "Epoch 80/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6151 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5889 - acc: 0.6445\n",
            "Epoch 00080: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6125 - acc: 0.6577 - val_loss: 0.6131 - val_acc: 0.6430\n",
            "Epoch 81/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6005 - acc: 0.6552Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6059 - acc: 0.6426\n",
            "Epoch 00081: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.6014 - acc: 0.6557 - val_loss: 0.5982 - val_acc: 0.6430\n",
            "Epoch 82/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6061 - acc: 0.6472Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5892 - acc: 0.6543\n",
            "Epoch 00082: val_loss did not improve from 0.58579\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6041 - acc: 0.6528 - val_loss: 0.5894 - val_acc: 0.6542\n",
            "Epoch 83/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6002 - acc: 0.6573Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5964 - acc: 0.6504\n",
            "Epoch 00083: val_loss improved from 0.58579 to 0.57044, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5963 - acc: 0.6602 - val_loss: 0.5704 - val_acc: 0.6579\n",
            "Epoch 84/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6054 - acc: 0.6477Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6132 - acc: 0.6270\n",
            "Epoch 00084: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.6070 - acc: 0.6453 - val_loss: 0.5960 - val_acc: 0.6318\n",
            "Epoch 85/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5999 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5938 - acc: 0.6445\n",
            "Epoch 00085: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 345ms/step - loss: 0.5985 - acc: 0.6582 - val_loss: 0.5834 - val_acc: 0.6449\n",
            "Epoch 86/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5881 - acc: 0.6621Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6040 - acc: 0.6406\n",
            "Epoch 00086: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 0.5953 - acc: 0.6617 - val_loss: 0.5956 - val_acc: 0.6430\n",
            "Epoch 87/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6022 - acc: 0.6626Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6018 - acc: 0.6562\n",
            "Epoch 00087: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.6014 - acc: 0.6607 - val_loss: 0.5904 - val_acc: 0.6561\n",
            "Epoch 88/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6115 - acc: 0.6387Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5978 - acc: 0.6758\n",
            "Epoch 00088: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6087 - acc: 0.6403 - val_loss: 0.5872 - val_acc: 0.6748\n",
            "Epoch 89/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5813 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5916 - acc: 0.6602\n",
            "Epoch 00089: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5835 - acc: 0.6821 - val_loss: 0.5855 - val_acc: 0.6617\n",
            "Epoch 90/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6042 - acc: 0.6281Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5928 - acc: 0.6543\n",
            "Epoch 00090: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6044 - acc: 0.6269 - val_loss: 0.5965 - val_acc: 0.6505\n",
            "Epoch 91/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5967 - acc: 0.6610Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5917 - acc: 0.6465\n",
            "Epoch 00091: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5965 - acc: 0.6612 - val_loss: 0.5854 - val_acc: 0.6542\n",
            "Epoch 92/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6045 - acc: 0.6493Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5856 - acc: 0.6543\n",
            "Epoch 00092: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6031 - acc: 0.6508 - val_loss: 0.5847 - val_acc: 0.6561\n",
            "Epoch 93/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6042 - acc: 0.6552Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5963 - acc: 0.6504\n",
            "Epoch 00093: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6020 - acc: 0.6572 - val_loss: 0.5978 - val_acc: 0.6505\n",
            "Epoch 94/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5935 - acc: 0.6546Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5927 - acc: 0.6621\n",
            "Epoch 00094: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5927 - acc: 0.6557 - val_loss: 0.5924 - val_acc: 0.6636\n",
            "Epoch 95/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6079 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5876 - acc: 0.6875\n",
            "Epoch 00095: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6073 - acc: 0.6602 - val_loss: 0.6018 - val_acc: 0.6804\n",
            "Epoch 96/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5970 - acc: 0.6464Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5913 - acc: 0.6562\n",
            "Epoch 00096: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5958 - acc: 0.6489 - val_loss: 0.6035 - val_acc: 0.6542\n",
            "Epoch 97/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5955 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5991 - acc: 0.6328\n",
            "Epoch 00097: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5985 - acc: 0.6592 - val_loss: 0.6000 - val_acc: 0.6299\n",
            "Epoch 98/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5988 - acc: 0.6483Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5939 - acc: 0.6582\n",
            "Epoch 00098: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5973 - acc: 0.6448 - val_loss: 0.5865 - val_acc: 0.6579\n",
            "Epoch 99/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6000 - acc: 0.6589Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6052 - acc: 0.6484\n",
            "Epoch 00099: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5995 - acc: 0.6577 - val_loss: 0.5989 - val_acc: 0.6486\n",
            "Epoch 100/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5978 - acc: 0.6499Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5927 - acc: 0.6660\n",
            "Epoch 00100: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5955 - acc: 0.6523 - val_loss: 0.5863 - val_acc: 0.6654\n",
            "Epoch 101/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6031 - acc: 0.6562Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5947 - acc: 0.6562\n",
            "Epoch 00101: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.6036 - acc: 0.6602 - val_loss: 0.6031 - val_acc: 0.6523\n",
            "Epoch 102/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5874 - acc: 0.6668Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6007 - acc: 0.6367\n",
            "Epoch 00102: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 0.5884 - acc: 0.6637 - val_loss: 0.6028 - val_acc: 0.6374\n",
            "Epoch 103/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6005 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5993 - acc: 0.6562\n",
            "Epoch 00103: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.6002 - acc: 0.6552 - val_loss: 0.6055 - val_acc: 0.6542\n",
            "Epoch 104/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5909 - acc: 0.6769Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5865 - acc: 0.6582\n",
            "Epoch 00104: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5934 - acc: 0.6746 - val_loss: 0.6019 - val_acc: 0.6505\n",
            "Epoch 105/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6000 - acc: 0.6477Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6115 - acc: 0.6523\n",
            "Epoch 00105: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5990 - acc: 0.6503 - val_loss: 0.6156 - val_acc: 0.6505\n",
            "Epoch 106/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5934 - acc: 0.6604Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5973 - acc: 0.6719\n",
            "Epoch 00106: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5983 - acc: 0.6572 - val_loss: 0.6020 - val_acc: 0.6710\n",
            "Epoch 107/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5897 - acc: 0.6727Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5944 - acc: 0.6504\n",
            "Epoch 00107: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5919 - acc: 0.6689 - val_loss: 0.6108 - val_acc: 0.6505\n",
            "Epoch 108/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6048 - acc: 0.6653Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5899 - acc: 0.6699\n",
            "Epoch 00108: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6032 - acc: 0.6642 - val_loss: 0.6081 - val_acc: 0.6673\n",
            "Epoch 109/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5901 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5931 - acc: 0.6699\n",
            "Epoch 00109: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5906 - acc: 0.6622 - val_loss: 0.6056 - val_acc: 0.6673\n",
            "Epoch 110/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5915 - acc: 0.6547Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5944 - acc: 0.6504\n",
            "Epoch 00110: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5921 - acc: 0.6538 - val_loss: 0.6074 - val_acc: 0.6486\n",
            "Epoch 111/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5979 - acc: 0.6525Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5857 - acc: 0.6543\n",
            "Epoch 00111: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5985 - acc: 0.6547 - val_loss: 0.6008 - val_acc: 0.6523\n",
            "Epoch 112/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5981 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5950 - acc: 0.6543\n",
            "Epoch 00112: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5994 - acc: 0.6657 - val_loss: 0.6112 - val_acc: 0.6542\n",
            "Epoch 113/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5886 - acc: 0.6721Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5888 - acc: 0.6504\n",
            "Epoch 00113: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5893 - acc: 0.6721 - val_loss: 0.5817 - val_acc: 0.6486\n",
            "Epoch 114/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5846 - acc: 0.6719Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5952 - acc: 0.6621\n",
            "Epoch 00114: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5853 - acc: 0.6684 - val_loss: 0.5935 - val_acc: 0.6579\n",
            "Epoch 115/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6005 - acc: 0.6755Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5832 - acc: 0.6719\n",
            "Epoch 00115: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.6026 - acc: 0.6741 - val_loss: 0.5934 - val_acc: 0.6673\n",
            "Epoch 116/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5932 - acc: 0.6635Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5892 - acc: 0.6582\n",
            "Epoch 00116: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5929 - acc: 0.6636 - val_loss: 0.5885 - val_acc: 0.6561\n",
            "Epoch 117/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5938 - acc: 0.6658Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5920 - acc: 0.6621\n",
            "Epoch 00117: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5948 - acc: 0.6667 - val_loss: 0.6033 - val_acc: 0.6561\n",
            "Epoch 118/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5919 - acc: 0.6684Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6168 - acc: 0.5977\n",
            "Epoch 00118: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 345ms/step - loss: 0.5965 - acc: 0.6628 - val_loss: 0.6181 - val_acc: 0.6019\n",
            "Epoch 119/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5936 - acc: 0.6635Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5941 - acc: 0.6523\n",
            "Epoch 00119: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 350ms/step - loss: 0.5944 - acc: 0.6660 - val_loss: 0.5979 - val_acc: 0.6523\n",
            "Epoch 120/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5903 - acc: 0.6653Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5911 - acc: 0.6543\n",
            "Epoch 00120: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.5917 - acc: 0.6622 - val_loss: 0.5901 - val_acc: 0.6561\n",
            "Epoch 121/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5965 - acc: 0.6573Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5812 - acc: 0.6641\n",
            "Epoch 00121: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5955 - acc: 0.6602 - val_loss: 0.5798 - val_acc: 0.6636\n",
            "Epoch 122/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5992 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5907 - acc: 0.6621\n",
            "Epoch 00122: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5983 - acc: 0.6667 - val_loss: 0.5901 - val_acc: 0.6598\n",
            "Epoch 123/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5917 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6048 - acc: 0.6621\n",
            "Epoch 00123: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5929 - acc: 0.6753 - val_loss: 0.6021 - val_acc: 0.6598\n",
            "Epoch 124/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5945 - acc: 0.6643Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5858 - acc: 0.6523\n",
            "Epoch 00124: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5901 - acc: 0.6719 - val_loss: 0.5845 - val_acc: 0.6505\n",
            "Epoch 125/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5858 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5912 - acc: 0.6641\n",
            "Epoch 00125: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5857 - acc: 0.6657 - val_loss: 0.5873 - val_acc: 0.6673\n",
            "Epoch 126/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5843 - acc: 0.6729Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5967 - acc: 0.6484\n",
            "Epoch 00126: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5848 - acc: 0.6733 - val_loss: 0.5929 - val_acc: 0.6486\n",
            "Epoch 127/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5959 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5965 - acc: 0.6562\n",
            "Epoch 00127: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5935 - acc: 0.6860 - val_loss: 0.5960 - val_acc: 0.6561\n",
            "Epoch 128/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5925 - acc: 0.6621Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5823 - acc: 0.6699\n",
            "Epoch 00128: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5918 - acc: 0.6633 - val_loss: 0.5804 - val_acc: 0.6692\n",
            "Epoch 129/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5822 - acc: 0.6635Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6056 - acc: 0.6543\n",
            "Epoch 00129: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5810 - acc: 0.6665 - val_loss: 0.6070 - val_acc: 0.6579\n",
            "Epoch 130/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5913 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5903 - acc: 0.6543\n",
            "Epoch 00130: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5952 - acc: 0.6701 - val_loss: 0.5906 - val_acc: 0.6561\n",
            "Epoch 131/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5716 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5800 - acc: 0.6602\n",
            "Epoch 00131: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5721 - acc: 0.6826 - val_loss: 0.5852 - val_acc: 0.6579\n",
            "Epoch 132/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5961 - acc: 0.6488Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6014 - acc: 0.6562\n",
            "Epoch 00132: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5963 - acc: 0.6468 - val_loss: 0.5886 - val_acc: 0.6598\n",
            "Epoch 133/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5875 - acc: 0.6647Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5913 - acc: 0.6758\n",
            "Epoch 00133: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5895 - acc: 0.6632 - val_loss: 0.5903 - val_acc: 0.6766\n",
            "Epoch 134/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5781 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5942 - acc: 0.6543\n",
            "Epoch 00134: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5743 - acc: 0.6865 - val_loss: 0.5872 - val_acc: 0.6598\n",
            "Epoch 135/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5939 - acc: 0.6504Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5917 - acc: 0.6562\n",
            "Epoch 00135: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5969 - acc: 0.6491 - val_loss: 0.5886 - val_acc: 0.6654\n",
            "Epoch 136/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5803 - acc: 0.6776Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5926 - acc: 0.6562\n",
            "Epoch 00136: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5800 - acc: 0.6782 - val_loss: 0.5777 - val_acc: 0.6579\n",
            "Epoch 137/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5900 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6001 - acc: 0.6523\n",
            "Epoch 00137: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.5897 - acc: 0.6612 - val_loss: 0.5967 - val_acc: 0.6523\n",
            "Epoch 138/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5911 - acc: 0.6605Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5998 - acc: 0.6680\n",
            "Epoch 00138: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5878 - acc: 0.6687 - val_loss: 0.5952 - val_acc: 0.6673\n",
            "Epoch 139/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5853 - acc: 0.6651Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6004 - acc: 0.6523\n",
            "Epoch 00139: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5869 - acc: 0.6694 - val_loss: 0.6003 - val_acc: 0.6542\n",
            "Epoch 140/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6028 - acc: 0.6486Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5851 - acc: 0.6621\n",
            "Epoch 00140: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5989 - acc: 0.6507 - val_loss: 0.5903 - val_acc: 0.6636\n",
            "Epoch 141/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5791 - acc: 0.6729Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5874 - acc: 0.6602\n",
            "Epoch 00141: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5836 - acc: 0.6719 - val_loss: 0.5938 - val_acc: 0.6579\n",
            "Epoch 142/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5770 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5799 - acc: 0.6680\n",
            "Epoch 00142: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5750 - acc: 0.6880 - val_loss: 0.5901 - val_acc: 0.6710\n",
            "Epoch 143/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5842 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5798 - acc: 0.6719\n",
            "Epoch 00143: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5843 - acc: 0.6716 - val_loss: 0.5771 - val_acc: 0.6710\n",
            "Epoch 144/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5908 - acc: 0.6562Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5878 - acc: 0.6582\n",
            "Epoch 00144: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5934 - acc: 0.6532 - val_loss: 0.5808 - val_acc: 0.6617\n",
            "Epoch 145/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6040 - acc: 0.6465\n",
            "Epoch 00145: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5777 - acc: 0.6761 - val_loss: 0.5959 - val_acc: 0.6505\n",
            "Epoch 146/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5938 - acc: 0.6801Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5792 - acc: 0.6797\n",
            "Epoch 00146: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5966 - acc: 0.6786 - val_loss: 0.5781 - val_acc: 0.6804\n",
            "Epoch 147/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5762 - acc: 0.6674Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5984 - acc: 0.6465\n",
            "Epoch 00147: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5845 - acc: 0.6652 - val_loss: 0.5957 - val_acc: 0.6505\n",
            "Epoch 148/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5936 - acc: 0.6682Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5913 - acc: 0.6621\n",
            "Epoch 00148: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5916 - acc: 0.6680 - val_loss: 0.5837 - val_acc: 0.6636\n",
            "Epoch 149/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5976 - acc: 0.6654Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5725 - acc: 0.6797\n",
            "Epoch 00149: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5971 - acc: 0.6633 - val_loss: 0.5821 - val_acc: 0.6729\n",
            "Epoch 150/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5856 - acc: 0.6708Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5764 - acc: 0.6621\n",
            "Epoch 00150: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5847 - acc: 0.6709 - val_loss: 0.5822 - val_acc: 0.6598\n",
            "Epoch 151/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5931 - acc: 0.6658Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5786 - acc: 0.6738\n",
            "Epoch 00151: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5947 - acc: 0.6647 - val_loss: 0.5868 - val_acc: 0.6748\n",
            "Epoch 152/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5770 - acc: 0.6801Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5825 - acc: 0.6621\n",
            "Epoch 00152: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5810 - acc: 0.6756 - val_loss: 0.5984 - val_acc: 0.6617\n",
            "Epoch 153/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5780 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6026 - acc: 0.6562\n",
            "Epoch 00153: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5750 - acc: 0.6836 - val_loss: 0.6130 - val_acc: 0.6579\n",
            "Epoch 154/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5820 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5895 - acc: 0.6562\n",
            "Epoch 00154: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5803 - acc: 0.6796 - val_loss: 0.5971 - val_acc: 0.6579\n",
            "Epoch 155/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5877 - acc: 0.6719Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5975 - acc: 0.6445\n",
            "Epoch 00155: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5862 - acc: 0.6726 - val_loss: 0.5946 - val_acc: 0.6467\n",
            "Epoch 156/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5821 - acc: 0.6766Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5792 - acc: 0.6758\n",
            "Epoch 00156: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5843 - acc: 0.6731 - val_loss: 0.5778 - val_acc: 0.6766\n",
            "Epoch 157/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5821 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5805 - acc: 0.6641\n",
            "Epoch 00157: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5837 - acc: 0.6786 - val_loss: 0.5813 - val_acc: 0.6636\n",
            "Epoch 158/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5890 - acc: 0.6610Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5838 - acc: 0.6562\n",
            "Epoch 00158: val_loss did not improve from 0.57044\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5874 - acc: 0.6622 - val_loss: 0.5870 - val_acc: 0.6542\n",
            "Epoch 159/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5732 - acc: 0.6790Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5826 - acc: 0.6543\n",
            "Epoch 00159: val_loss improved from 0.57044 to 0.56544, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5706 - acc: 0.6821 - val_loss: 0.5654 - val_acc: 0.6579\n",
            "Epoch 160/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5925 - acc: 0.6542Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6816\n",
            "Epoch 00160: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5906 - acc: 0.6567 - val_loss: 0.5737 - val_acc: 0.6841\n",
            "Epoch 161/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5765 - acc: 0.6801Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5807 - acc: 0.6738\n",
            "Epoch 00161: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5743 - acc: 0.6796 - val_loss: 0.5861 - val_acc: 0.6710\n",
            "Epoch 162/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5857 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5954 - acc: 0.6504\n",
            "Epoch 00162: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5859 - acc: 0.6781 - val_loss: 0.5985 - val_acc: 0.6505\n",
            "Epoch 163/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5848 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5704 - acc: 0.6777\n",
            "Epoch 00163: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5829 - acc: 0.6801 - val_loss: 0.5819 - val_acc: 0.6766\n",
            "Epoch 164/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5817 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5788 - acc: 0.6738\n",
            "Epoch 00164: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5815 - acc: 0.6736 - val_loss: 0.5863 - val_acc: 0.6692\n",
            "Epoch 165/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5856 - acc: 0.6716Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5726 - acc: 0.6621\n",
            "Epoch 00165: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5842 - acc: 0.6716 - val_loss: 0.5887 - val_acc: 0.6617\n",
            "Epoch 166/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5723 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5790 - acc: 0.6719\n",
            "Epoch 00166: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5706 - acc: 0.6801 - val_loss: 0.5823 - val_acc: 0.6729\n",
            "Epoch 167/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5851 - acc: 0.6716Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5882 - acc: 0.6484\n",
            "Epoch 00167: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5825 - acc: 0.6721 - val_loss: 0.5998 - val_acc: 0.6486\n",
            "Epoch 168/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5787 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6023 - acc: 0.6328\n",
            "Epoch 00168: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5773 - acc: 0.6850 - val_loss: 0.6125 - val_acc: 0.6336\n",
            "Epoch 169/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5742 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5704 - acc: 0.6699\n",
            "Epoch 00169: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 0.5709 - acc: 0.6860 - val_loss: 0.5859 - val_acc: 0.6710\n",
            "Epoch 170/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5896 - acc: 0.6693Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5736 - acc: 0.6816\n",
            "Epoch 00170: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5909 - acc: 0.6733 - val_loss: 0.5799 - val_acc: 0.6822\n",
            "Epoch 171/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5725 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5740 - acc: 0.6699\n",
            "Epoch 00171: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.5705 - acc: 0.6831 - val_loss: 0.5878 - val_acc: 0.6710\n",
            "Epoch 172/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5923 - acc: 0.6594Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5818 - acc: 0.6758\n",
            "Epoch 00172: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5905 - acc: 0.6642 - val_loss: 0.5914 - val_acc: 0.6766\n",
            "Epoch 173/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5765 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5855 - acc: 0.6719\n",
            "Epoch 00173: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5739 - acc: 0.6806 - val_loss: 0.5978 - val_acc: 0.6729\n",
            "Epoch 174/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5747 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6004 - acc: 0.6582\n",
            "Epoch 00174: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5752 - acc: 0.6870 - val_loss: 0.6035 - val_acc: 0.6598\n",
            "Epoch 175/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5668 - acc: 0.6839Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5841 - acc: 0.6680\n",
            "Epoch 00175: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5672 - acc: 0.6836 - val_loss: 0.5921 - val_acc: 0.6692\n",
            "Epoch 176/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5942 - acc: 0.6605Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5862 - acc: 0.6641\n",
            "Epoch 00176: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5905 - acc: 0.6668 - val_loss: 0.5892 - val_acc: 0.6636\n",
            "Epoch 177/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5852 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5842 - acc: 0.6543\n",
            "Epoch 00177: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5873 - acc: 0.6821 - val_loss: 0.5875 - val_acc: 0.6561\n",
            "Epoch 178/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5482 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5713 - acc: 0.6738\n",
            "Epoch 00178: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5516 - acc: 0.6841 - val_loss: 0.5793 - val_acc: 0.6673\n",
            "Epoch 179/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5797 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5758 - acc: 0.6621\n",
            "Epoch 00179: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5799 - acc: 0.6766 - val_loss: 0.5829 - val_acc: 0.6636\n",
            "Epoch 180/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5812 - acc: 0.6551Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5730 - acc: 0.6738\n",
            "Epoch 00180: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5839 - acc: 0.6512 - val_loss: 0.5783 - val_acc: 0.6766\n",
            "Epoch 181/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.6750Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5860 - acc: 0.6797\n",
            "Epoch 00181: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5722 - acc: 0.6733 - val_loss: 0.5999 - val_acc: 0.6766\n",
            "Epoch 182/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5852 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5887 - acc: 0.6738\n",
            "Epoch 00182: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5866 - acc: 0.6751 - val_loss: 0.5911 - val_acc: 0.6748\n",
            "Epoch 183/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5659 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5804 - acc: 0.6699\n",
            "Epoch 00183: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5681 - acc: 0.6841 - val_loss: 0.5898 - val_acc: 0.6710\n",
            "Epoch 184/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5731 - acc: 0.6695Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5891 - acc: 0.6641\n",
            "Epoch 00184: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5740 - acc: 0.6692 - val_loss: 0.6020 - val_acc: 0.6598\n",
            "Epoch 185/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5705 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5816 - acc: 0.6758\n",
            "Epoch 00185: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5711 - acc: 0.6870 - val_loss: 0.5949 - val_acc: 0.6729\n",
            "Epoch 186/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5753 - acc: 0.6759Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5816 - acc: 0.6777\n",
            "Epoch 00186: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5769 - acc: 0.6761 - val_loss: 0.5918 - val_acc: 0.6729\n",
            "Epoch 187/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5636 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5654 - acc: 0.6855\n",
            "Epoch 00187: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5648 - acc: 0.6776 - val_loss: 0.5779 - val_acc: 0.6860\n",
            "Epoch 188/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5616 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5824 - acc: 0.6855\n",
            "Epoch 00188: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 5s 302ms/step - loss: 0.5661 - acc: 0.6791 - val_loss: 0.5950 - val_acc: 0.6766\n",
            "Epoch 189/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5740 - acc: 0.6714Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5767 - acc: 0.6738\n",
            "Epoch 00189: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5750 - acc: 0.6709 - val_loss: 0.6088 - val_acc: 0.6636\n",
            "Epoch 190/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5843 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5719 - acc: 0.6875\n",
            "Epoch 00190: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5821 - acc: 0.6677 - val_loss: 0.5902 - val_acc: 0.6766\n",
            "Epoch 191/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5677 - acc: 0.6773Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5815 - acc: 0.6680\n",
            "Epoch 00191: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5675 - acc: 0.6790 - val_loss: 0.6092 - val_acc: 0.6598\n",
            "Epoch 192/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5853 - acc: 0.6760Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5874 - acc: 0.6797\n",
            "Epoch 00192: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5850 - acc: 0.6748 - val_loss: 0.5951 - val_acc: 0.6729\n",
            "Epoch 193/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5640 - acc: 0.6941Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5709 - acc: 0.6875\n",
            "Epoch 00193: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5676 - acc: 0.6906 - val_loss: 0.5861 - val_acc: 0.6822\n",
            "Epoch 194/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5636 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5711 - acc: 0.6816\n",
            "Epoch 00194: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5607 - acc: 0.6865 - val_loss: 0.5895 - val_acc: 0.6710\n",
            "Epoch 195/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5822 - acc: 0.6760Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5724 - acc: 0.6953\n",
            "Epoch 00195: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5848 - acc: 0.6758 - val_loss: 0.5887 - val_acc: 0.6822\n",
            "Epoch 196/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5719 - acc: 0.6946Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5787 - acc: 0.6777\n",
            "Epoch 00196: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5719 - acc: 0.6931 - val_loss: 0.5893 - val_acc: 0.6710\n",
            "Epoch 197/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5705 - acc: 0.6906Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5817 - acc: 0.6758\n",
            "Epoch 00197: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5755 - acc: 0.6807 - val_loss: 0.5968 - val_acc: 0.6673\n",
            "Epoch 198/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5648 - acc: 0.6903Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5809 - acc: 0.6836\n",
            "Epoch 00198: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5617 - acc: 0.6946 - val_loss: 0.6084 - val_acc: 0.6748\n",
            "Epoch 199/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5755 - acc: 0.6911Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5612 - acc: 0.6953\n",
            "Epoch 00199: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5742 - acc: 0.6919 - val_loss: 0.5928 - val_acc: 0.6822\n",
            "Epoch 200/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5765 - acc: 0.6730Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6014 - acc: 0.6523\n",
            "Epoch 00200: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5749 - acc: 0.6759 - val_loss: 0.5894 - val_acc: 0.6542\n",
            "Epoch 201/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5826 - acc: 0.6682Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6130 - acc: 0.6523\n",
            "Epoch 00201: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5859 - acc: 0.6704 - val_loss: 0.6089 - val_acc: 0.6542\n",
            "Epoch 202/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5682 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5767 - acc: 0.6660\n",
            "Epoch 00202: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5658 - acc: 0.6736 - val_loss: 0.5745 - val_acc: 0.6673\n",
            "Epoch 203/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5675 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5720 - acc: 0.6699\n",
            "Epoch 00203: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5661 - acc: 0.6841 - val_loss: 0.5960 - val_acc: 0.6692\n",
            "Epoch 204/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5709 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5771 - acc: 0.6797\n",
            "Epoch 00204: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 346ms/step - loss: 0.5700 - acc: 0.6900 - val_loss: 0.5925 - val_acc: 0.6822\n",
            "Epoch 205/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5677 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5984 - acc: 0.6719\n",
            "Epoch 00205: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.5691 - acc: 0.6865 - val_loss: 0.5915 - val_acc: 0.6748\n",
            "Epoch 206/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5727 - acc: 0.6802Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5791 - acc: 0.6777\n",
            "Epoch 00206: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5689 - acc: 0.6826 - val_loss: 0.6095 - val_acc: 0.6822\n",
            "Epoch 207/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5665 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5960 - acc: 0.6602\n",
            "Epoch 00207: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5697 - acc: 0.6805 - val_loss: 0.5853 - val_acc: 0.6636\n",
            "Epoch 208/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5611 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5740 - acc: 0.6816\n",
            "Epoch 00208: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5630 - acc: 0.6860 - val_loss: 0.5810 - val_acc: 0.6822\n",
            "Epoch 209/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5648 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5787 - acc: 0.6738\n",
            "Epoch 00209: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5671 - acc: 0.6763 - val_loss: 0.5781 - val_acc: 0.6748\n",
            "Epoch 210/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5740 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5687 - acc: 0.6738\n",
            "Epoch 00210: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5754 - acc: 0.6754 - val_loss: 0.5772 - val_acc: 0.6748\n",
            "Epoch 211/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5822 - acc: 0.6621Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5834 - acc: 0.6582\n",
            "Epoch 00211: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5800 - acc: 0.6627 - val_loss: 0.5854 - val_acc: 0.6617\n",
            "Epoch 212/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5875 - acc: 0.6660\n",
            "Epoch 00212: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5748 - acc: 0.6736 - val_loss: 0.5888 - val_acc: 0.6692\n",
            "Epoch 213/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5776 - acc: 0.6699\n",
            "Epoch 00213: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5649 - acc: 0.6831 - val_loss: 0.5933 - val_acc: 0.6710\n",
            "Epoch 214/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5732 - acc: 0.6802Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6108 - acc: 0.6426\n",
            "Epoch 00214: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5754 - acc: 0.6792 - val_loss: 0.6047 - val_acc: 0.6449\n",
            "Epoch 215/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5682 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5687 - acc: 0.6719\n",
            "Epoch 00215: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5674 - acc: 0.6886 - val_loss: 0.5732 - val_acc: 0.6729\n",
            "Epoch 216/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5607 - acc: 0.6755Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5728 - acc: 0.6836\n",
            "Epoch 00216: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5615 - acc: 0.6763 - val_loss: 0.5792 - val_acc: 0.6860\n",
            "Epoch 217/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5658 - acc: 0.6778Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6738\n",
            "Epoch 00217: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5672 - acc: 0.6749 - val_loss: 0.5762 - val_acc: 0.6748\n",
            "Epoch 218/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5725 - acc: 0.6802Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5684 - acc: 0.6699\n",
            "Epoch 00218: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5729 - acc: 0.6816 - val_loss: 0.5676 - val_acc: 0.6710\n",
            "Epoch 219/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5605 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5783 - acc: 0.6797\n",
            "Epoch 00219: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5584 - acc: 0.6945 - val_loss: 0.5846 - val_acc: 0.6822\n",
            "Epoch 220/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5704 - acc: 0.6907Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5793 - acc: 0.6641\n",
            "Epoch 00220: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5722 - acc: 0.6895 - val_loss: 0.5824 - val_acc: 0.6673\n",
            "Epoch 221/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5575 - acc: 0.6944Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5885 - acc: 0.6543\n",
            "Epoch 00221: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 346ms/step - loss: 0.5559 - acc: 0.6955 - val_loss: 0.5931 - val_acc: 0.6542\n",
            "Epoch 222/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5647 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5843 - acc: 0.6738\n",
            "Epoch 00222: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 0.5618 - acc: 0.6880 - val_loss: 0.5918 - val_acc: 0.6748\n",
            "Epoch 223/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5693 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5726 - acc: 0.6660\n",
            "Epoch 00223: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5691 - acc: 0.6801 - val_loss: 0.5810 - val_acc: 0.6710\n",
            "Epoch 224/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5617 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5859 - acc: 0.6582\n",
            "Epoch 00224: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5648 - acc: 0.6816 - val_loss: 0.6001 - val_acc: 0.6579\n",
            "Epoch 225/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5729 - acc: 0.6790Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5662 - acc: 0.6738\n",
            "Epoch 00225: val_loss did not improve from 0.56544\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5711 - acc: 0.6786 - val_loss: 0.5691 - val_acc: 0.6785\n",
            "Epoch 226/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5775 - acc: 0.6719\n",
            "Epoch 00226: val_loss improved from 0.56544 to 0.55989, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5486 - acc: 0.6905 - val_loss: 0.5599 - val_acc: 0.6729\n",
            "Epoch 227/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5825 - acc: 0.6653Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5818 - acc: 0.6602\n",
            "Epoch 00227: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5758 - acc: 0.6706 - val_loss: 0.5648 - val_acc: 0.6636\n",
            "Epoch 228/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5656 - acc: 0.6802Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5874 - acc: 0.6797\n",
            "Epoch 00228: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5683 - acc: 0.6772 - val_loss: 0.5663 - val_acc: 0.6804\n",
            "Epoch 229/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5694 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5818 - acc: 0.6719\n",
            "Epoch 00229: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5728 - acc: 0.6771 - val_loss: 0.5664 - val_acc: 0.6729\n",
            "Epoch 230/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5593 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6049 - acc: 0.6602\n",
            "Epoch 00230: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5548 - acc: 0.7004 - val_loss: 0.5885 - val_acc: 0.6579\n",
            "Epoch 231/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5595 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5736 - acc: 0.6895\n",
            "Epoch 00231: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5587 - acc: 0.6880 - val_loss: 0.5719 - val_acc: 0.6897\n",
            "Epoch 232/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5749 - acc: 0.6790Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5880 - acc: 0.6641\n",
            "Epoch 00232: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5754 - acc: 0.6811 - val_loss: 0.5823 - val_acc: 0.6673\n",
            "Epoch 233/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5739 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5925 - acc: 0.6660\n",
            "Epoch 00233: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5713 - acc: 0.6855 - val_loss: 0.6060 - val_acc: 0.6654\n",
            "Epoch 234/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5631 - acc: 0.6923Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5722 - acc: 0.6660\n",
            "Epoch 00234: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5613 - acc: 0.6900 - val_loss: 0.5692 - val_acc: 0.6692\n",
            "Epoch 235/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5700 - acc: 0.6759Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5917 - acc: 0.6426\n",
            "Epoch 00235: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5688 - acc: 0.6781 - val_loss: 0.5979 - val_acc: 0.6449\n",
            "Epoch 236/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5681 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5720 - acc: 0.6777\n",
            "Epoch 00236: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5680 - acc: 0.6880 - val_loss: 0.5784 - val_acc: 0.6748\n",
            "Epoch 237/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5750 - acc: 0.6695Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5803 - acc: 0.6816\n",
            "Epoch 00237: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5747 - acc: 0.6667 - val_loss: 0.5821 - val_acc: 0.6748\n",
            "Epoch 238/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5785 - acc: 0.6719\n",
            "Epoch 00238: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5709 - acc: 0.6736 - val_loss: 0.5801 - val_acc: 0.6710\n",
            "Epoch 239/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5660 - acc: 0.6880Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5852 - acc: 0.6699\n",
            "Epoch 00239: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5626 - acc: 0.6885 - val_loss: 0.5853 - val_acc: 0.6673\n",
            "Epoch 240/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5714 - acc: 0.6797Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5716 - acc: 0.6973\n",
            "Epoch 00240: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5698 - acc: 0.6802 - val_loss: 0.5810 - val_acc: 0.6935\n",
            "Epoch 241/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.6978Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5791 - acc: 0.6855\n",
            "Epoch 00241: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5539 - acc: 0.6941 - val_loss: 0.5808 - val_acc: 0.6822\n",
            "Epoch 242/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5641 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5812 - acc: 0.6641\n",
            "Epoch 00242: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5652 - acc: 0.6836 - val_loss: 0.5863 - val_acc: 0.6579\n",
            "Epoch 243/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5626 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5688 - acc: 0.6660\n",
            "Epoch 00243: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5653 - acc: 0.6846 - val_loss: 0.5867 - val_acc: 0.6710\n",
            "Epoch 244/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5736 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5732 - acc: 0.6758\n",
            "Epoch 00244: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5713 - acc: 0.6812 - val_loss: 0.5872 - val_acc: 0.6785\n",
            "Epoch 245/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5633 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5875 - acc: 0.6465\n",
            "Epoch 00245: val_loss did not improve from 0.55989\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.5645 - acc: 0.6945 - val_loss: 0.5704 - val_acc: 0.6523\n",
            "Epoch 246/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5633 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5784 - acc: 0.6855\n",
            "Epoch 00246: val_loss improved from 0.55989 to 0.55889, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5628 - acc: 0.6920 - val_loss: 0.5589 - val_acc: 0.6879\n",
            "Epoch 247/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5566 - acc: 0.6892Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5730 - acc: 0.7051\n",
            "Epoch 00247: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5595 - acc: 0.6866 - val_loss: 0.5695 - val_acc: 0.7028\n",
            "Epoch 248/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5646 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6038 - acc: 0.6387\n",
            "Epoch 00248: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5659 - acc: 0.6846 - val_loss: 0.5774 - val_acc: 0.6449\n",
            "Epoch 249/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5679 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6306 - acc: 0.6426\n",
            "Epoch 00249: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5637 - acc: 0.6870 - val_loss: 0.6060 - val_acc: 0.6467\n",
            "Epoch 250/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.6724Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5788 - acc: 0.6777\n",
            "Epoch 00250: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5656 - acc: 0.6795 - val_loss: 0.5713 - val_acc: 0.6785\n",
            "Epoch 251/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5554 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5755 - acc: 0.6797\n",
            "Epoch 00251: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5545 - acc: 0.7017 - val_loss: 0.5726 - val_acc: 0.6804\n",
            "Epoch 252/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5629 - acc: 0.6789Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5684 - acc: 0.6875\n",
            "Epoch 00252: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5602 - acc: 0.6795 - val_loss: 0.5622 - val_acc: 0.6879\n",
            "Epoch 253/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5665 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5699 - acc: 0.6797\n",
            "Epoch 00253: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5658 - acc: 0.6895 - val_loss: 0.5655 - val_acc: 0.6804\n",
            "Epoch 254/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5613 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5824 - acc: 0.6816\n",
            "Epoch 00254: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5599 - acc: 0.6885 - val_loss: 0.5808 - val_acc: 0.6804\n",
            "Epoch 255/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5636 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5693 - acc: 0.6895\n",
            "Epoch 00255: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5694 - acc: 0.6900 - val_loss: 0.5699 - val_acc: 0.6860\n",
            "Epoch 256/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5652 - acc: 0.6753Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5925 - acc: 0.6543\n",
            "Epoch 00256: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5675 - acc: 0.6716 - val_loss: 0.5888 - val_acc: 0.6542\n",
            "Epoch 257/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5780 - acc: 0.6721Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5658 - acc: 0.6797\n",
            "Epoch 00257: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5745 - acc: 0.6746 - val_loss: 0.5622 - val_acc: 0.6804\n",
            "Epoch 258/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5597 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5721 - acc: 0.6719\n",
            "Epoch 00258: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5571 - acc: 0.6920 - val_loss: 0.5721 - val_acc: 0.6729\n",
            "Epoch 259/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5577 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5785 - acc: 0.6836\n",
            "Epoch 00259: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.5560 - acc: 0.6970 - val_loss: 0.5724 - val_acc: 0.6822\n",
            "Epoch 260/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5665 - acc: 0.6833Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5643 - acc: 0.6875\n",
            "Epoch 00260: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5669 - acc: 0.6846 - val_loss: 0.5805 - val_acc: 0.6860\n",
            "Epoch 261/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5634 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5801 - acc: 0.6738\n",
            "Epoch 00261: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5629 - acc: 0.6796 - val_loss: 0.5701 - val_acc: 0.6785\n",
            "Epoch 262/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5584 - acc: 0.6918Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5860 - acc: 0.6738\n",
            "Epoch 00262: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5576 - acc: 0.6920 - val_loss: 0.5685 - val_acc: 0.6766\n",
            "Epoch 263/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5447 - acc: 0.7068Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5791 - acc: 0.6914\n",
            "Epoch 00263: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5461 - acc: 0.7051 - val_loss: 0.5755 - val_acc: 0.6935\n",
            "Epoch 264/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5637 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5929 - acc: 0.6445\n",
            "Epoch 00264: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5624 - acc: 0.6865 - val_loss: 0.5745 - val_acc: 0.6486\n",
            "Epoch 265/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5611 - acc: 0.6941Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5865 - acc: 0.6680\n",
            "Epoch 00265: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5572 - acc: 0.6962 - val_loss: 0.5696 - val_acc: 0.6748\n",
            "Epoch 266/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7016Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6621\n",
            "Epoch 00266: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5549 - acc: 0.7002 - val_loss: 0.5675 - val_acc: 0.6673\n",
            "Epoch 267/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5599 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5826 - acc: 0.6562\n",
            "Epoch 00267: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5603 - acc: 0.6870 - val_loss: 0.5636 - val_acc: 0.6636\n",
            "Epoch 268/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5759 - acc: 0.6660\n",
            "Epoch 00268: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5531 - acc: 0.7014 - val_loss: 0.5617 - val_acc: 0.6692\n",
            "Epoch 269/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6610Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5728 - acc: 0.6855\n",
            "Epoch 00269: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5731 - acc: 0.6627 - val_loss: 0.5708 - val_acc: 0.6897\n",
            "Epoch 270/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.6989Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5725 - acc: 0.6738\n",
            "Epoch 00270: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5534 - acc: 0.6992 - val_loss: 0.5755 - val_acc: 0.6785\n",
            "Epoch 271/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5553 - acc: 0.6979Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5663 - acc: 0.6855\n",
            "Epoch 00271: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5569 - acc: 0.6958 - val_loss: 0.5638 - val_acc: 0.6897\n",
            "Epoch 272/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5580 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6007 - acc: 0.6543\n",
            "Epoch 00272: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5571 - acc: 0.6870 - val_loss: 0.6055 - val_acc: 0.6598\n",
            "Epoch 273/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5721 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5770 - acc: 0.6602\n",
            "Epoch 00273: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.5686 - acc: 0.6875 - val_loss: 0.5722 - val_acc: 0.6654\n",
            "Epoch 274/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5704 - acc: 0.6895\n",
            "Epoch 00274: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5546 - acc: 0.7009 - val_loss: 0.5657 - val_acc: 0.6935\n",
            "Epoch 275/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5635 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5750 - acc: 0.6758\n",
            "Epoch 00275: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5620 - acc: 0.6801 - val_loss: 0.5908 - val_acc: 0.6804\n",
            "Epoch 276/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5496 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5562 - acc: 0.6914\n",
            "Epoch 00276: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5537 - acc: 0.6930 - val_loss: 0.5897 - val_acc: 0.6841\n",
            "Epoch 277/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.7005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5853 - acc: 0.6797\n",
            "Epoch 00277: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5469 - acc: 0.7021 - val_loss: 0.6109 - val_acc: 0.6766\n",
            "Epoch 278/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5736 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5670 - acc: 0.6816\n",
            "Epoch 00278: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5741 - acc: 0.6815 - val_loss: 0.6038 - val_acc: 0.6766\n",
            "Epoch 279/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5463 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5595 - acc: 0.6836\n",
            "Epoch 00279: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5483 - acc: 0.6870 - val_loss: 0.6181 - val_acc: 0.6804\n",
            "Epoch 280/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5655 - acc: 0.6875\n",
            "Epoch 00280: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5488 - acc: 0.6890 - val_loss: 0.5882 - val_acc: 0.6860\n",
            "Epoch 281/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5565 - acc: 0.6895\n",
            "Epoch 00281: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5684 - acc: 0.6865 - val_loss: 0.5928 - val_acc: 0.6822\n",
            "Epoch 282/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5598 - acc: 0.6919Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5718 - acc: 0.6836\n",
            "Epoch 00282: val_loss did not improve from 0.55889\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5601 - acc: 0.6921 - val_loss: 0.6080 - val_acc: 0.6766\n",
            "Epoch 283/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.6938Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6777\n",
            "Epoch 00283: val_loss improved from 0.55889 to 0.55615, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5557 - acc: 0.6934 - val_loss: 0.5561 - val_acc: 0.6822\n",
            "Epoch 284/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5590 - acc: 0.6960Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5740 - acc: 0.6699\n",
            "Epoch 00284: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5568 - acc: 0.6970 - val_loss: 0.5697 - val_acc: 0.6766\n",
            "Epoch 285/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5584 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5679 - acc: 0.6895\n",
            "Epoch 00285: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5592 - acc: 0.6860 - val_loss: 0.5727 - val_acc: 0.6841\n",
            "Epoch 286/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5522 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5663 - acc: 0.6855\n",
            "Epoch 00286: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5518 - acc: 0.6960 - val_loss: 0.5677 - val_acc: 0.6879\n",
            "Epoch 287/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5511 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5789 - acc: 0.6797\n",
            "Epoch 00287: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5520 - acc: 0.6865 - val_loss: 0.5761 - val_acc: 0.6785\n",
            "Epoch 288/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5594 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5892 - acc: 0.6816\n",
            "Epoch 00288: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5638 - acc: 0.6880 - val_loss: 0.5854 - val_acc: 0.6841\n",
            "Epoch 289/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5557 - acc: 0.6944Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5766 - acc: 0.6719\n",
            "Epoch 00289: val_loss did not improve from 0.55615\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5537 - acc: 0.6970 - val_loss: 0.5730 - val_acc: 0.6692\n",
            "Epoch 290/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5545 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5588 - acc: 0.6855\n",
            "Epoch 00290: val_loss improved from 0.55615 to 0.54744, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.5518 - acc: 0.6925 - val_loss: 0.5474 - val_acc: 0.6916\n",
            "Epoch 291/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5632 - acc: 0.6918Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5578 - acc: 0.6895\n",
            "Epoch 00291: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5625 - acc: 0.6890 - val_loss: 0.5617 - val_acc: 0.6897\n",
            "Epoch 292/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5583 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5659 - acc: 0.6797\n",
            "Epoch 00292: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5578 - acc: 0.6816 - val_loss: 0.5593 - val_acc: 0.6822\n",
            "Epoch 293/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5609 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5642 - acc: 0.6875\n",
            "Epoch 00293: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5601 - acc: 0.6841 - val_loss: 0.5686 - val_acc: 0.6879\n",
            "Epoch 294/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5829 - acc: 0.6738\n",
            "Epoch 00294: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5547 - acc: 0.6980 - val_loss: 0.5899 - val_acc: 0.6785\n",
            "Epoch 295/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5449 - acc: 0.7081Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5625 - acc: 0.6934\n",
            "Epoch 00295: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5452 - acc: 0.7078 - val_loss: 0.5625 - val_acc: 0.6916\n",
            "Epoch 296/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5503 - acc: 0.6948Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5553 - acc: 0.6973\n",
            "Epoch 00296: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5506 - acc: 0.6963 - val_loss: 0.5589 - val_acc: 0.6972\n",
            "Epoch 297/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5517 - acc: 0.7005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5748 - acc: 0.6680\n",
            "Epoch 00297: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5502 - acc: 0.7017 - val_loss: 0.5768 - val_acc: 0.6673\n",
            "Epoch 298/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5533 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5627 - acc: 0.6895\n",
            "Epoch 00298: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5559 - acc: 0.6915 - val_loss: 0.5687 - val_acc: 0.6879\n",
            "Epoch 299/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5496 - acc: 0.6807Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5921 - acc: 0.6602\n",
            "Epoch 00299: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5538 - acc: 0.6807 - val_loss: 0.5940 - val_acc: 0.6579\n",
            "Epoch 300/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5462 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5685 - acc: 0.6914\n",
            "Epoch 00300: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5513 - acc: 0.6930 - val_loss: 0.5722 - val_acc: 0.6897\n",
            "Epoch 301/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6924Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5823 - acc: 0.6836\n",
            "Epoch 00301: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5516 - acc: 0.6911 - val_loss: 0.5825 - val_acc: 0.6804\n",
            "Epoch 302/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5451 - acc: 0.7026Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5940 - acc: 0.6602\n",
            "Epoch 00302: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5469 - acc: 0.6992 - val_loss: 0.5967 - val_acc: 0.6617\n",
            "Epoch 303/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5594 - acc: 0.6941Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5709 - acc: 0.6934\n",
            "Epoch 00303: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5556 - acc: 0.6977 - val_loss: 0.5788 - val_acc: 0.6897\n",
            "Epoch 304/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5645 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5802 - acc: 0.6895\n",
            "Epoch 00304: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5625 - acc: 0.6865 - val_loss: 0.5791 - val_acc: 0.6860\n",
            "Epoch 305/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5532 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5704 - acc: 0.6836\n",
            "Epoch 00305: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5552 - acc: 0.7094 - val_loss: 0.5824 - val_acc: 0.6766\n",
            "Epoch 306/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5585 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5584 - acc: 0.6934\n",
            "Epoch 00306: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5604 - acc: 0.6895 - val_loss: 0.5618 - val_acc: 0.6935\n",
            "Epoch 307/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5575 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5679 - acc: 0.6953\n",
            "Epoch 00307: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.5512 - acc: 0.6980 - val_loss: 0.5652 - val_acc: 0.6935\n",
            "Epoch 308/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5518 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5818 - acc: 0.6680\n",
            "Epoch 00308: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5542 - acc: 0.6915 - val_loss: 0.5896 - val_acc: 0.6654\n",
            "Epoch 309/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5678 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5839 - acc: 0.6602\n",
            "Epoch 00309: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5681 - acc: 0.6865 - val_loss: 0.5760 - val_acc: 0.6636\n",
            "Epoch 310/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5476 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5638 - acc: 0.6973\n",
            "Epoch 00310: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5495 - acc: 0.7054 - val_loss: 0.5617 - val_acc: 0.7028\n",
            "Epoch 311/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5567 - acc: 0.6969Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5673 - acc: 0.6836\n",
            "Epoch 00311: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 0.5522 - acc: 0.6973 - val_loss: 0.5579 - val_acc: 0.6860\n",
            "Epoch 312/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5387 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5821 - acc: 0.6738\n",
            "Epoch 00312: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.5401 - acc: 0.7149 - val_loss: 0.5740 - val_acc: 0.6766\n",
            "Epoch 313/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.6989Epoch 1/500\n",
            " 5/16 [========>.....................] - ETA: 4s - loss: 0.6014 - acc: 0.6654\n",
            "Epoch 00313: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 7s 415ms/step - loss: 0.5514 - acc: 0.6967 - val_loss: 0.6014 - val_acc: 0.6654\n",
            "Epoch 314/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5586 - acc: 0.6855\n",
            "Epoch 00314: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5551 - acc: 0.7000 - val_loss: 0.5709 - val_acc: 0.6822\n",
            "Epoch 315/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5547 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5771 - acc: 0.6914\n",
            "Epoch 00315: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5542 - acc: 0.6930 - val_loss: 0.5833 - val_acc: 0.6860\n",
            "Epoch 316/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5547 - acc: 0.6932Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5690 - acc: 0.6895\n",
            "Epoch 00316: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5571 - acc: 0.6885 - val_loss: 0.5859 - val_acc: 0.6841\n",
            "Epoch 317/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5585 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5968 - acc: 0.6719\n",
            "Epoch 00317: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5603 - acc: 0.6766 - val_loss: 0.5994 - val_acc: 0.6710\n",
            "Epoch 318/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.6968Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5599 - acc: 0.6973\n",
            "Epoch 00318: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5529 - acc: 0.6992 - val_loss: 0.5643 - val_acc: 0.6953\n",
            "Epoch 319/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5551 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5820 - acc: 0.6719\n",
            "Epoch 00319: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5535 - acc: 0.6763 - val_loss: 0.5774 - val_acc: 0.6748\n",
            "Epoch 320/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7167Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5655 - acc: 0.6816\n",
            "Epoch 00320: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5352 - acc: 0.7163 - val_loss: 0.5652 - val_acc: 0.6841\n",
            "Epoch 321/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5557 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5741 - acc: 0.6934\n",
            "Epoch 00321: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5525 - acc: 0.6880 - val_loss: 0.5779 - val_acc: 0.6953\n",
            "Epoch 322/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5604 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5729 - acc: 0.6836\n",
            "Epoch 00322: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5595 - acc: 0.6975 - val_loss: 0.5745 - val_acc: 0.6841\n",
            "Epoch 323/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5666 - acc: 0.6875\n",
            "Epoch 00323: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5520 - acc: 0.6870 - val_loss: 0.5676 - val_acc: 0.6879\n",
            "Epoch 324/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5834 - acc: 0.6621\n",
            "Epoch 00324: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.5528 - acc: 0.7009 - val_loss: 0.5799 - val_acc: 0.6636\n",
            "Epoch 325/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5850 - acc: 0.6504\n",
            "Epoch 00325: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5554 - acc: 0.6826 - val_loss: 0.5867 - val_acc: 0.6579\n",
            "Epoch 326/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5624 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5584 - acc: 0.6836\n",
            "Epoch 00326: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5639 - acc: 0.6860 - val_loss: 0.5597 - val_acc: 0.6879\n",
            "Epoch 327/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5686 - acc: 0.7012\n",
            "Epoch 00327: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5460 - acc: 0.7059 - val_loss: 0.5683 - val_acc: 0.6991\n",
            "Epoch 328/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5493 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5743 - acc: 0.6816\n",
            "Epoch 00328: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 398ms/step - loss: 0.5467 - acc: 0.7044 - val_loss: 0.5632 - val_acc: 0.6860\n",
            "Epoch 329/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5776 - acc: 0.6719\n",
            "Epoch 00329: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5470 - acc: 0.7074 - val_loss: 0.5843 - val_acc: 0.6766\n",
            "Epoch 330/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5539 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5648 - acc: 0.6895\n",
            "Epoch 00330: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5531 - acc: 0.6855 - val_loss: 0.5559 - val_acc: 0.6916\n",
            "Epoch 331/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5520 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5641 - acc: 0.6836\n",
            "Epoch 00331: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5546 - acc: 0.6945 - val_loss: 0.5614 - val_acc: 0.6822\n",
            "Epoch 332/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5648 - acc: 0.6914\n",
            "Epoch 00332: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5456 - acc: 0.7098 - val_loss: 0.5608 - val_acc: 0.6935\n",
            "Epoch 333/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5391 - acc: 0.7000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5813 - acc: 0.6914\n",
            "Epoch 00333: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5438 - acc: 0.6963 - val_loss: 0.5802 - val_acc: 0.6972\n",
            "Epoch 334/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5493 - acc: 0.6876Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5711 - acc: 0.6914\n",
            "Epoch 00334: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5489 - acc: 0.6871 - val_loss: 0.5724 - val_acc: 0.6860\n",
            "Epoch 335/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5528 - acc: 0.6974Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5802 - acc: 0.6719\n",
            "Epoch 00335: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5502 - acc: 0.6978 - val_loss: 0.5749 - val_acc: 0.6692\n",
            "Epoch 336/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5680 - acc: 0.7188\n",
            "Epoch 00336: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5402 - acc: 0.7114 - val_loss: 0.5635 - val_acc: 0.7159\n",
            "Epoch 337/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5604 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5648 - acc: 0.6914\n",
            "Epoch 00337: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5579 - acc: 0.6860 - val_loss: 0.5673 - val_acc: 0.6879\n",
            "Epoch 338/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5547 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5684 - acc: 0.7012\n",
            "Epoch 00338: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5531 - acc: 0.7004 - val_loss: 0.5668 - val_acc: 0.6991\n",
            "Epoch 339/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5465 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5765 - acc: 0.6875\n",
            "Epoch 00339: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5471 - acc: 0.7119 - val_loss: 0.5745 - val_acc: 0.6841\n",
            "Epoch 340/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5568 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5563 - acc: 0.6953\n",
            "Epoch 00340: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5511 - acc: 0.6910 - val_loss: 0.5556 - val_acc: 0.6916\n",
            "Epoch 341/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5613 - acc: 0.6934\n",
            "Epoch 00341: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5515 - acc: 0.7039 - val_loss: 0.5611 - val_acc: 0.6879\n",
            "Epoch 342/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5680 - acc: 0.6816\n",
            "Epoch 00342: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5458 - acc: 0.6990 - val_loss: 0.5722 - val_acc: 0.6785\n",
            "Epoch 343/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5499 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5806 - acc: 0.6699\n",
            "Epoch 00343: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5498 - acc: 0.6860 - val_loss: 0.5948 - val_acc: 0.6654\n",
            "Epoch 344/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5524 - acc: 0.6951Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5696 - acc: 0.6953\n",
            "Epoch 00344: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5540 - acc: 0.6957 - val_loss: 0.5719 - val_acc: 0.6897\n",
            "Epoch 345/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5544 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5692 - acc: 0.6855\n",
            "Epoch 00345: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5531 - acc: 0.6965 - val_loss: 0.5707 - val_acc: 0.6785\n",
            "Epoch 346/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5352 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5736 - acc: 0.7051\n",
            "Epoch 00346: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5357 - acc: 0.7064 - val_loss: 0.5766 - val_acc: 0.7009\n",
            "Epoch 347/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5458 - acc: 0.6948Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5810 - acc: 0.6543\n",
            "Epoch 00347: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.5494 - acc: 0.6924 - val_loss: 0.5844 - val_acc: 0.6505\n",
            "Epoch 348/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.6908Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5630 - acc: 0.7051\n",
            "Epoch 00348: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5472 - acc: 0.6936 - val_loss: 0.5683 - val_acc: 0.7009\n",
            "Epoch 349/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5839 - acc: 0.6699\n",
            "Epoch 00349: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5566 - acc: 0.6890 - val_loss: 0.5869 - val_acc: 0.6673\n",
            "Epoch 350/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5412 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5558 - acc: 0.7129\n",
            "Epoch 00350: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5432 - acc: 0.7085 - val_loss: 0.5803 - val_acc: 0.7065\n",
            "Epoch 351/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5647 - acc: 0.6914\n",
            "Epoch 00351: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5465 - acc: 0.7113 - val_loss: 0.5786 - val_acc: 0.6841\n",
            "Epoch 352/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5500 - acc: 0.7042Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5632 - acc: 0.6973\n",
            "Epoch 00352: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5507 - acc: 0.7031 - val_loss: 0.5833 - val_acc: 0.6916\n",
            "Epoch 353/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5417 - acc: 0.6951Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5523 - acc: 0.7227\n",
            "Epoch 00353: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5431 - acc: 0.6962 - val_loss: 0.5720 - val_acc: 0.7140\n",
            "Epoch 354/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5409 - acc: 0.6979Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5606 - acc: 0.7148\n",
            "Epoch 00354: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5441 - acc: 0.6958 - val_loss: 0.5840 - val_acc: 0.7065\n",
            "Epoch 355/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5542 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5744 - acc: 0.6836\n",
            "Epoch 00355: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5518 - acc: 0.7074 - val_loss: 0.5796 - val_acc: 0.6822\n",
            "Epoch 356/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5504 - acc: 0.6960Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5661 - acc: 0.6875\n",
            "Epoch 00356: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5438 - acc: 0.6990 - val_loss: 0.5793 - val_acc: 0.6822\n",
            "Epoch 357/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5553 - acc: 0.6973\n",
            "Epoch 00357: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5520 - acc: 0.6995 - val_loss: 0.5734 - val_acc: 0.6897\n",
            "Epoch 358/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5553 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5430 - acc: 0.7305\n",
            "Epoch 00358: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5526 - acc: 0.7029 - val_loss: 0.5793 - val_acc: 0.7215\n",
            "Epoch 359/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5664 - acc: 0.6836\n",
            "Epoch 00359: val_loss improved from 0.54744 to 0.54347, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5546 - acc: 0.6968 - val_loss: 0.5435 - val_acc: 0.6897\n",
            "Epoch 360/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.7022Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5518 - acc: 0.6953\n",
            "Epoch 00360: val_loss did not improve from 0.54347\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5553 - acc: 0.6977 - val_loss: 0.5772 - val_acc: 0.6935\n",
            "Epoch 361/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5428 - acc: 0.7034Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5671 - acc: 0.6934\n",
            "Epoch 00361: val_loss did not improve from 0.54347\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5444 - acc: 0.7019 - val_loss: 0.5558 - val_acc: 0.6935\n",
            "Epoch 362/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5623 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5516 - acc: 0.7051\n",
            "Epoch 00362: val_loss improved from 0.54347 to 0.54002, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.5584 - acc: 0.6905 - val_loss: 0.5400 - val_acc: 0.7047\n",
            "Epoch 363/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5517 - acc: 0.7034Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5656 - acc: 0.6875\n",
            "Epoch 00363: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5479 - acc: 0.7074 - val_loss: 0.5495 - val_acc: 0.6860\n",
            "Epoch 364/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5375 - acc: 0.6984Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5675 - acc: 0.7031\n",
            "Epoch 00364: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5384 - acc: 0.6992 - val_loss: 0.5581 - val_acc: 0.7009\n",
            "Epoch 365/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5409 - acc: 0.6923Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5719 - acc: 0.6875\n",
            "Epoch 00365: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5392 - acc: 0.6940 - val_loss: 0.5553 - val_acc: 0.6841\n",
            "Epoch 366/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5607 - acc: 0.6836\n",
            "Epoch 00366: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5503 - acc: 0.6965 - val_loss: 0.5413 - val_acc: 0.6841\n",
            "Epoch 367/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5454 - acc: 0.7125Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5814 - acc: 0.6973\n",
            "Epoch 00367: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5417 - acc: 0.7178 - val_loss: 0.5652 - val_acc: 0.6935\n",
            "Epoch 368/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5399 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5660 - acc: 0.7031\n",
            "Epoch 00368: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5409 - acc: 0.7044 - val_loss: 0.5500 - val_acc: 0.7009\n",
            "Epoch 369/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5603 - acc: 0.6934\n",
            "Epoch 00369: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5495 - acc: 0.6870 - val_loss: 0.5456 - val_acc: 0.6916\n",
            "Epoch 370/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5469 - acc: 0.6912Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5735 - acc: 0.6855\n",
            "Epoch 00370: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5488 - acc: 0.6950 - val_loss: 0.5607 - val_acc: 0.6879\n",
            "Epoch 371/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5337 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5685 - acc: 0.7070\n",
            "Epoch 00371: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5345 - acc: 0.7064 - val_loss: 0.5481 - val_acc: 0.7065\n",
            "Epoch 372/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5804 - acc: 0.6816\n",
            "Epoch 00372: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5380 - acc: 0.7123 - val_loss: 0.5628 - val_acc: 0.6841\n",
            "Epoch 373/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5450 - acc: 0.6984Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5788 - acc: 0.6992\n",
            "Epoch 00373: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5452 - acc: 0.6973 - val_loss: 0.5698 - val_acc: 0.7009\n",
            "Epoch 374/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5705 - acc: 0.6895\n",
            "Epoch 00374: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5461 - acc: 0.6915 - val_loss: 0.5814 - val_acc: 0.6879\n",
            "Epoch 375/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5436 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5628 - acc: 0.7031\n",
            "Epoch 00375: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5438 - acc: 0.7019 - val_loss: 0.5822 - val_acc: 0.7009\n",
            "Epoch 376/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5389 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5581 - acc: 0.7031\n",
            "Epoch 00376: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5409 - acc: 0.7089 - val_loss: 0.5607 - val_acc: 0.7009\n",
            "Epoch 377/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5654 - acc: 0.7031\n",
            "Epoch 00377: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5514 - acc: 0.7014 - val_loss: 0.5713 - val_acc: 0.7009\n",
            "Epoch 378/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5778 - acc: 0.6836\n",
            "Epoch 00378: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5437 - acc: 0.6919 - val_loss: 0.5999 - val_acc: 0.6822\n",
            "Epoch 379/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.7000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5611 - acc: 0.6992\n",
            "Epoch 00379: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5499 - acc: 0.7007 - val_loss: 0.5620 - val_acc: 0.6972\n",
            "Epoch 380/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5489 - acc: 0.6990Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5705 - acc: 0.7031\n",
            "Epoch 00380: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 399ms/step - loss: 0.5497 - acc: 0.6997 - val_loss: 0.5844 - val_acc: 0.6991\n",
            "Epoch 381/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5511 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5627 - acc: 0.6934\n",
            "Epoch 00381: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5478 - acc: 0.7014 - val_loss: 0.5594 - val_acc: 0.6897\n",
            "Epoch 382/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5625 - acc: 0.6953\n",
            "Epoch 00382: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5544 - acc: 0.6990 - val_loss: 0.5557 - val_acc: 0.6897\n",
            "Epoch 383/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5448 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5732 - acc: 0.6836\n",
            "Epoch 00383: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5439 - acc: 0.7128 - val_loss: 0.5587 - val_acc: 0.6841\n",
            "Epoch 384/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5322 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5890 - acc: 0.6816\n",
            "Epoch 00384: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5380 - acc: 0.7100 - val_loss: 0.5773 - val_acc: 0.6804\n",
            "Epoch 385/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.6978Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5551 - acc: 0.6992\n",
            "Epoch 00385: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5419 - acc: 0.6992 - val_loss: 0.5649 - val_acc: 0.6953\n",
            "Epoch 386/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5510 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5656 - acc: 0.6875\n",
            "Epoch 00386: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5524 - acc: 0.6851 - val_loss: 0.5647 - val_acc: 0.6879\n",
            "Epoch 387/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5296 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6044 - acc: 0.6758\n",
            "Epoch 00387: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5310 - acc: 0.7019 - val_loss: 0.6188 - val_acc: 0.6710\n",
            "Epoch 388/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5886 - acc: 0.6816\n",
            "Epoch 00388: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5502 - acc: 0.7093 - val_loss: 0.5867 - val_acc: 0.6804\n",
            "Epoch 389/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5393 - acc: 0.7042Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5678 - acc: 0.6758\n",
            "Epoch 00389: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5391 - acc: 0.7026 - val_loss: 0.5448 - val_acc: 0.6804\n",
            "Epoch 390/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5519 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5620 - acc: 0.6953\n",
            "Epoch 00390: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5488 - acc: 0.6865 - val_loss: 0.5456 - val_acc: 0.6991\n",
            "Epoch 391/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5419 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5827 - acc: 0.6602\n",
            "Epoch 00391: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5434 - acc: 0.6940 - val_loss: 0.5628 - val_acc: 0.6673\n",
            "Epoch 392/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5402 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5605 - acc: 0.6973\n",
            "Epoch 00392: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.5418 - acc: 0.7009 - val_loss: 0.5445 - val_acc: 0.6972\n",
            "Epoch 393/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5354 - acc: 0.7031Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5648 - acc: 0.6855\n",
            "Epoch 00393: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5361 - acc: 0.7021 - val_loss: 0.5674 - val_acc: 0.6822\n",
            "Epoch 394/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7032Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5711 - acc: 0.6855\n",
            "Epoch 00394: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5432 - acc: 0.7058 - val_loss: 0.5814 - val_acc: 0.6822\n",
            "Epoch 395/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.6885Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5501 - acc: 0.7031\n",
            "Epoch 00395: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5561 - acc: 0.6890 - val_loss: 0.5665 - val_acc: 0.6991\n",
            "Epoch 396/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5387 - acc: 0.7204Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5605 - acc: 0.7109\n",
            "Epoch 00396: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5361 - acc: 0.7188 - val_loss: 0.5715 - val_acc: 0.7084\n",
            "Epoch 397/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5277 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5707 - acc: 0.6777\n",
            "Epoch 00397: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5344 - acc: 0.7063 - val_loss: 0.5799 - val_acc: 0.6748\n",
            "Epoch 398/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5418 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5680 - acc: 0.6855\n",
            "Epoch 00398: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5411 - acc: 0.7039 - val_loss: 0.5556 - val_acc: 0.6897\n",
            "Epoch 399/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.6907Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5686 - acc: 0.6875\n",
            "Epoch 00399: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5376 - acc: 0.6945 - val_loss: 0.5670 - val_acc: 0.6897\n",
            "Epoch 400/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5550 - acc: 0.7047Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5747 - acc: 0.6855\n",
            "Epoch 00400: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5571 - acc: 0.7036 - val_loss: 0.5660 - val_acc: 0.6879\n",
            "Epoch 401/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5368 - acc: 0.7076Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5646 - acc: 0.7070\n",
            "Epoch 00401: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5381 - acc: 0.7042 - val_loss: 0.5491 - val_acc: 0.7028\n",
            "Epoch 402/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5382 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5754 - acc: 0.6875\n",
            "Epoch 00402: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5375 - acc: 0.7084 - val_loss: 0.5605 - val_acc: 0.6860\n",
            "Epoch 403/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.6927Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5784 - acc: 0.6797\n",
            "Epoch 00403: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5468 - acc: 0.6973 - val_loss: 0.5632 - val_acc: 0.6766\n",
            "Epoch 404/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5386 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5663 - acc: 0.6992\n",
            "Epoch 00404: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5372 - acc: 0.7049 - val_loss: 0.5643 - val_acc: 0.6991\n",
            "Epoch 405/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5331 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5618 - acc: 0.7031\n",
            "Epoch 00405: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5399 - acc: 0.7037 - val_loss: 0.5521 - val_acc: 0.7047\n",
            "Epoch 406/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5375 - acc: 0.7104Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5604 - acc: 0.7070\n",
            "Epoch 00406: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5363 - acc: 0.7100 - val_loss: 0.5476 - val_acc: 0.7103\n",
            "Epoch 407/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5343 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5520 - acc: 0.7148\n",
            "Epoch 00407: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5393 - acc: 0.7059 - val_loss: 0.5423 - val_acc: 0.7159\n",
            "Epoch 408/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5611 - acc: 0.6914\n",
            "Epoch 00408: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5427 - acc: 0.6990 - val_loss: 0.5490 - val_acc: 0.6953\n",
            "Epoch 409/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5405 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5762 - acc: 0.6836\n",
            "Epoch 00409: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5406 - acc: 0.6950 - val_loss: 0.5692 - val_acc: 0.6841\n",
            "Epoch 410/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5600 - acc: 0.6895\n",
            "Epoch 00410: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5390 - acc: 0.6960 - val_loss: 0.5552 - val_acc: 0.6897\n",
            "Epoch 411/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5492 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5588 - acc: 0.7148\n",
            "Epoch 00411: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5467 - acc: 0.7119 - val_loss: 0.5477 - val_acc: 0.7159\n",
            "Epoch 412/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5428 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5604 - acc: 0.6934\n",
            "Epoch 00412: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5440 - acc: 0.6945 - val_loss: 0.5530 - val_acc: 0.6953\n",
            "Epoch 413/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.7047Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6934\n",
            "Epoch 00413: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.5462 - acc: 0.7064 - val_loss: 0.5611 - val_acc: 0.6935\n",
            "Epoch 414/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5705 - acc: 0.7012\n",
            "Epoch 00414: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 402ms/step - loss: 0.5417 - acc: 0.6965 - val_loss: 0.5618 - val_acc: 0.7009\n",
            "Epoch 415/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5379 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5570 - acc: 0.7012\n",
            "Epoch 00415: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5388 - acc: 0.7074 - val_loss: 0.5699 - val_acc: 0.6991\n",
            "Epoch 416/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5274 - acc: 0.7047Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5590 - acc: 0.6953\n",
            "Epoch 00416: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5287 - acc: 0.7041 - val_loss: 0.5892 - val_acc: 0.6897\n",
            "Epoch 417/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5457 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5507 - acc: 0.7070\n",
            "Epoch 00417: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5459 - acc: 0.7083 - val_loss: 0.5777 - val_acc: 0.7009\n",
            "Epoch 418/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5325 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5599 - acc: 0.7012\n",
            "Epoch 00418: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5312 - acc: 0.7046 - val_loss: 0.5805 - val_acc: 0.6972\n",
            "Epoch 419/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5717 - acc: 0.6973\n",
            "Epoch 00419: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5503 - acc: 0.6846 - val_loss: 0.6018 - val_acc: 0.6935\n",
            "Epoch 420/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5349 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5756 - acc: 0.6777\n",
            "Epoch 00420: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5357 - acc: 0.7219 - val_loss: 0.6086 - val_acc: 0.6729\n",
            "Epoch 421/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5448 - acc: 0.7042Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5577 - acc: 0.7031\n",
            "Epoch 00421: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5493 - acc: 0.6997 - val_loss: 0.5732 - val_acc: 0.6991\n",
            "Epoch 422/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5589 - acc: 0.6973\n",
            "Epoch 00422: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5435 - acc: 0.7000 - val_loss: 0.5773 - val_acc: 0.6953\n",
            "Epoch 423/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5327 - acc: 0.7003Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5658 - acc: 0.6992\n",
            "Epoch 00423: val_loss did not improve from 0.54002\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5323 - acc: 0.7004 - val_loss: 0.5628 - val_acc: 0.7047\n",
            "Epoch 424/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5603 - acc: 0.6914\n",
            "Epoch 00424: val_loss improved from 0.54002 to 0.53319, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5450 - acc: 0.7064 - val_loss: 0.5332 - val_acc: 0.6972\n",
            "Epoch 425/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5427 - acc: 0.6960Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5496 - acc: 0.6934\n",
            "Epoch 00425: val_loss improved from 0.53319 to 0.52742, saving model to model_1_benmal_best.h5\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5428 - acc: 0.6960 - val_loss: 0.5274 - val_acc: 0.6991\n",
            "Epoch 426/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5434 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5706 - acc: 0.6953\n",
            "Epoch 00426: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5395 - acc: 0.7084 - val_loss: 0.5723 - val_acc: 0.7009\n",
            "Epoch 427/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5370 - acc: 0.7057Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5704 - acc: 0.6895\n",
            "Epoch 00427: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5386 - acc: 0.7021 - val_loss: 0.5479 - val_acc: 0.6953\n",
            "Epoch 428/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5262 - acc: 0.7216Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5575 - acc: 0.6816\n",
            "Epoch 00428: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5249 - acc: 0.7209 - val_loss: 0.5656 - val_acc: 0.6785\n",
            "Epoch 429/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5537 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5572 - acc: 0.6953\n",
            "Epoch 00429: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5551 - acc: 0.6826 - val_loss: 0.5591 - val_acc: 0.6953\n",
            "Epoch 430/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5491 - acc: 0.7031\n",
            "Epoch 00430: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5486 - acc: 0.6955 - val_loss: 0.5630 - val_acc: 0.6972\n",
            "Epoch 431/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5402 - acc: 0.6957Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5590 - acc: 0.6875\n",
            "Epoch 00431: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5419 - acc: 0.6936 - val_loss: 0.5584 - val_acc: 0.6897\n",
            "Epoch 432/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5379 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5728 - acc: 0.6719\n",
            "Epoch 00432: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5390 - acc: 0.7099 - val_loss: 0.5750 - val_acc: 0.6729\n",
            "Epoch 433/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.6938Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5705 - acc: 0.6758\n",
            "Epoch 00433: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5502 - acc: 0.6953 - val_loss: 0.5722 - val_acc: 0.6785\n",
            "Epoch 434/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5309 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5756 - acc: 0.6914\n",
            "Epoch 00434: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5317 - acc: 0.7053 - val_loss: 0.5709 - val_acc: 0.6916\n",
            "Epoch 435/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5344 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5473 - acc: 0.7031\n",
            "Epoch 00435: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5321 - acc: 0.7129 - val_loss: 0.5558 - val_acc: 0.7028\n",
            "Epoch 436/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5505 - acc: 0.6944Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5599 - acc: 0.7070\n",
            "Epoch 00436: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5482 - acc: 0.6965 - val_loss: 0.5772 - val_acc: 0.7047\n",
            "Epoch 437/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5510 - acc: 0.6816\n",
            "Epoch 00437: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5369 - acc: 0.7029 - val_loss: 0.5753 - val_acc: 0.6804\n",
            "Epoch 438/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5371 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5505 - acc: 0.7109\n",
            "Epoch 00438: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5400 - acc: 0.7089 - val_loss: 0.5433 - val_acc: 0.7103\n",
            "Epoch 439/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5316 - acc: 0.7066Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5547 - acc: 0.6934\n",
            "Epoch 00439: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5313 - acc: 0.7079 - val_loss: 0.5732 - val_acc: 0.6897\n",
            "Epoch 440/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5495 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5598 - acc: 0.6914\n",
            "Epoch 00440: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5501 - acc: 0.7019 - val_loss: 0.5692 - val_acc: 0.6879\n",
            "Epoch 441/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5395 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5740 - acc: 0.6973\n",
            "Epoch 00441: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5381 - acc: 0.6940 - val_loss: 0.6044 - val_acc: 0.6935\n",
            "Epoch 442/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5497 - acc: 0.7177Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5674 - acc: 0.6758\n",
            "Epoch 00442: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5470 - acc: 0.7183 - val_loss: 0.5525 - val_acc: 0.6766\n",
            "Epoch 443/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5691 - acc: 0.6914\n",
            "Epoch 00443: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5375 - acc: 0.7039 - val_loss: 0.5522 - val_acc: 0.6953\n",
            "Epoch 444/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5482 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5913 - acc: 0.6875\n",
            "Epoch 00444: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5447 - acc: 0.7049 - val_loss: 0.5626 - val_acc: 0.6879\n",
            "Epoch 445/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5327 - acc: 0.7029Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5867 - acc: 0.6875\n",
            "Epoch 00445: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5372 - acc: 0.6995 - val_loss: 0.5669 - val_acc: 0.6879\n",
            "Epoch 446/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5461 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5748 - acc: 0.6914\n",
            "Epoch 00446: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5468 - acc: 0.6925 - val_loss: 0.5562 - val_acc: 0.6953\n",
            "Epoch 447/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5263 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5600 - acc: 0.7207\n",
            "Epoch 00447: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5237 - acc: 0.7064 - val_loss: 0.5424 - val_acc: 0.7215\n",
            "Epoch 448/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5360 - acc: 0.7016Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5686 - acc: 0.6934\n",
            "Epoch 00448: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 0.5358 - acc: 0.7012 - val_loss: 0.5494 - val_acc: 0.6916\n",
            "Epoch 449/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5488 - acc: 0.6968Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5524 - acc: 0.6895\n",
            "Epoch 00449: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5476 - acc: 0.6992 - val_loss: 0.5350 - val_acc: 0.6897\n",
            "Epoch 450/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5248 - acc: 0.7063Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5732 - acc: 0.6816\n",
            "Epoch 00450: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5225 - acc: 0.7061 - val_loss: 0.5551 - val_acc: 0.6822\n",
            "Epoch 451/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5617 - acc: 0.6914\n",
            "Epoch 00451: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5513 - acc: 0.7019 - val_loss: 0.5399 - val_acc: 0.6916\n",
            "Epoch 452/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5353 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5579 - acc: 0.6953\n",
            "Epoch 00452: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5351 - acc: 0.7134 - val_loss: 0.5442 - val_acc: 0.6935\n",
            "Epoch 453/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5391 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5627 - acc: 0.6895\n",
            "Epoch 00453: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5418 - acc: 0.7004 - val_loss: 0.5461 - val_acc: 0.6935\n",
            "Epoch 454/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5254 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5712 - acc: 0.6895\n",
            "Epoch 00454: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5261 - acc: 0.7084 - val_loss: 0.5746 - val_acc: 0.6935\n",
            "Epoch 455/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7032Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5590 - acc: 0.6895\n",
            "Epoch 00455: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5460 - acc: 0.7007 - val_loss: 0.5546 - val_acc: 0.6916\n",
            "Epoch 456/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5157 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5622 - acc: 0.7012\n",
            "Epoch 00456: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5169 - acc: 0.7158 - val_loss: 0.5635 - val_acc: 0.7047\n",
            "Epoch 457/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.6927Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5659 - acc: 0.6914\n",
            "Epoch 00457: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5430 - acc: 0.6929 - val_loss: 0.5747 - val_acc: 0.6935\n",
            "Epoch 458/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5385 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5570 - acc: 0.6934\n",
            "Epoch 00458: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5388 - acc: 0.7114 - val_loss: 0.5524 - val_acc: 0.6935\n",
            "Epoch 459/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7125Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5548 - acc: 0.7051\n",
            "Epoch 00459: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5317 - acc: 0.7089 - val_loss: 0.5659 - val_acc: 0.7065\n",
            "Epoch 460/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5746 - acc: 0.6816\n",
            "Epoch 00460: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5309 - acc: 0.7124 - val_loss: 0.6036 - val_acc: 0.6841\n",
            "Epoch 461/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5313 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5559 - acc: 0.6953\n",
            "Epoch 00461: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5354 - acc: 0.7193 - val_loss: 0.5681 - val_acc: 0.7009\n",
            "Epoch 462/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5343 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5463 - acc: 0.7031\n",
            "Epoch 00462: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5329 - acc: 0.7069 - val_loss: 0.5489 - val_acc: 0.7028\n",
            "Epoch 463/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5402 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5632 - acc: 0.6895\n",
            "Epoch 00463: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5402 - acc: 0.7021 - val_loss: 0.5614 - val_acc: 0.6935\n",
            "Epoch 464/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5393 - acc: 0.7054Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5743 - acc: 0.6934\n",
            "Epoch 00464: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5365 - acc: 0.7093 - val_loss: 0.5802 - val_acc: 0.6935\n",
            "Epoch 465/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5248 - acc: 0.7224Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5520 - acc: 0.7090\n",
            "Epoch 00465: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5248 - acc: 0.7208 - val_loss: 0.5609 - val_acc: 0.7121\n",
            "Epoch 466/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5481 - acc: 0.7068Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5576 - acc: 0.7070\n",
            "Epoch 00466: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5502 - acc: 0.7056 - val_loss: 0.5707 - val_acc: 0.7065\n",
            "Epoch 467/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5187 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5700 - acc: 0.6914\n",
            "Epoch 00467: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5161 - acc: 0.7209 - val_loss: 0.5796 - val_acc: 0.6916\n",
            "Epoch 468/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5349 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5573 - acc: 0.7012\n",
            "Epoch 00468: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5396 - acc: 0.7034 - val_loss: 0.5541 - val_acc: 0.7047\n",
            "Epoch 469/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5336 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5571 - acc: 0.7070\n",
            "Epoch 00469: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5339 - acc: 0.7026 - val_loss: 0.5738 - val_acc: 0.7065\n",
            "Epoch 470/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5368 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5471 - acc: 0.7129\n",
            "Epoch 00470: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5370 - acc: 0.6960 - val_loss: 0.5751 - val_acc: 0.7103\n",
            "Epoch 471/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5359 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5541 - acc: 0.6992\n",
            "Epoch 00471: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5371 - acc: 0.7049 - val_loss: 0.5558 - val_acc: 0.7009\n",
            "Epoch 472/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5321 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5871 - acc: 0.6855\n",
            "Epoch 00472: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5291 - acc: 0.7104 - val_loss: 0.5793 - val_acc: 0.6879\n",
            "Epoch 473/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5417 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5602 - acc: 0.7109\n",
            "Epoch 00473: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5387 - acc: 0.7124 - val_loss: 0.5457 - val_acc: 0.7140\n",
            "Epoch 474/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5564 - acc: 0.6875\n",
            "Epoch 00474: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5390 - acc: 0.7099 - val_loss: 0.5395 - val_acc: 0.6935\n",
            "Epoch 475/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5314 - acc: 0.7156Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5615 - acc: 0.7051\n",
            "Epoch 00475: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5276 - acc: 0.7173 - val_loss: 0.5366 - val_acc: 0.7103\n",
            "Epoch 476/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5225 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6015 - acc: 0.6680\n",
            "Epoch 00476: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5247 - acc: 0.7059 - val_loss: 0.5700 - val_acc: 0.6729\n",
            "Epoch 477/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5616 - acc: 0.7031\n",
            "Epoch 00477: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 5s 319ms/step - loss: 0.5403 - acc: 0.7049 - val_loss: 0.5372 - val_acc: 0.7084\n",
            "Epoch 478/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5361 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5648 - acc: 0.6855\n",
            "Epoch 00478: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5375 - acc: 0.7124 - val_loss: 0.5353 - val_acc: 0.6916\n",
            "Epoch 479/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5269 - acc: 0.7073Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5658 - acc: 0.6875\n",
            "Epoch 00479: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5292 - acc: 0.7046 - val_loss: 0.5444 - val_acc: 0.6916\n",
            "Epoch 480/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7125Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5499 - acc: 0.7168\n",
            "Epoch 00480: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5403 - acc: 0.7094 - val_loss: 0.5757 - val_acc: 0.7121\n",
            "Epoch 481/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5353 - acc: 0.7011Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5649 - acc: 0.6953\n",
            "Epoch 00481: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5393 - acc: 0.6992 - val_loss: 0.5801 - val_acc: 0.6916\n",
            "Epoch 482/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5558 - acc: 0.6943Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5661 - acc: 0.6875\n",
            "Epoch 00482: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5532 - acc: 0.6958 - val_loss: 0.5571 - val_acc: 0.6897\n",
            "Epoch 483/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5279 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5706 - acc: 0.6836\n",
            "Epoch 00483: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5267 - acc: 0.7139 - val_loss: 0.5497 - val_acc: 0.6916\n",
            "Epoch 484/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5422 - acc: 0.6953Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5741 - acc: 0.6973\n",
            "Epoch 00484: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5380 - acc: 0.6963 - val_loss: 0.5504 - val_acc: 0.7065\n",
            "Epoch 485/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5150 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5519 - acc: 0.6855\n",
            "Epoch 00485: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5213 - acc: 0.7203 - val_loss: 0.5399 - val_acc: 0.6953\n",
            "Epoch 486/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5456 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5549 - acc: 0.7031\n",
            "Epoch 00486: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5437 - acc: 0.7012 - val_loss: 0.5453 - val_acc: 0.7121\n",
            "Epoch 487/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5572 - acc: 0.6973\n",
            "Epoch 00487: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5314 - acc: 0.7089 - val_loss: 0.5378 - val_acc: 0.7009\n",
            "Epoch 488/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5397 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5622 - acc: 0.6992\n",
            "Epoch 00488: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5423 - acc: 0.7031 - val_loss: 0.5648 - val_acc: 0.7028\n",
            "Epoch 489/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5546 - acc: 0.6914\n",
            "Epoch 00489: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5398 - acc: 0.6930 - val_loss: 0.5461 - val_acc: 0.6935\n",
            "Epoch 490/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5321 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5513 - acc: 0.6914\n",
            "Epoch 00490: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5283 - acc: 0.7094 - val_loss: 0.5491 - val_acc: 0.6953\n",
            "Epoch 491/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5363 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5399 - acc: 0.7090\n",
            "Epoch 00491: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5379 - acc: 0.7083 - val_loss: 0.5535 - val_acc: 0.7140\n",
            "Epoch 492/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5303 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5734 - acc: 0.6875\n",
            "Epoch 00492: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5259 - acc: 0.7212 - val_loss: 0.5737 - val_acc: 0.6935\n",
            "Epoch 493/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5573 - acc: 0.6836\n",
            "Epoch 00493: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5393 - acc: 0.7039 - val_loss: 0.5872 - val_acc: 0.6860\n",
            "Epoch 494/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5353 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5405 - acc: 0.7090\n",
            "Epoch 00494: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5357 - acc: 0.6935 - val_loss: 0.5508 - val_acc: 0.7065\n",
            "Epoch 495/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5386 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5597 - acc: 0.6953\n",
            "Epoch 00495: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5353 - acc: 0.7094 - val_loss: 0.5746 - val_acc: 0.6916\n",
            "Epoch 496/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5284 - acc: 0.7083Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5716 - acc: 0.6934\n",
            "Epoch 00496: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5299 - acc: 0.7084 - val_loss: 0.5801 - val_acc: 0.6916\n",
            "Epoch 497/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5358 - acc: 0.7115Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5649 - acc: 0.6992\n",
            "Epoch 00497: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5339 - acc: 0.7129 - val_loss: 0.5991 - val_acc: 0.6953\n",
            "Epoch 498/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5373 - acc: 0.7027Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5620 - acc: 0.6875\n",
            "Epoch 00498: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5369 - acc: 0.7037 - val_loss: 0.5790 - val_acc: 0.6841\n",
            "Epoch 499/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5494 - acc: 0.6934\n",
            "Epoch 00499: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5270 - acc: 0.7139 - val_loss: 0.5578 - val_acc: 0.6916\n",
            "Epoch 500/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5381 - acc: 0.7003Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5656 - acc: 0.6895\n",
            "Epoch 00500: val_loss did not improve from 0.52742\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5402 - acc: 0.7019 - val_loss: 0.5767 - val_acc: 0.6860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o266TEMY4GRa",
        "colab_type": "code",
        "outputId": "70646fa8-8949-4dc4-cb86-9a647b079a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# History of accuracy and loss\n",
        "tra_loss_1 = history_1.history['loss']\n",
        "tra_acc_1 = history_1.history['acc']\n",
        "val_loss_1 = history_1.history['val_loss']\n",
        "val_acc_1 = history_1.history['val_acc']\n",
        "\n",
        "# Total number of epochs training\n",
        "epochs_1 = range(1, len(tra_acc_1)+1)\n",
        "end_epoch_1 = len(tra_acc_1)\n",
        "\n",
        "# Epoch when reached the validation loss minimum\n",
        "opt_epoch_1 = val_loss_1.index(min(val_loss_1)) + 1\n",
        "\n",
        "# Loss and accuracy on the validation set\n",
        "end_val_loss_1 = val_loss_1[-1]\n",
        "end_val_acc_1 = val_acc_1[-1]\n",
        "opt_val_loss_1 = val_loss_1[opt_epoch_1-1]\n",
        "opt_val_acc_1 = val_acc_1[opt_epoch_1-1]\n",
        "\n",
        "# Loss and accuracy on the test set\n",
        "opt_model_1 = models.load_model('model_1_benmal_best.h5')\n",
        "test_loss_1, test_acc_1 = model_1.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_test_loss_1, opt_test_acc_1 = opt_model_1.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_pred_1 = opt_model_1.predict([test_images, test_labels])\n",
        "pred_classes_1 = np.rint(opt_pred_1)\n",
        "\n",
        "print(\"Model 1\\n\")\n",
        "\n",
        "print(\"Epoch [end]: %d\" % end_epoch_1)\n",
        "print(\"Epoch [opt]: %d\" % opt_epoch_1)\n",
        "print(\"Valid accuracy [end]: %.4f\" % end_val_acc_1)\n",
        "print(\"Valid accuracy [opt]: %.4f\" % opt_val_acc_1)\n",
        "print(\"Test accuracy [end]:  %.4f\" % test_acc_1)\n",
        "print(\"Test accuracy [opt]:  %.4f\" % opt_test_acc_1)\n",
        "print(\"Valid loss [end]: %.4f\" % end_val_loss_1)\n",
        "print(\"Valid loss [opt]: %.4f\" % opt_val_loss_1)\n",
        "print(\"Test loss [end]:  %.4f\" % test_loss_1)\n",
        "print(\"Test loss [opt]:  %.4f\" % opt_test_loss_1)\n",
        "\n",
        "print(classification_report(test_labels, pred_classes_1, digits=4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1\n",
            "\n",
            "Epoch [end]: 500\n",
            "Epoch [opt]: 425\n",
            "Valid accuracy [end]: 0.6860\n",
            "Valid accuracy [opt]: 0.6991\n",
            "Test accuracy [end]:  0.6667\n",
            "Test accuracy [opt]:  0.6488\n",
            "Valid loss [end]: 0.5767\n",
            "Valid loss [opt]: 0.5274\n",
            "Test loss [end]:  0.6120\n",
            "Test loss [opt]:  0.6331\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7701    0.6575    0.7094       219\n",
            "           1     0.4966    0.6325    0.5564       117\n",
            "\n",
            "    accuracy                         0.6488       336\n",
            "   macro avg     0.6333    0.6450    0.6329       336\n",
            "weighted avg     0.6748    0.6488    0.6561       336\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PclCfat_4Qr4",
        "colab_type": "code",
        "outputId": "4307a133-aecd-413b-908e-6753dc0e55e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "# Model accuracy\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 1 accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(epochs_1, tra_acc_1, 'r', label='Training set')\n",
        "plt.plot(epochs_1, val_acc_1, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_1, val_acc_1[opt_epoch_1-1], 'go')\n",
        "plt.vlines(opt_epoch_1, min(val_acc_1), opt_val_acc_1, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_acc_1, 1, opt_epoch_1, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Model loss\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 1 loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(epochs_1, tra_loss_1, 'r', label='Training set')\n",
        "plt.plot(epochs_1, val_loss_1, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_1, val_loss_1[opt_epoch_1-1], 'go')\n",
        "plt.vlines(opt_epoch_1, min(val_loss_1), opt_val_loss_1, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_loss_1, 1, opt_epoch_1, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAHnCAYAAABHfw/FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd5gUxdbG38kzOzubl7wEQYkKAoqC\nSFIxgQjqNYuCeBGz16wXzHpNiJ8IigkDKmYFVEyYUAEFRckSlrTsLhsmx/7+qK7uqg4zswtL0Po9\nj8/u9HSo7sV+6z116pRFkiQJAoFAIBAIDiqs+7sBAoFAIBAIGo4QcIFAIBAIDkKEgAsEAoFAcBAi\nBFwgEAgEgoMQIeACgUAgEByECAEXCAQCgeAgRAi4QCAQCAQHIULABYKDkM8//xwWiyXr/b/++mtY\nLBYkEokmbJVAINiXCAEXCJqAwYMHw2KxYObMmdx2v98Pn88Hi8WC9evX76fW6QmHwzj77LNx6KGH\nwmq14s4779zfTRIIBBkQAi4QNBHdunXTCfgrr7yCdu3a7acWmWOxWNC/f388++yzOProo/d3cxpF\nPB7f300QCPYpQsAFgiZixIgRqKiowE8//aRse+aZZ3DFFVfo9p03bx769OmD/Px8HHbYYXj00UeR\nSqWU75ctW4Z+/fohNzcXffv2xW+//aY7x+zZs9GzZ0/k5+eje/fueOONN7Juq9vtxvXXX48hQ4bA\n7XZndczXX3+N/v37o7i4GIWFhRg6dCiWL1/O7bN48WIMHToUJSUlKCoqwpAhQxAOhwEAu3fvxpVX\nXokOHTrA5/OhS5cu+PTTTwEAY8eOxYUXXsida/DgwVxkwGKx4IknnkD//v3h9XrxzjvvYOXKlRg2\nbBhKS0uRn5+Pfv364csvv+TOs2rVKowcORItWrRAfn4+jjnmGJSXl+P5559Hx44dwVaXjkajKCkp\nwfvvv5/1sxQI9hVCwAWCJsJut2P8+PGYMWMGAOC7775DfX09TjvtNG6/JUuW4Mwzz8Qtt9yC6upq\nzJkzB48//jimTZsGAKivr8fJJ5+Mk046CdXV1Zg9ezamT5/OneOll17CnXfeieeffx41NTWYOXMm\nJkyYgO+++67J7s/hcODRRx/Fjh07sGXLFnTq1AlnnHEGYrEYAOCPP/7A0KFDMWbMGGzZsgU7d+7E\n5MmTYbVaIUkSRo0ahU2bNmHRokWor6/H/PnzUVZW1qA2zJw5E7NmzUIgEMAZZ5wBALj11luxZcsW\n7Nq1C6eccgrOPPNM7Nq1CwBQUVGBgQMHokePHli7di12796Np556Ch6PB+eddx6qq6vx+eefK+d/\n++234Xa7MWLEiL301ASCvYgkEAj2OoMGDZLuuOMOqby8XPL5fFJNTY10/vnnS/fee6+0ceNGCYC0\nbt06SZIkacKECdKoUaO44x9//HGpc+fOkiRJ0quvvio1a9ZMSiQSyvfTpk2T2P99Dz/8cGnGjBnc\nOcaPHy+NGzdOkiRJ+uqrryQAUjwez7rtDWX37t0SAOm3336TJEmSJk2aJJ122mmG+y5ZskSyWCzS\nrl27DL+/5JJLpAsuuCBtuwDo7tmI/Px86cMPP5QkSZIeeeQRqXv37qb7XnPNNdJZZ52lfB44cKA0\nefLkjNcQCPYHwoELBE1ImzZtMGTIEDz66KP44IMPMG7cON0+5eXl6NixI7etU6dO2LJlCwBg69at\nKCsrg81mU77v0KEDt/+6detw4403oqCgQPlvzpw52L59exPcFeG3337DiBEj0Lp1a+Tl5Sltom53\n48aN6Ny5s+GxGzduRGFhIUpLS/eoDdrnsGXLFpx77rlo27Yt8vLyUFBQgPr6+qzaBAATJ07Ehx9+\niIqKCqxatQo//PADxo8fv0dtFAiaCiHgAkETM3HiRDzwwAM45ZRT0LJlS933ZWVl2LBhA7dtw4YN\naNu2LQDSCSgvL0cymVS+37RpE7d/ixYtMH36dNTW1ir/BQIBzJ8/f+/fkMzZZ5+Njh07YuXKlaiv\nr8fGjRsBQBlDbt++PdauXWt4bPv27VFTU4OqqirD730+H4LBILfNqDNitfKvsMsvvxypVApLlixB\nfX09ampqkJeXx7Vp3bp1pvfUpUsXDBgwAC+++CJmzpyJ0047DW3atDHdXyDYnwgBFwiamOHDh2Ph\nwoV44oknDL+/7LLLMG/ePLzzzjtIJpP49ddf8cgjj2DChAkAgNNPPx3JZBL33HMPotEoVq9ejSef\nfJI7x3XXXYd7770XS5YsQSqVQjQaxZIlS7Bs2bKs2xmNRhGJRJBKpZBMJhGJRJTxbCPq6uqQl5eH\n/Px87N69GzfeeCP3/cSJE7Fw4ULMmDED4XAY8XgcixYtQjQaRd++fdG/f39ceuml2Lp1KwDijlet\nWgUA6Nu3L7766iusXr0a8XgcU6dOVToI6airq0Nubi4KCwsRDAZx2223IRAIKN9ffPHF2Lp1K+66\n6y74/X4kk0ksXbqU60hceeWVePbZZzF79mzDhEOB4IBhf8fwBYK/I+nGkbVj4JIkSR988IF05JFH\nSj6fT+rYsaP00EMPcWPeP/30k9S3b1/J6/VKffr0kR577DFJ+7/vq6++KvXu3VvKz8+XiouLpUGD\nBkmLFi2SJCm7MfB27dpJALj/Bg0aZLr/vHnzpC5dukg5OTnSoYceKr311lsSAGnhwoXKPt9++610\n/PHHSwUFBVJhYaE0dOhQKRQKSZIkSVVVVdLll18utWnTRsrNzZW6dOkiffrpp5IkSVI8HpeuuOIK\nqbCwUGrevLk0efJkwzFw9lr0OfXu3VvKycmR2rZtK02bNk1q166d9Nxzzyn7/P7779Ipp5wiFRcX\nS/n5+dIxxxwjlZeXK9/H43GpZcuWUrt27aRkMml6/wLB/sYiScycCYFAIBCgX79+GDlyJO644479\n3RSBwBT7/m6AQCAQHEjMnz8fK1euxLx58/Z3UwSCtAgBFwgEApmysjKEw2HMmDEDJSUl+7s5AkFa\nRAhdIBAIBIKDEJGFLhAIBALBQYgQcIFAIBAIDkL+1mPgLpdrjys9CQQCgUCwv6isrEQ0GjX87m8t\n4KWlpUqRCIFAIBAIDjbSVQIUIXSBQCAQCA5ChIALBAKBQHAQIgRcIBAIBIKDECHgAoFAIBAchAgB\nFwgEAoHgIEQIuEAgEAgEByFCwAUCgUAgOAgRAi4QCAQCwUGIEHCBQCAQCA5ChIALBAKBQHAQIgRc\nIBAIBIKDECHgAoFAIBAchAgBFwgEAoHgIEQIuEAgEAgEByFCwAUCgUAgOAgRAi4QCAQCwUGIEHCB\nQCA4QJAkCZIk7e9m7BlvvAHMnbvXT3vQP5cmQAi4QCAQHCAc/szhGPzy4P3djD3jvPOAc87Rb4/H\ngR07GnXKZ5Y8A+s9Vmz3b29cm0IhoKqqccca8d57wHff7b3zNRIh4AKBQHCA8EflH/hm8zf7uxlN\nw4QJQKtWwIYNDT500vxJAIBvN3/bsAOXLwcsFsDrBUpLG3xdRCLAokX67aNHAwMHNvx8exkh4AKB\nQCBoel56ifxct67Bh0og4XOLxdKwA19/nf+cSDTs+AkTgMGDgQULGnbcPkIIuEAgEPzT2bED2LZt\n31zLbm/0oRZkIeAff0xc98aNQDLJfxeJNOyCn39Ofq5erW6Lxxt2jiZECLhAIBD802nVCmjTZt9c\nKxsB37EDuPdevQBnw5VXkp9z5+65gFtliUyl1G3BYMPb1EQIARcIBIKDgVtvBUaO3N+t2HOyEfDR\no4H//lcNu8tkFUIPh8nPnBx9yJx+R5Ek4K+/zM9FBZzNgBcCLhAIBIIG8fDDwEcf7e9W7Dk2W+Z9\naLb67t0NPz8Vabebd86A3oHPnAl07Gg+7Y12GNjzBAINb1MTIQRcIBAItIwbB9x3X9a7S5KEDbs3\nICWlMu+8pzTlfOg0597u345YMgYAqApVIRDLIGRm4e9swuLUpWsctOkYeE2N+nsoRH7G45lD6G++\nSX7SsW4twoELBALBQcYLLwB33ZX17gvWL0Cnpzrh3kX3NvqSWYt/NNroa2QkFjPcXBmsRKdpnfDk\nj08CAEofKUW3p7s16lxZZYKbCLghs2YBRUXAZ5+Rz1RsA4HMIXTaRpfL+NyZxsD3c3EZIeACgeCf\nSSAAnHUWsGbNHp9qcfliAMC7q99t9DkSqSynOFGH2RSYhIc31GxAOBHGlrotyrby+vL05zLraGQj\nyg6H4b5JycC9v/AC+TlvHr89ENALttaB0zaaCXimELpZJ2UfIQRcIBD8M5k+HXjnHWDUqD0+ldVC\nXqUNDaF/uOZDvLuKiH7WAt6UIVwTAacV0KLJaPqSpqzQNYEDjycNpnDRfWw2fajb7+f3ZQR89orZ\n+CpfHmOnHQaWH38EKirI78x5v9m1BC/20p8P8Tjw4Yf71JULARcIBP9M6It/L8zrbayAn/HGGRjz\n1hjSDCNxMqIpBdzk3KyAx1Np2smOOZs58IaMgWv+Nsq1f/kFuO460mGg57Pbgbo6dedAQC/gjCO/\n5P1LMPT4jeSD1pmHQsCxx6odGqZjMmjTZFxG+3ysw7/lFuCMM4Bnn818f3sJIeACgeCfTUOrexlA\nBTyZasS8ZRnWgac9DxXZnTuBrVvN9wuHyfzu//0v+0ZkcuCJqJLIZgjrmLNx4JJEnv+55/L7ZHLg\nffoATz4JLF6sCrjNBlRW8veivR+zeeB+P/Dnn8CuXcb7GdyLxO7ncABPPEF+X7XK+BpNgBBwgUDw\nz2Qvhjo5B/7KKw2e7hVJRDhnmzacTgW8ZUugrMx8v/JyMh3rlluyb0g2DjxdpCAbB86KMh3Pp9ng\nFDMB17r/ZJIPobOLpQQCWGWpgmUK8G5XeRsVXG3b6uuB7t3x3sntYLnbgt+3/6p8dcXpgMVxvy66\nErPJ56up4dtp3XeyKgRcIBAIWLRzh7PAKk9jSoWCwMUXZy648ssvQN++ysfqUDUnjGnD1NmG0FnX\nmK6zworu3nTg2Qi42QphdK64NoRu1Hmg7b//fmDQIHV7MIh3m1UDAG46Ud4mh7yT33zNn6OcJOTd\nczQR+NdXv6189az8ZwrF+eTBqF0+36ZN/LmEgAsEAsF+ohHlO60/LwUApKqzXLLy9NOBZcuUj9Xh\nas516xy4dhpTNm1ks9XL02SMsyKZjQOPpylH2tAQenW18uu2+m34/C95PjZ14Joscl3HJhw2fxaB\nAIrqyf67PWRTMhzCnN/nILjgQ2W3N7sDyc0b+WOT+giIP8qPp0epA9+oOTabQjV7CSHgAoHg708g\nANTWGn+nHQPPdsWqtWuBMWOA+npYk0RgpWyH0zXrYleFqjhx0jlN1s2GQpzwmcKK8a9ySFiS9G6c\nFXAzB76dLOYRTUQRC9abXzNbB75qFdClC7BkibK569NdceIrJ6I6VK2KoGbKnO65mHVmOnUC/H74\n6kkbamUBvz84H+e/ez7urn1f2fXcs4HHOuw0vw8Zf8zPbY/aQcbOf/iB31FkoQsEAsFepEULoLCQ\n32YWKs/WgY8ZA7z7LjBtGqzyuZLI4uVt8IKvDvEOXOc0WTcbDPIdgHicdCbeeYc/hhW/igpyjNWq\nrzCXQcDDsRBqHOSZRJNRxENpKrCZOXD2WSeTwI03kvn3116rbPbHiMMNxpgCLBoB10UmzAQ8Lw/Y\ntQtxjcItSpC1yNdaa7jt64vIT9oBkwzqo/uXfg/83/8pn6M2APfcAzz2GL9jU87T1yAEXCAQ/P0x\nCg3vyTxlQBW+REJx4KlsBFwOuaYYt14VquLcpU6oWDcbDJIMdEooBHTuTIrS1Nfz+1H8ftWF//e/\nuvu493hgymDmmEQCOO884OOPsX2bmlUdjUcQC2mmZgEklHz11XxRHNrmRAI480x1eyKhRj0MXHrq\n00/Uv43WgWs7NuPGAZs389suuQTIzQUqKxHRrJuyUyJtL6zlr/tcH+DGk5gN2nMC8P/7MmDRIvX2\n7AC+/163376slS4EXCAQHPz85z9kUYqGQDOSGxpC/+03Evqlx0lSwwRcTtyKMUOl1Uu+RpwJTSti\n/uOPJNysFXDWgbMix4bW2e319eYRh0QC/x0K3D0Yqvj8+ivwxhvAiBHYvuVPZddoJIhYWBXwZCpJ\nnsf11xN3yibvURFetYoUOGGul27qXuKZp80FXBtC187hP/54UpnN6yXtZQQ8ZQEqQDooYav+WTze\nH7DIfz6LwYppfieAtm2Vz1EbAKdTfwNCwAUCgaABPPYYWRayIaUttWU2KWxI1ig827MncPTRnIDb\nEg0Q8GgUf5QCQab4V9VHbyFx793KZ8VpHnss0K2bTsBD2zdjc77BfbACrnXgNXzYWL2YQRIb4/C3\n71zHND2IeFg9bzQZJc9jxgyygRXcdGPgaQQ8unVz9g5cw9YSJ/zxIHHgAOfAd3mBamtE+T0tMXKd\ntcXqJr8LkGrVZxixA6vzYuQvzkY1tMVjmhAh4AKB4O9DppcnK8hmRT1YB86KWyzGjzOzK1XJ5zUd\nA1+wgIjWt99iSeUK9JgEjGUquO72APE/flebkCGEfkroWbS/HqjKAS9y7LQsrQM3mrIlSZC2bVM/\n0+dHM6sdDmyvJr9bU0AkHkYsrDrMaF4aJYzFSFj/iCP47RkEPBKqV9uhGfqI11YDjz9ueFzSApQd\n8Tn6PtcXKCbKywr4D8yU+cpMAh4K4ssOQOer1U1+J5BgstWfOQroehXwQh8LUFqq7igcuEAg+Nuw\na9e+W4Ix08uTCuHrrwPPP2+8j5mAP/ssESQK48CTsuByDpwNWdMqXS+9hD/9JJHq485Ms2xAItss\n9GAQ3zjItK6qHPDPNp0DNxLwqVMRHnKc+vl3uROxfj352bw5ttcRgS+rB6KpGOIRxoHrI818m7WJ\ndQDp7KRz4HYAW7bo7wFAfNkSkgBnQEiOaKytXgu0aweAF/A3u6u/V+YYXztJFTEUwk+t+e/8LiCy\neYPy+UP577egs5UkzVGEgAsEgr8NvXsDN9ywb66VyYFTIbzgAnWbdmyYdemsgGvPTR14MqmUPmUT\n0zjRpZ0Cux22uD4sH7MBcWalrUQqgS/WforpRxmci3HWFgn8eHg6B86KOw1R338/6tiFuH79ldQT\n3yAL1dat2L70KwBAhxogKiUQYwU8zZTnv0LbcPOJQEKrMhoHHrEzhVboOelzr6gALrtM+S5uMIWt\nKge47mSNq5bHqlkBf6sH4EpZUYwcVJk48LC8fyochk0TTPE7gcgu9VnT+0rabfzcbyHgAoHgb0Es\nBmzbpjqqpiaTgBuFzbVjtWYO3OPh96MiFA4bC/gNN6hzz+k5HQ7YEsYCrp1GdsKckzHpNHkDO37N\nuNKUBXzGNCvgdD+LRe/A6e/V1ah1Mw1JpYD33weWLlU2bfcBBWGgIAJELUnEo+qYezoHflr9M3hk\nAPBGD80XGgH/3wDg0QHq19pzJl96Ufk9Xq+fy3/5CODJY4DbhjEbZQeu7WD0DvjQOmFiv6EKftQO\n2DX9Or+L7xBQAU/ZbHxEQYyBCwSCvwVURMzGm/cGrOBm68BZtG0zc+Da/eh87mBQCaFzhVxmzAAm\nTODPk9aBMwLOhNBTFvAum2lH3AZewI2y0Fu00I+B00U7AF7AAWDsWO777T6glR9wuXKQsgChiPqM\nOYE87TSwVCSJW67XLrWtEfD3uvBfa0WXzdaPB/V/3xq5X7WNiWJLbdtCAnTTyPJDSRRoJ4czhFsU\nK22waQS8TiPgccWBW4HDD1e/EA5cIBAciFzy/iWw3G3JfulL+jLLVsBDIfPsZTNY0TZ6ebKFU+Rz\nd7kKaE2j+tq2mTlw2q5evfj9QiEk5dKbSe3QLp0XzYTQrQbT1IxC6MplbeAFnMk6j9rARzeMHHjz\n5noHXlGh3HedVsABtPwPcPgtPgBEwFv3GgiXgyhlIKKGsRW3fMMNwL/+xZ3DniTP3TCEzjyD5S35\nr5VzDh8OXHEFJ+C6c0Gd+sV2FC5deifybwMCmlle3t0B5G/ZBTPCiTDfBoYXegOTh6ifU1oBX7eO\nrC2frrzrXqbJBXzdunXo378/DjvsMBx11FH4448/dPu8+OKL6NWrl/JfSUkJRo8erXz/8ccfo0uX\nLjj00EMxevRo1NenKeUnEAiajNkrZgOQX3Q1NcBrr6UvHWnkwGtrzad7eb2kxGZDYNeAfvxxNQGL\nYlDec00JsJ06toYKuI8Im3JvwSCSCXI/Ka2A797Nn1OSYIuZhNAZAY8vV+ukR+wwdeAxrQOnIh2J\nkKVG3W6goEDvwOvrlTC5zoED2JkLrPT44XcCARfQqvQQuJ0k9OyPqZ0kxZH6fICLt9pUwLUuGImE\n0gmJGYyhKw7c6QRatyZRBpm4wf5W+Z+fnxHrl3+bDb+L3AeLN0aGAsyIJCJKG8IO/fdzDtdvS9pk\nGe3UCSiSS7rto6TNJhfwK664AhMmTMDatWtxyy23YOzYsbp9Lr30Uixfvlz5r0WLFrhATjIJBAIY\nN24c3n//faxbtw6tWrXCvffe29TNFggEmRgzBrjwQuDjj8330Qp4LEZKmp58srpPMkkywqmz1K7u\nlAlWwL//HjjqKP57trNgFAmIx83nfsdiwIoVwFdfqcfSjGN63SwEPJiK4rnewLfxDfgqtgZaYjZw\nZT8Tt6lLgM45HNhZtQkA8GUHYJVDHQf+qDOwJL4JDw8A7h4EfIctRKhPPZWsle1wkPb6/XykIhxW\nSoCyAq5t/3a5r9LK1wouN8n8CgTV8fh1RSQELvl8mB38gXPBdnmo4LfmwKRTgXevHEJy9JNJ5W/t\nN6iDorhfpxNo1YoPodNnxBSMUQRcG6oHUNGtLffZm7SmFXC6ZGjUrma1ZyJpZ/5wXboAAwboC8w0\nEU0q4Lt27cLSpUtx4YUXAgDGjBmD8vJyrNf2kBl++ukn7Nq1CyPlP9CCBQtw5JFHoovcK7/yyisx\nZ86cpmy2QPD3J5UiZSg/+aRRhydTSbIkJqBbmIODhrSjUeII584ln7/6St1n6lRg/Hjgiisa1RZO\nwAH9oiVsSD4aNa60xoqb1oH36gUMHaqeJz+fv046AZfHoW/vvgMTRgLHF3+Ap1KLdZeP2fjwMCvm\nk04DBrT/AhKAYZcA3U5V5yI/fBxw9JlVuPVEYMoQ4KrOG8icZPp8/X7ijuNx0hY2evDRR0CLFlwW\netzJW1xOwD258ilVJ3/ZKGD0ucCztl9xyZYn8TTTd5LkjtDrhwPTjwbObrYIXScBm1O7FQHXjY8D\niHrlHgV14Oxzoc374ANlGxVwbbgcACoS/L8Frz0HeVmM0ERtakZ6JlLs8qE33QR8950yD72paVIB\nLy8vR8uWLWGXy9JZLBa0bdsWW9JkpD7//PO46KKL4HCQ7s+WLVvQTs4oBID27dtjx44dSGRbr1gg\nEOjZtImUnDzllEYdTp1KRlgH3qkTcewUGnqnU5Z+/rlRbdEJOEs0qhdwozH2wkLVeWcbQqeEQkgk\nyH5JozdqKoVVvvSqEbPx4eGgRoz+yokaCpQWw33YOco0xLttG7nfHj04B54oyOfG8XkBJx9YB055\nLUZC/n/K9UwkABVy+Jom9qWQwoYi4GTve5DCpGNj5JojOfJNyAJu6MABYPp04OabYZVzEozGreuj\n/HCr15WL3CyK9Wkd+ANDH0CboLGiJ23ZLkG398myj7FvCAaDeOONN/Djjz826vjHH38cjzNVegJ7\nORswmoiS0oEyDqsDHocH4XiYK/HnsrngsrsQjAWRZMa13HY3nDYnArEA9wLMceTAbrXr/7E5vLBa\nrMoqPRSf04eUlEIwzo+z5LnykEgluIXnrRYrcp25iCVjyvgOANgsNnidXnFP/9B7siRCYGUo23ui\n1EZqUQTAAiAcD8MDGN+T/P+gFAnDohVOqxWBZYvhjIbgBCAF/KCvQrY9me4psnsX2GFcyW6HBYB/\n+yb4WndA/OwxoO9iKRyGv75S2TdhVacL+XdshuONt+C+6Tb1+2hYeUn+Gt+Ksf8G5uVZ0Ia5XjLg\nR0JO6pMswJhzgHfeYnaorFSS3MzQhtB3e/T7VJvPfgIAFMXtiNgT+Ko9cPsw4JNXgfwoEM1xQtHJ\noiLM9W7G1MRMfGkDLK1aoJadHu5xQmKEiwp4nisPVk8OEADqI/oO07dhstzomhIShRh4qfF4dcIG\n/CXV43ufG8fBJITukRtgFEJnzzlxIgDA8trvwPrlps8lx5Gj/Lv0OnPhzkLAIxoBL3AXoGXcha3Q\n/x3/tgJeVlamuGW73Q5JkrBlyxa0bdvWcP+5c+eie/fu6Natm7Ktbdu2WLhwofJ506ZNnKtnueGG\nG3ADUzCiTZs2un32hAe/exB3L1LrFY87chxmjZyFqxdcjed/Vas6TR40GVMGT8Hot0bjsw2fKduf\nG/Ecxvcej36z+uHPSnWBgE8u+ATDOw1Hm8fbcC/MlRNXoiy/DPkP5YOl7tY6lNeVo8cz6gRLn9OH\n+tvq8cVfX+Dk19TxxW6l3fDHlX9g9orZuPyjy5XtJ3U8CZ9e+Km4p3/oPXXYDbALJmZ7T5ROT3VC\nTQQoAHDL57dg2qRrje/J8x8AQNhfCyP9eWHisciNAZcBCNdWKfuw1810T48suBN3MedMeN1wADjj\nvu74EoBjrloNLPb5p+i9cCQgRzgjdiiOrN+DHbFyOt++X7b8hKPl3y/wfYpVpcBDll/xf8w+uyr+\nQqqT+vndbiQbXSkEUlmZsYZ3zAa1tjmMBbwqg4A7oglE7MDQseTzwosGYMWG74FfnoSSNVRUhHMG\nAkAVVjYDPt78KoJF6jnKQzvRghEumqF+4isn4tAqACVAIGG+XOaaYqDGDfxg/IoHADglK9a7Q0TA\njULoHsaBFxYi7rACIL2seJdDgW/4Cno0c9yMQnehKuAWJ9xZDE9HbbyAexwe5KecAPTJaUnr/hNw\niyQ17erjgwcPxtixYzF27Fi8/fbbeOihh7CUKRLAMnDgQFxyySUYP368ss3v96Njx4745ptv0KVL\nF1x11VVwu9149NFHM167TZs22Lp16167F+HsxD39Xe7JumYtcnvKA5aSlPU9UWFdNWkVOh96DCy1\ndQg//SQ8V15jfE8zXwSuudqVeUAAACAASURBVAZmJE4YilSzZnC+/gYkmw0WOYzNurxM9xS9/264\n7pyCrXmAIwk0KyqDZcsWBD9+D94Ro3XXTPbuBftI4th2/Q8olZsc/GwecoafDgvzSpS6doVlFVlO\ns//1eVicX4+r7QMw7U6yjGSdC6jIs+HpPklMO0a9hv8BtWOAr7/GcbMH4/s0olYStePakyfjrq9I\nV+Tqn4Cn+vH7fPIKcPJF5ufol2iOpdYKJYz/wbkfYHD7wXBOfQruW+8kG886C5YebwMA3nsDOPqq\nh3DtolvxtlxmdNurzRGrqkCH68jna34Eph0DLP/3cnzy42u4dfkjGPoX8OUh5u1Y/gzQa6L59w7J\ninfed2HEijDmdgPOOYf//pbNZXjoxXKSEzFjBn4+ohj9xpBkwD4t+2DpBF4/jn7uaCzZvsT0eoc3\nOxy/7yIlYl/efCRcP/+Kc882bx8A9NwJtK0jSYIA8OIZL2Le87fg7RL9FLQ+iWZYem9F+hPuAel0\nrMlD6DNnzsTYsWPxwAMPIC8vDy++SKrqjB8/HiNHjlSS1dasWYPly5dj/vz53PE+nw+zZs3CqFGj\nkEgk0KNHD7z88stN3WxDXHbyItHicXjggb7L7HUa1+vLdeYabs9z5WW93WaxGW63W+2G2502J5w2\nfbxK3NM/9J4kPsW2IfcEEIGn8289dmLTlHvavRs45BBg2rSM02ns6zYApc0BQBFv3XUXLQJOOQXe\n5cuBww4DvvgCGDGC1O3u2BGuCAlrlsnBN2k66eh4A8bjzrZflgNyEjM7xclbp3eWVLwBQJLD4Fab\nelDfCcD64iTGL+OPCzkYAd+xw3D+stKeFBCzSbBb1fPWGEztyuTAi+15SEIVEn/UT55jHmOxi9Tf\nzzwXKKq9H/01S26GDBx4iacE+S1ILpKRa2b5vbn5d7YkELelcN2gMEasMHHgbrlB8vBLPM8LgAi4\nUSQjEEs/VMr+f+y1uOBqhANPpBLIl4xvPGIQVt9XNPk0ss6dO2Px4sVYu3Ytli5disPlijWzZs1S\nxJvu5/f74dMmiAAYOXIkVq9ejfXr1+P9999Hfr5xWE8gEGRJQ5bdNIB1wjoWLiSJZZdckrkq1ebN\nmYteXHklyVo+/3xS5vPGG8nnmTPJ98EgX0CF1okwWz6TIfzgPeqHykrzHQGk5HayAr5eDsXXaPpQ\n3BSkBQv0BV4YfFEgZpG44ji7u3fQ7ZdOwB1JwOfgO5x1UTmK4WU6c4yAA8DuhJ+b7xy3pHgBP50U\nKXfanHDJ88AzJdOtbKb+7rF74EgCTlnjiuTg0l9yMwzHwGkmvPxvJ5ajCqdulTZAF+HS4rarvSGv\n1Q1vI5LYEqkECkwEPPx3FnCBQLCfqKkhU6C++EL/XSMEnB1O4LLQtQJMhTMvL7uCFkbrcrMLjNDl\nLpctA848k2SMs9cJhfiFLCIRkj1Oi6iku/Tokep0uMpKrC6WcP1wklB284nAry3UfSVZPKxy/g07\n9qgthsIJ+OzZSDjMV/3IjQExq8SJ0+42+mlI6ZLYvCmbUmhFaVOETKFaaa3CDcPlKnH02TGw06US\nFv5znZ20iY0AGYkuy++MgB/Z4kgs/KI1hsgz37Tjz4YO3Elk6UvbZjzw7QOIu9ULGlUADMYaIOBj\nzoU3SwfOPod4Mo4CGIRFAIQscTzy/SOYt3Ze5hPvZYSACwQHGlu3ErdptPxjtvz4I5lbvWIFKe+o\npaECnkoh/JQ6w4Mu3mF4LiqchYXZ1YU2uk/2nNppYgUF5CczD3u7NnDn92cl4JFEBGgmK05lJQZe\nCkw9Frj1BOCRAUDvf6v7UgdukUPdbJUvbchbO4c44TNfgNrX5hCkLBKX+1ATM1h1q5nxkA4AuCQb\n3C7+GlTAj980GU8cC3zSCfopcNAs0GGReAcuu3inzYkcB+kgaEuvFnvUzoYtxYfQnzzlSRz55Z/4\n9FDyuVz+03lkEfWX6odoog4iS8OOWI47vrwDFXmqTBmF0M0cOB2S4AS851Hw/rLScH+uDc2KEOrS\nEQDQJq8Nzul+DvJNBLzKGsHNn9+M0+ecnvG8exsh4ALBgcbkycCcOcC11zb+HMceC7z7LvndaRSn\nZMaHTfJYE/M/hmSxkPnZCxcifNtNynecA9dOD9u5k/wsKlLC22kzZSsr9WPE9JzapT4BdRlPxoHr\nBLy+XifgIQd0+4UTYaCkRGkHDYUbOUNapCVsJUK+pkT9Lq0DB5BIM9Uo10cEkBWitbvX6farTiPg\nFglKpTRKnZwIWJ8kY/t1buhXVANfMjR+5QRewCOqgPuc5OFpn03bfDU7r3MVUC6PcD447EH0bdUX\nPqcPH5z7AXdM2AGEeh8O/6Wk4iY7/h91WrkpdWt9ameOOnBJklBeV46/av7iOj4sXgd5HpyAO7ym\nuS0s0VQMISmKfq37ofz6cjTPbY4cq/r/ET03QDo9+wsh4ALBgcSsWcBK2SEsW5Z+32xxGNSEZB2u\nQWGTLXVb4FgyAlMGgxR8AS9K3Bi49vjycvLT5wOCQdgnAyelyZ7+3rEDjv8Cr7F1puk5jao20vPT\nn8EgthdrOik//qh2YGQ6XgO0vhGY3VPdFo6HSQ3vvDygslJJzNNVVGO2BSzk2a1jhpMzCXi6ucI+\nFxFGNoNfMujyVPUwT/2WIMHt4XsntVHiwHPsHrVNRgLOOvAxoxGaM1s9R6QWdqsdVotVaSdlWAey\nfmept1TZ1pmZU+6wkoeQklJKR4ClemAfZZYDm7T4vXc3nP9V91vjJQLtSVoVB/7Qdw+h7dS26Dit\no+68lAI3sfus2HqdXtMET5ZoIopQPKREHQDAZiP3c2p1Edcp2J8IARcIDiQuv1ytSEZXstpTjBw4\nK+AGYe5fd/wKALhnMJTFJ1hR4hy4NoROKy2Gw0gGyAv6c/P3LN4tI87zSWYaliLgNFGNhdZK37yZ\nOPRQCNuLNDHr884jLlwer05agJ2y/qxnhFdxb82bAzt3KoVkjBK1aCJaUCIiwo5JZ3Tgad601BFq\nQ8HNNH+WWqQp4i1JcOfw4WgaQvfYXGqb3Hrh4ULoUhKhpHqdumgdXPLx1IFT3jnnHTx58pO4+uir\nlW1snXE6myIYD+Li9y/WXbf6uivgj/phtVg5odwR3Mntt8ZNHkSLmFNx4K+vfB2F7kKlTcdXenHL\nd0DP4u7KcTNPn4mb+9+MG/vfqGzLdeZygv7wCQ/r2gWQUH0wFuTadcHJN+PJBcCbPe9TOgf7GyHg\nAsGBglEoO2ReNCNrMjlwAwHnHEYiAYRCvANPpXHgdM5qMIhIxLiSG4tTPhW3MlU0Sp7Hiy+SKWks\nlZVYUwzM7RglLjwYxHZ5nNQKjdOVy6KyIsU6TqUISPv2wKZNigM3StSi5U2DiOHDzsDPrdXvUpo3\n6azefGEWwxKrMlSEWAcOAC/wUWdDF6sgpeB28wJL91fGrl3IHEJPxrnCKIlUQhFiNvRsgQX57nxc\n0+8alOSoYwkuJiHbYUu/GkhVKgB/zA+f0weL9u/GsMZNOjYty7oinorjr5q/sHLXSozqMgoXHE5C\n8DVd2uGhR5djVI+zlPYN7zQcD5/4MLqUqKvbeR1e7t/25b0vhxnRZBQeh/q8HCefimvmVyP3sn+j\nOMe41vn0JdMNM+WbCiHgAsGBQCBgXKM7i0SsjBitwMVey0DAuTnnySQQDHLCl9aB04VBAgFEwnsg\n4H4/GeceOFB3zIkXkwIga5Z9BoRC2JVLBMBnNShhBl7A2Y5IOM4IeCiklnI1GAOnol6XCuGM84D3\nuprf0/zDgCOYYiaJNNPIaPhYm03dSfOnr4nUoLVNn0UOAHB74C5UhbTQXag48BzZcVbnwNCBcyF0\nTTEeQP23wIbQ2VoFrBt1Mf06o3oGLNWhagRiAeQ6c2GxmD+gaDIKp82JsuaHIRgLYsG6BQCAEw45\nAZOOngQA+He/SUDPnso1rRZe2k445AQApFPBXsuok1HkUUM0JZ4SzZdFgMXCdVpYJs2fhMd+eMz0\nXvY2QsAFgv3Np5+S8eJXXtF/R6dY7d5NCpdkgzbxq7qauNl16wCLhawIxopuMEjGmrdvNz6fLOBp\nx8AliazClEioHYZAAOFo5mlkpgJO29O6te4YOm1s3pqPgGAQARd5KSeguffnSdlN1mWyvysh9Pbt\nAUBx4IYCLm+riuszxI2oZ7QyaTVPdCp0E1HWhtBzNAnXu8O70dPd3vgkublwMc6ypa+lGkKXp5dV\n5cDQgccyCLiRA2eFjxPwhP44gB+Htst/76pQlSrgaRw4QDLdh3ccDgkSXvv9NQBAc29z9GjWA6Hb\nQ5jYl/SWaLhfK+CfXvgp4nfpM9jZNv7n2P+g/PpyvHrmq8q2kzudrDsGgCLgx7Y5Fpuv24zRXdWK\nf2ur16a9l72JEHCBYH/z0Ufk5wMP6L+jAn7HHcDRR6tFStKhDbtHo2Tbm2+q54rFsKkAaHkj8POO\npcCwYWT8XSaWZATeQMA5B/7228QlDxxIBJMOBZgI+M5coN11wOnnA/3HAQ75VKYC3qqV7hwDg+QF\nOq9+KRAKIegg14zLHYspg4GTry7E6pH90eka4JeW6rERoxB6B1I4hcqIdkw7blVXu6qMZS4QwzLy\nPKAiYRz+9jq8isPVCqf7/Y90+5e6TBw4+GGPVr5WqAhW4KbPbiIzCQBUe2Ao4CwnvHIC7v3mXm4b\nFUW71Q6PnBBHE9QAIN+ljhW42RC6vE+eKw/bbtimbG8j/xOuDlcjGAvC6/SmdeAAEczTDzsdFliw\neCtZjjXfTa7rcXiU46kg26z8vHurxcplumvbCJCOSJu8NhjSYYiy7cSOJxq2h06dC8QCaJvfFs1y\n1MnvsdSeFUlqCELABYL9DXWYNDmLhQr4+vXE2aZLbFu3jqzNbVQ8pbpaXRrTbgdiMUzrRxK7rl09\nlSSeMQ48votJJEok9A6cHQOvqgK+J7XBlcxwAIjFEAnpOxwv9wS2FADzDgMWlzG7s+/cl14inQrA\nUMBtHuLoNkjVQDCIoNy2hCzgdw8GPi2uwQPfPoANRcDE09RjuTFwNoQOdW1pbdEUdupUTZYOHCCL\nktB62kb4XD5FIKtCVXDZXLh1wK347tLv4O4/SLd/vklpW4AXcJpY9tHajxCSFx+pMgmhA7xz1sK6\nVBpGZ8PMHocH9w65F59d+JlhCD2RSuCbzd8o25vL/zz9UT+C8SApy5vJgecUo9RbinYF6tLSRolk\ntDOkdeBmsB0Hdt74zNNn4ulTn+aS2Lj2MAIO8MML0UQWC47vJYSAC/7+rFgBvPpq5v32F94001qo\ngO/YQX6mE/DDDiNiRwX85puB++4jv1dXq+tcOxxANKqIVSos719ZCTz4IBAKIXbZJXwbgkEu9JyS\nUoAkoTIHWG08HEgOterLpDo1m+j0LE7An31W/V0W8K15wEb5nR1zyK7SFsfikgjq5LhsCil8xywa\n0txLqopsZ3TPMITejggDDaGz63H/2Ma8+hgrZEasNc51UvA5fYrQ7QzsRElOCR484UEMaDvAsPZ9\ngcOHNia5bKyAj+w8EicccgK2+7crnZTqHCDpMr4Rs9wBgM+HoAl3rXx8p+rO4+/EiR1PNExiC8VD\nGPmGWja7UP4nHUlEEIwFkevMVQT30KJDDdtAQ9Zs0RgjAbdZyD+ibAWchXXoE/pMwJVHXWm6L81b\nUAScydBXStjuA4SAC/7+9OoFXHSRcZLY3mTbtvTfRyLGSWlGpUQpNBxOi6OsXm28HzvuTZPScnOB\nYvmFxzpwpxOIxRQBl+g1tm0Dbr8duP12ft1lv1+XxJasqQYCAXS5Cuh6FTNvWlN/XFuRDNALOHX2\nMbNqo3KEouwG4BB5layoi5w44JTQfzywxamGnwdeph5qJIKGWehyiVEjH3jseODPUoMvQATFkaaU\n+/r0+g6fy8c5XDa7mQ3vUgpSLlxqsPT1mV3O1M1NbuVrBX/Mj4ogWeCk2qOWKdW1o6iF4XaAd+D0\neWoFXPk+iyS2QrnPFIwHEU6EuRD6KZ1OMTymLI+EatjkMTZ0T6Hz5/dUwDNB8wGMHPgO/44GX7ux\nCAEX/H14+21gucHbjdKUAv7GG0CbNsBbb5nvM2wYEdSEJl6ZbqpYOEwSzqrlChlmDtzPZHtTB+71\n8gJOE9ccDk7AU0FNFvrWrbyY1tcDwSAn6qlRZwCpFHbLEcba4/rKv9TyzWc1SJ6TbSbgcStIQRUt\nzZvrCqvECs1DySxVIX2Z1rBRFrrbDdhsMCuqpV2shGK1WOGT/1n5YhZ8zy9VjcoMK4ixDhzgBcpo\nXDg/vxnu/gpY8uMRyrZlE5bhyVOeVELxlFa5RGRplCHs4Mf/uXYwAvTqma9iwQULlM/sealgmQq4\nwRi4liL5kVeHyb9pNoRenFOM1ZPUTmrfVn3x/r/ex5TBU5TvAfLcjTLIaW5GJgGvvKkSG6/dyG1r\niIDT50U7gKwD3+43SQZtAoSACw4+fvgBGDNGL8hnnw0ceaT5cemcbkNZuxY44QTyE1AT0eSqZYb8\n8AMkAP+Zfx03Jpi2XeEwUMGsNbx6NanQNmIEP9ZdV4ekBbjyNGDZTnlxDq2A046DPAauCHhKo6jJ\nJFfKMlVfBwSDSNjVjdrVtda3ycHFZwI7Zbe3o5kHF44GtrFTk6dMAX7/HQ4fL76hf5EM3pgdSnt/\nbk3GreM5LsDh4M6TGjYU0SxXgDIU8BLVuUXkoiVTf3oSM/o7TEdizYQvJaXgKyFilhMHjtYEYXZl\nKPqlc+Ce9DH3gu59YPnoI/Sds0jZ1rtlbzhtTkMHzhK3AUGDIQ2AF6BWvlbo2VwtV8e2zx/1G56b\nYuTArRYrupV2U7bnRck8bfq38TpUBy5JEjqXdFba0620G87ocoYSsqbTurgkSgaam5FJwEtyStC+\noD23rTEOnMJ2gKrD1ftsHFwIuODgY8AAUiZz4UJ1m1HNbC1G86EbS7duZJWvDz8kn6lQZliAZHMB\n8NivT2PQS0yCUiYB38kklK1bBwweDHz8MYk4UGpr8XV74JmjgL6/XUW2mYXQ5TFw07KhksQ58ECE\nOPAEsyqU9piLD1mBV3oCNzX/DQBwxenAa0cAdw1ldrr9dqBHD8Tbt+OODRczgi4vdzloLDDjKOCr\nJXMBAGueUzP0/e+/xWfJp6EypF8iNNxCdbmBWACSJOH6T6/HxGERruPCEjQ2k0hKSXi9ZCzWVlwC\n+0h+4Zhdw49L2750DhwALjriImUcH5DHfU8/XV3QhSGTgANAXdx4Xj4rQDmOHK7cKDsMQUufNvMy\nS44xGE0jy3Xm4o8r/1C2e+KA2+ZCdUh24E7VgdMQOBV0bVTBrIAKhQo7HQtvCA0R8OPbHY/+Zf2V\nGu80Ox8AWvtaK9GFpkYIuODgxc6u95fFGoENdOCSJCkhQ45du9QlNGmVMxoGp6Fuk9W+jGpsZxRw\nmsBWXEyiDnSMm80orqvTn1vrwFkBZx14BgH3B2uAL79Ewq2qWNLKL1CyxkESd1wRcg2abMYJovxS\njmrek+z0KSrmEflSFXESkl/TXhX5raGdpgtYaKkMGgg4U2msLlKHXcFdaltMktVoUluRna92lpJS\nysvb5nQD773HX7+N+bQvILOAzz5zNlfu02jcl5KNgNeEjafAsQ48x5HDzc1m20erjOWZZMOzDpyG\nuGPJGGb9MkvZ7kkQwTNz4ACUa2vvyayACiXbELoRDRHwHEcOvr/se4zsTJLz2FXStt6w1TRCsbcR\nAi44eGEFPJvx7XQOfMUKtQSozPQl0+F70IfF5Yv5fdnlLWkYe5csAjt2kIIrLhfw3HNk259/KuJl\n6OQyjYFTB3788fx3bNShtlYf/tU6cNpRyCTgqRQ33k2nUCWcjIBb9MtKAkBpHXmRVXlI2ySDDktU\nY47YCmQ5/b8CoDq5dfKqXOuY1bl6PNMDm2o36U9sgKEDj6sCXhupxeoqk8RAto3yrZdqnGcylVTK\nbWrnHgPgOgdGdCvtljGEzrrjdA40KwGPGAs4K8g5jhxYLVbFhbMuuHspqTVOk8q0uK66Tvmd3lck\nEcHlH6k1BjxxMvWMulSjQi5U0LX3lGmIoaWPTPjv1aJX2v2MyFT6NR20XX1b9W30ORqDEHDBwQsr\n4Nmsb53O6fbqBZTxL6UZy2YAAD5e+zG/Lzv2PHcu8PTT6jh1NEqWAwWAqVPJz3feUXY3WiQjowOn\niWHHHst/RzsSf/wBXH65Thjh9ZLCHW43EXDq3JNJIBZTXpk6AY9E+BC63OZERO1opA7tSOYVayip\nIR2pKrt+wQ9K1M5nimkLmITtgFf+c66pJkl7NJOaJd+WeVUpwzHwBC/g9BpaTmg1EEe1OgqA6sBL\n8vhs7aSUVB24HLb9Zuw3+GXCL8hz5aUV8OdGPIer+12d1oEDpBrYIyc+gudGPMeN2y6bsAzfX/a9\n8lmbcd8mr43uXLQ6mxbWgdMOCb0vtn0LL1qIt856C31a9TE8j+vUEcrvZkls7gTgZmqMs1noSgjd\nxIHT4i1mjOk6Bq+c+QrePOvNtPsZ0RAHrmVQ+0F4bfRrXPLfvkAIuKDx+P17b8nLxhCPkwIiFgtZ\nPjITZg7cREAdUWID2fAYAEUIt/mAGfblkK66SnXgALBITjDq0YP8ZKIDQQMBj0QCeOxY4Ov2wEeH\nkW0bCoGXeoG48/p6rCoBXmtXh9cOB75pB/zf0YBEhf3UU4GKCu7cP5QB8/1yMltxMRFwmqkejeod\neDHjbGprudC3/5JzyXOIqc8v6XGTyl4arLEEgg4gYTHPSdB2NLQCXulV3f2aqjWYvmQ6fqv4TXee\nljm8G852iUc2/F4XrcOaKmMBL80pUeYChxQHzs8nS0kp5brUgQ9sNxBHtjwSBe4CwwgAZdyR42C1\nWE2nkVFyHDn4T///YHzv8dz23i17o39Zf+WzNmzMRgSomJoJOJuURYuXUCFnHXhLX0uc3f1s03ti\n9zWbRuZJgFskhM1CV0LoJg48k8haLBZceMSFGYXeiD0RcAA4//DzM4b49zZCwAWN5+STgb59gY0b\nM+/bFMRiwL1y2cfHslhAwEzA2bnZdJ9Fi+D8g4RWdQlTsgMfMhaYeDoRXi5TnJ6PdgyY6IBRCP2u\nlqvxn+HkfCPPJ9se7Q9cOgqoiuwG6uvR7SrgwlX348IxwKBLgatPBd4ILOauw557wDjgtMVyMltx\nMVmilHYsIhFSyEV+wacsAA5nFuOuqeEd+NEkHMkuiZnyuAwdeNwGbMnw7oxa0zvwVSXq6l0rKlZg\n0vxJ+LPyT915tA6zMUs81kZqsdVPhk5cmiXFCtwF6pKYA8lap9rFLaYMmqKIkTYMnO/KN86hkKEi\nxVYXY1fOaii0qMy4I8cp21rmkpAyFWizMfDOJWq5OEXADRx4JljBpSFpm8WGkzqepGz33HgLl/Tl\ndXqVzod2HXRtEhvNjr/r+LuyblO27KmA7w+EgAsazw8/kJ8V+vDmPiEaVQXXmsU/ZbNQNSvgtDPy\n22/KfGW6BrGCLODrZLNU6zY5N81IN3Hg1G1scvKlTyUAFbIhikSDpvXPy6Oyu2vRQnduDm35TNmB\nS7bsBDzehoylsgKe9LgNw+MJq76OuJaoLb2A/9YcWZHnK4H03QnK52wEXCsIsWQMu8O74bQ5kZ/g\nX+CcgLcjQsg68E3XbsIVfa9QxEgbqcm2Q3FI4SGou7UO9bfW66Y2NQS33Q1psoRZI9WEsbb5pCwd\ndd5aB/7ev95D3a11OLyZ+venbp12TBoi4GwYnx7ndXrx6YWfquc/9jhO6NkkNpqERjtD2mGBUm8p\npMkS7hlyT9ZtyhYh4IJ/JkzBiXPfPhcj54xMs/NeJBbLLOBs0ZRIBK/+9ips99hQETBwzACZpgUA\nqZQq4Kk48PLLJBltxQpSp9sIrVDSjHRWwBmXTN1ZMsnPaQ47gKq2xOnFoiFTAQ/Ic3JRWqo7N0WS\nJOK+Ge7vUI4uPb9FzCkLeH6eWnccICF0RsBjbgfwwgtI/OssZVvK4zac43zHMKD/eP12lkgGB/7b\n9SQM0SKXH2+mbpLisruBhQuVl71Z3WoWNlmLvrB3+HfAY/fAK/Ev8HxWwOWVwkpzVAGn16NipI3U\nNCSMm+fK45LV9hY9mpFhHHqvU3+ayn1f6C5EniuPEy8qprRj0pD1rdkOEu0IRBNRTPl6CrefRzMG\nTpPA6DOlbchUI31vIgRc8I/nzT/exEdr9asoNQnRqCqOZqsZsc44HMa4D8chJaXw+V+fq9urmTmb\nVMAlSSmRGXv7TWDsWDL/ulcvtWiLDA33oplmbqyBA2eT2GrlcKa2kIr/w7mobkeEIp5GwINU+OQQ\nvZEDjyajaj10mTt77MKanBAq5TW0U0WF/LziVIpz4LFkDLj0UiSK1bqgSbcLX7cnS0Pe9wXQtTh9\n6JdNaDJz4L3lAlY/7VgCABhQNoDbr2NRR+4zFVf64s3mZc8KOB2v3BnYCY/DA29K48BzClUBlzPl\n2TFObahZG6lpTEh/b/P48Mdx1VFX4dGTHjX8noa5jcSLiiyb9JcJIwceTUZx96K7+XMzIfRcZy5e\nPONF/LvPv3FNv2sAQDcvfF8gBFzwzyTDUoBNRiyWefoYK+CRCDN5mXkxsA58+3bgl1+Ab79VHXhA\nFtBK44QkRZQLNXN+a2pIxndAHQdlRbbu84+Afv2QDPMO1N+pLarkKTbxaNjcgSfl4+TvjTLcQ/EQ\nWT508WL80pKvDFbhJc8gJaXU+ewybBJbLBnDip0rsM2vlhnzDxuILw6xYFC4FHdYjsdH52sy9QE0\nc6jPg52iE9UkuIXiIdgsNrxwNymDS6eMDWk/hNuvYyEv4NTtNWT6Dyvg1PVVh6vhtrvhAp9dl+8p\n1DlwNslMydZ2mITQXftfwPNceXjq1Ke4fIEpg6Yov9OOldEzpCKrjZCkI5skNgC6EHq7gnZ45vRn\nlL9PpuVFmwIh4IJ/GMi1mwAAIABJREFUJvvyfzaJEV6zMXCLRa0ZrhVwWsxkwwZ1O+vAKyuBPn2A\nd9/Vr1N9yCGGTfKbCbgkERFn6oOzYe7av1YBP/+sOniZemtcqVIVj6UTcPne6+qAtm0RHDJAtw99\n+cZ6dEWfK4BDr1a/25nDCLjEOx3WgYfiIfSa2Qvz1s1Ttv0cXo+IXcLwUTcBixYZvvy6tlLrdbPu\nOGrlBTyeisNhc3Di6HP60Ltlb24/tiIZoArE2J5jAUC3vxFs6JYTY7sHDk31roKcIp0DZ50jTbyi\n2/YkhN7UsO1mx/Hp383o70cjDI114Np58aO7kpK5nYo6cX8HbVlSALqs9H2B2bS3Axkh4II9Z18K\nODvfmx0D17Zh5kzyUxNCV4QqyYStWQfOuGwlhE7fQwHjjGLF+RYxS0/RDkV1NS/grAOvJ1PPtHXF\nt0UqkZTXtY7HIqYC7occfaivBzp2RLBAPwZMhYeuwFXPDNPv9MhLcEop4OijgdGjle/YMfCdAaaU\nq0wgTp5FoYd0Wowc3JiuY5Tf6f0ARMBtKeDuZaobdtqcXJGOziWd0TqvNXe+RCqBzddtVj5TsZh6\n8lSsuWoNN6WKhX0xs7+z4XCPwwOHRRtCL9I5cKP7NBsD9zrUcMd9Q/hhjH0N63jZ+84qhB5vgIDb\n9Ku/OawOjDtyHJ4f8TzWXb0OXUq6mHYotJjVPG8KjArxHOgIARfsOftSwFkRZULoLxZuwkNs2Wma\nvMYKeE2NUhlMYkPvVMDz8rAosgYTTyOZ2XSoNuay4dpJHfFJvnEI/fYTgJd7gnfgZWWYdyhwzPzR\nuLtMdfusA58V/RF3DtWHvjf6tzC3aO7Aa21x0iGprwfy8hA0WOCDOvBoUj/UwAm4w0EKzsjzwVkH\nbrS6El2sgQqikQCM6qLWBU8y4/xRSwo5caBHDS+srCvrXNxZl8SWSCXQNr+t4nypuDpsDhxWfJip\ng2JFl/2d7TC47W6dA8935escuNE1lBC6ZgycrSfetbSrYdv2Feyz5QSchtCN7su+Z2Pg7LVnjZyF\nAk8BOhV1AsB3KIz+7TSmFOqesj+uuaccfC0WHLDsk94yWwWNCaFf1nk1bjuB2Y86bFbA6bQ3eiyl\nupo45k6dMHjwRsw4CljZTJ02Ve0BppVuwIs9zcN5Y88E78BbtMDc7sBP9X9iSs8a1MvvNdaBv59b\njvuP10+7YsuExqMhSEF+mpnSbEeCFNJJJoH8fAQt+nrw1DkarY4UsZG/F/d3c5GGsgK+I6Bf35h2\nCKggGglAM28z3D/0fgB8JnPUkoIrATiYOqv0PJf3vhyHFB6Cs7udDafNiQm9J+C6ftfhuLbH4dbj\nbgWgvvC1bs9sLNxhdeB/J/wPNx57IzcuS+dNA3IIvT0/xs5OI6MdIYfNgQeGPoDbjruNOxbQJ1yx\noeGGTMVqChrjwCcPmozj2h6HGafNyPo6RiIYjocx/sPxnJOnz8wofA4AH5z7AY5vdzzG9hqb9bX/\niRx8o/aCA4bKHKA0BKUmN1vhKiWl9m6Pdvp0YN484JFH1G1sCF2LkQPfzjhJWcATqQTq/btQVFDA\nZZFHbaqIUfFl5z0bSXmqsEDtEefnc876k07AOX8AgWIfAH5FKG1BlI21amGceGWF6VKWfzYDQpMm\nIAcA8vIQsOmXigzGgqgMVho6cPVe1LtZV2KFO8AnsaVz4PTFbySeLrsLtw+8Hct3LsfcP+eiMliJ\nUm8popYkXEnAzgq43AF4dsSz3DlmjpipOy8tWaoVxXQO/KYBNwEARr2hRgXYLHGPwwMwLpV+TztA\nNEHNYXXgtoG3cfuZVYBjQ+j7W8DZkDV73+kiKC19LfHtpd/u8bXjqTie//V5PD78cXggV3eTnXqh\n23ixl6NaH4VFYxcZfidQEQ5c0Che//11NLsZeLcrFAFne9i64icAcP31ZBnOdGzaxC8WQpk0CZg/\nX11/GyAibLaMqJEDZ88rC/ikeZNQfNz3KG+Ro8ynBoh4UxGrdRGBY4XWSFTL85mhhIICLly+gEQO\nEWylL7W4S2NCOAduTVOgBcA5h60gv+TlKWLD8vSSp9Hs0Wb4cM2HpuegDvybzd/gsNFb0fYGoJzJ\nv9rhT+PA0wgAhY4tNnu0GRZuWEgEPAE4mCLsDRE4xYHbs3Pg7LnZfVjxcNvdug6A2+7WdxKMsrU1\nwk9hQ+j7XcAdJgKexoE3JbQGAhsFETQcIeCCRjH3T7JO89xuUMSSdeCG6zVPnQqsWmV+0mQS6NCB\nlGc1gy1KYrCAieIcjRx4fb2yBrZFPvbZX4jjW93KoRNw6sBrZAFna3/79UN9WONhBLSgAEEnUBqx\noYUf+FM+ddCSuSjGxhrGgdvUcfMh7Yfoipls8ckdmPx8blUvCp2TP3OZ3slS6Pg067T/YPKKjNY2\n1jnwNBm87NrM7656F1FLCu4ElCx/oGFTwWiHQCuKZiLEbmfbqQ2h0/P1aNYD34z9BhaLJSuXz7pb\nlgPJgbNRAjZszVZd+/yiz7kkwaaElnSlSZCCxiEEXNAo6Es5aYUi4Gyyi24BEJakPtQLQF0QZP16\n82N/+kn93WAOeJi+X40cODMlJRWJ4P9+/j/1c44HKFHdcdSuZmLHbKoDX9oKePUIZuoYwxo7U6Yy\nPx9BB+ANJ9G5zo41JcDPrYElqfKMrqMuqkYKYjbVgQ9sOxDHlpEVybqVdkMvv1d1+bID71zcmTsX\nHSdeW70WZtDOFhs1SVnTuzLtGHi64RI2u9cf8yOKBFxJzRh4A6bw0H972muanYOdisR2FLgQut2j\nfNcuvx0GthsIILtOgmkInXHgRtnZ+xKj6W8A/zyGHTJMKb26N3HZXJg8aDL3DOiypmYhdEF2CAEX\nNAr6IktaYBhCN3TglLiBuP/1Fz++bca3zJicgQNXQttGDpzhp/hGXL1AnRQteTxAG7XYRdDBJ3IB\nQMQBHDUBuGi0sQMvtzBj2/n5CDoBbxzoHM1FnRu4RU6yO7bNsfqDTYhbVQfudXrVla8sNuRKDjW8\nnpeHcDyMsnx+SVSzJCEWKsbaThfrIHXHaBy4xWLhQuJsBrqdmZ5VH61HVEroQuiNceDaEp9m52Cn\nsDmtqiCzIXg2hM7+292jEPoB5MDN2tgUofOuJV259bhddhemDJ7CPe9JR00CANx47I17/foN5eb+\nNwMADi06dD+3pOEIARc0CvoSZR14xhC68qXBdwMGAE88Yby/JKlrf7PiL08pYxPKwiYCnrTw+1Ul\n+USymNcNjFHnLQedegFn2d2lnW5b0M7EhOUkttwY0DlMXuRLyZog+ODcD8xPrCFuU6eZeR1euG1M\nBSuLS02Uy8tDLBnTCQXr5s1ISSkkU0ld3kI68deOgQPqvPmR7p5471/vKdtZB14frUeEOvBU4xy4\n0nnUlKA1OwebZc8KMCteHodHOZ7tyGQTQs/Gge9vATcT6qYoXvLnpD/x6xW/Kp+DsSCGvzqcG+IZ\n0mEIpMkSBrTVFx/a1zx84sNI3JU4oArvZIsQcEGjoGHMhBWqA09kduD1LsDxZDGm/sgvqoCd+mIh\nCsEgvygJRZ6/zRYd0YXQo1E8dBxgn0yW56RR21CSz14Peh1ATo5yX0EHn4mtpfqph3XbAmDuWU5i\n88aAzhHyIg+4gBKLFzarDTYpu7nzbBKb1+nlXIzX6kLIAUw/Cmj1+1iEE2HdCznbhSiiyajegTsz\nO3BWEG1uefWqdnzFOnYMvD5aT0LomjHwhgicMnwjaQTcxIFzAs48H7ZdHrsHzbxkFgI7xKF9nkbX\nMBPBA8mBm7EvkteSUhKfbfhM9/c6kDgYi7gAQsAFjUQJY7Jj4Jmy0AH83oyIyvWfXp/9xWqM1zCm\nAs5mhOtC6NEofpILen3PRJdroRFwD5/ZnMmBB+R5wZeVjcQs2VAHU8yYvJzE5o0DLST1Re4FeZFr\naz1f2+9a3DfkPhxSyIsfm8TmdXj5GtJWNyQLMOk0YEeMJJo5bA78PvF3TOw70bzxBsSSMeVvViab\ndo/dY7pACHXgnADIy5M6HLwj1TrwKJJ6B96IEHomB64IPbOfmQN329244/g7cMuAW/Ds6epUNovF\nYlrJjWImzgfSPHAAePGMF/H5RZ9z2/ZHzXHB3kMIuKBR0HHNpAWGSWxpQ+hGODUvOLYGsoGAb2ru\nwi8BsuhFmNEQ5fdQiKwgNmeO4mDrGF2ptPBj40E3ednTRJuAk3f2Wmhhj5NaDcS4X4GcGLAluA2L\n5Mh63OdF3EYcuM/Chr3lxmhenL1b9sYdx9+hS0KLXTEewf+RMpxep5dLBPIaZD87rA70aNYD9w1t\nWOnOn7f9jNVVqwEAPeRcQpfdZSo82kpsgDqfPF3iV120DnGQaWRG88CzQYn+aKILWjdJBZR1fux1\ntCF0t92Nh054iKuRrr0fo46G2TNiO1sHgoCP7TUWww4ZlnlHwUGDEHBBo6CZrEkmhJ71GDjlxx/V\n320atQwxKyBRAfeogtVhYhR9LpTrfDPvVOX3L74ga3ivXq042DqXOg5eZeMz2IMuIiZK6UyDJDau\nebKA212kTd44sGT7Ugy+FFhTDARz7Mp2n12dQO7NlcOzGgGn4VatgMe7HIbyHCJURZ4izs16Dda/\npgKTLgHNiOGvDlem1FEBd1gdpsJD/9ZGIVjtMWyouipElljNiTfegV90xEUAYFr7nELX12ZD6Gzb\n7Fa70n6zqWBGx2ihgn9c2+O47ay7PRAEfH/htrvx3IjnTHMFBI1HCLigUdDxUrMQelYCfvvt5Gcw\nqM8WZ+t/UwHvqq8nnbKYhNAZqAOP2YGErCVVzrjhPoqAO9OPgSsC7iQvJS9zu+X5aofAGwN8diaE\nXkRW1NKuskTHm1v5WnHb46k45q2bh3xXPvq07MMf49AnmdEsa6fN2ehKeFTAnTanTnjoZ+q2WeGl\n96QLZRuML7YIAA7mETTEgd816C5svHYjhnTglxvVjrFSB26WxGaz2tKuxkWh92y1WA2faa4zF9tv\n2I7PLvws4zn+iThtTozvPf4f/QyaCiHggkZBHVhDQ+gJ5l/ckT0X49oF1wIVFWTD2Wfj6ms64a3u\nMBTwZJfDcMko4HNmmDjgsRmG0KceA2Vxk2AW2kD32RMHTtmRC5z45aXK9hxmLNkss5tup4lUlPK6\ncizZvgSnHnoqHDaHch6LxQKvS38uKlAWi6XBLpzSnTpwm0MnbOnC41Qo0zlwSit/4yuxWS1WtC9o\nr9uuHRP3OX267doQOv2cLsFKWTQlTSejpa+l6VQt9hz/RAKxALpP765UXxPsPYSACxoFFfCYDYYh\ndF0hFzmpLMrowfKCCKb9PE0R8EjfXvi/ovV49QgAXboAW7eSHWUBX9m5CLN7ASderJ7D37OLYQj9\n+pOhLG6iXe3LCFpHnApSpiQ2RcDdJIzNOvCXegGr6tYr260ut5IcZyaqdPvZ3c/GqC6j8PzI5wEA\nq6vJuPRRrY4CoDpfSZKQ687TnYcVmRyDEDslXQGNwghwdctROLf7uTph04qx0Ri4Nhxu5G5b+QGH\nZFxQpLEc0fwInH/4+cpnwzFwTRIbbZtZ0iXAr3rWWNgohN1qV/6++4tHT3wUNxxzwz65VkpK4c/K\nP/fp0qD/FISACxoFFeugE2lD6LWRWtRH65WqaVEjUZQrsNWWkBeuUuWsrAwYNgyoIuOmqTatdIf6\nu3ZExKE6uQ2FvMtX2piBoCzgNDEq6Mguic0mh9BzGQGv7aWOY+fGADidivCZCrgcQs9x5OC9f72H\nwe0HA1BLTho5d6+RgNuyE3C2CpnuHFdMxLTx7+DSIy/VCZuu+plBCF3nwA1C6MSBM+fZC/ORbVYb\nXhv9mvKZjrmmc+BUwNNNt8vGgTeEl0e9jMuOvGyvnKux3Nj/Rjw2/LH92gbBniMEXMDz/ffAfZkz\nmKmAB1gBNwihFz5ciPyH8lUBNxhqlCrJOtt1+eSFy1U5+/JLxaFLvXppD4W/YxuEXapATBkC3Mos\nKyohyxB6iq9GltGBJ2QH7pCdNaNz6yJqTXFvHIDLpQgfFWrt9B2tsFOxqI3Ucsdxx+ToC0+wIpNu\nHne6RSwcd/yXLK8KfSRF2+6GJrFR9iSEni006sE6PzbMbbPYlI5SuwJ9YR5t2/ZGlIBeVyDYG4jl\nRAU8x8kDx+PHAy1amO5G5wEHHcguC11e9tMoySzqr4EbQK2bODhdnfHlywGnE4lm+pW8/O1bIuLi\n+6HPMrleYQep6+2UrIhZzEN4dJER6jj9TjnD3gQlhE6zvpnb9cfUKm+2FACXSxE+KtRmSWwUKha0\nZrSRc/d69S6aFcJ0DpydivXXNX+h47SOSgicFWWtA9fOCzcMoWeRxNYyANiYKH5zb3PTtjYW6sDZ\n5VIHlKmVv+xWO14a9RIu6XkJ/p+9Ow9vqszfx39nb7qkQCmILZW1la201QoiuwjCICooLoCibO4f\nRRREZwBHGZxRHDeksshPEVfUwWXcB7846ggiIqDIalspQlm6pG3W8/vj5KTnZGlP2qQJ6f26Li6a\n5CR9WjF33s95n+f5U/afgr5OcyrwA3cdwMnak4r7ztRFQ5oq0ZCIj6Z81OC/R2oaVuCt3K5ju/DE\n10+Il21NnFj/gLyJLIDGptD9zimOEsviQFPodZXiG9xpoxiw8grcpgMebLMdx7LSUOv03/u7KiMd\ntT4BbpSdtr1lvPh3hrvhNcGlaXZpKvV0gCte5G9A3gD3hF1icuAp6bIUAEajXwXuyzegpdCotIn/\nHaQpdEUTW5L/eWx5ldhQE5s8RLq27eqtRAFlUIVSgQebQg+4AYhT+WHAt/s+HAJdtnRu+3O9X+u0\nOiQaEhsMb6B5FXi3tt1w/tnK3fVaWwWu1+oxpseYFt+ytDVggLdy/Z7vh3mfzsPetX8H3qlfvzro\n6mceUoDb9IDLJYaeNK0MBKjAPduIBppCr7OKS39V6MXkrTLVv7GvzQeWDgUmX1qlmKKXVKWaFVPo\nALxbhgLAy/3Fv89GSoM/j7SXthRYJzxZLa+s5Vt5KgLcZoP78gkBX/eqPVBOoXtC9YXLXlAc5xsO\nvtWeFPxTc6cCAB4Z8QiSUpQLjvg+T/rAIQ/UVZetAgBc0+caxfMaW6xEei2/ClzXeBObPLD+1PNP\nuFSTLd6QfRjolKLcJjUc5JuoSDQaDeZfNB/d23aHxeTfQxCI9LsPVwC1tgq80lYJy98s3g+jFD4M\ncAIAWOFTMZ/w3wPaSxAU0+VWT3DLNyuwu+x+08RA4Arc5gnw01oxLauNAtxzxaVWpYa0H9vUKb6n\npMphRZ1B+c/Yt4kNADJ1wZu25GP3TqF7ZgFSZOu9yENGEeBGI5zwn57/5Rkg+wQUFbgUAjfn34zq\nB4JfVuMbgt6FXtrnQFgk4LKcy2BO9w89RQXuCR752t5Tc6dCWCSgd3pvxfMUAR5gqljqWm+oAg96\nGZkssN6//n38e5jnw8u113rvj0QFHuw6+GWjlmH/XftVB7I0NvkMUyTGFc/kp5UofFrfv6RWaNo7\n09DhHx0aPOYk6t+cRt0ADNh5Z/CDBwxA3R+/e29W794OlJV5q1hADHB5FS41hAWswKs9AS5bn9y6\ndAngcMDt+Rd6Wu8M+AZaZa9CrVEZKoHOs2foGt53WHqD8Z0yTpFV4BkpGd6vfafQA3Uxe58rq8CD\nNVT5ClaBy+kt/h9KAlXg8gCXwtX3fGRjq41JXeu+FXig6eBGm9iGDQOKixXNkpEIcGl3Kd9r60N1\ndrI4tuM1x5s9JqD1TaFT5PCkRCuwfud6AOI5So1GI+6pfeQIsH074HkfL0d9+H7eDYDtYP3xcqdP\nA1u3wjYC8OzLgepX1gFPr4P1xWHew+wuu7JKN4jnpoOeAzcaUeGsH0OVvQopKSko79IBgHiZWcAp\ndFsV9L4BHuBUZSdDO6CBzZDKa8pRba/2O3cvr8BnnzcbJZUl+Lrka78AD3Qdsfe5JhO0dWKAyxuq\nGqrEdFodtBqtN/ADnc/OTsvG05c+jRRTCm76l7hwjOIyMr1/gEvf03fpUHnoBtrgQlqWVP6YQWtQ\n3A5lJTZ0Vu5b3tyQDWToOUPRMakjrux1ZbNeR/pwEfL6/kG0til0ihxW4BG25bct+OmPn6LzzX/7\nDehWv2zZ+7++j+PW48DQoeL05d//7n3shFDj9/QjVfWXQ2HnTvGc5RpxAQp5lSt1jcsrcIfboQhc\nqUksYAV+7AhgsXgvmQLEYAaAE93qp4lXb1/t99wqe5XiOvBg0kwNT6EDwC/lvygCFlBe392lTRcs\nHrYYgLoK3HtpmdHorVxDWcxCzSVhdw64E7kdc7235UEsPSdQ+Mu3JfV9XiC+TXSAf6UeykIuviLV\n4HTvoHv9dngLVbhnB1pbBZ5kSMKuW3c1eWVACo4BHmFD1w1F7srcxg+MhCeegHDokPfmhNcmYMDq\nAd7b8qgqF6zwtffEXsVrAQDmzQPgE+CeLFCcA//iU9SWFXtvS9diB6zAy8uAlBRU2CrqX9MzpV3e\nt/7N95vSb/yeW2mrREly4/sMJxqTkeBTJEtv7OOzxVb1Xcd2+T1PPoUuXxvcN8Cv6n0VAGBCTn0z\nm1b6BZtMuOX8WwAoL2OSyCtkObWLsgQ7fy09xzesfZ8T6LYvaRc0RQUepCvbN6AaC6xgW5Y2V6Ae\njKYIe4C3sgpcq9Gic2rnVnnuP9L4G41nDoe3m1py6HR9oMu31zzhuVZVtrYG9pbLAly2O5hTq7xG\n2luB26vRxlN027dsRt1dt3uPaagCt7nsQEpK4ArcUQGLyeK3kYfkdN1p/LtTDfLLgJPLgHn/DXgY\n6vTipUty0/tPx5G5R/DXEX8FEDjA5RW4fHcu3924puZOxZG5RzDnvDn+39xoxENDH8LRe49iQOYA\nxUMVCypQfHex/3NQH8ZmvbnBNz/FftUBLiOTb0Ea6DlA4wEuPd5QBS7xnYJvKLAqF1Si8oHY7k5m\nBd48VfYqpC5LZSNbBDDA45nNhiMNXD1VLgv38q1fAlDurf1L+S/ATz8BGzfid8dJjLxRXKpUqr6l\nc7xSBV5dV4k2ntPeW88Ghvbb5n2tBitwPfym0KVLTsprytEhqYPiOmW5f+//N06aXJiwV1zDOzNI\nFtTqBb8ATzImoVNKJ/Rs1xMA8MQ34iyDfNlS+TnwQLtzyUOsU0onpJn9L+2Smtg6JvsvVmIxWYJO\nj0thHGwDFPm4vM+JVAXueQ35B4lgC5uoaXSTpJhSGv35mirQufymOCs5+IJGTdHaKnCKHAZ4PLPb\nGwzwE7I+JinM5euG/+fwf4ALLgCuugp3tf8O/+kK/GUEUJcudnSne2bMvRV4TQXaeSrwz7oD5bJc\nqm7oHLgeQafQT9ScQJo5Dakm/2VDgfpKeIJnsqCD/5kATCpJwdSzL/ULcCk4koxJ+FPP+sU80hPT\nvV9nyIqGQAHuG07tE/1Xi4PJP0DVkL5XQ0uiyo8DAk+7m3QmPDP2GTx+yePex3qm9cTsgtneLTCD\nBfhn0z7DzPyZ6NehHwBlKKqtwFt6AY8tN23BledeiUu6XRKW1zPoDJh/0XxsmLghLK/X2ipwihwG\n+Bmioq4CZVVloT0pSIBLZwblFfin3YFKk3Ld8J+O/YR3uogBeVAvXrN8dhVgayO+aLpnVr3KBPzS\nXgzntv6XagOQTaGrrMA3H94MQRBQXlOO9ontA26+Ma7nOO/X+WUAUlJwdoBZure+OhvJye38K3BZ\nU418d6j0pPoA73+0/niDztBgBQ4AaYkBKnBj09b5lircxpp/5KEdqInNpDPhjgvuwL2D7vU+ptVo\nUXRZES7pfonf8+Qu7nYxVk1YFTB0gp0D96vAW7jiHJw1GG9f83bAmYemWjZqGa7rd11YXovngilc\n+C8pgsLVRAMAOc/m4OzlIZ6LczhwNMDspHRe/KTPZch3X1pfKecfE990J14L/JoGHPJcRp1RBdSm\niCfPpQr8w55ArzvEr9sGWetC+mAQcC10HcQKvK7CeznRmh/W4Kvir2Bz2YIG+LV9xIVAxvyeIEZG\nkACHIAAJCQGn0CXSZVKAsorOXfCk92v5OXCJb4BLMwWKEGtiBS4FZEgVuMop9IZeIxB56Eg/W9Ap\ndN9z4Kw4W7UUYwoqFlR492en8GGAR5DvXsrN8YdV3JHL5rQ1cqSM3R5wJy6pKpf2zn76Q/Hvw23q\nK+Vrf3Rh8G/i12XJ9Q1vbg1gTRED4SzPQmL/6Vr/2m2CVeAW8QXkU+hSc1WdHnCmJKHKXoWh5wz1\nTn1+W/otAKB72+7eRTnkxvQYg89v+Byvf+r5dGE2Bw5wIHCAyypb+XXR8gDPmHKL92uNRtNogGs0\nGmyfvR0l95TU3xnpCrwJTWy+1Aa4W3D7rSrnK9oVOMUWt+BGSUUJ9wOPAAZ4BDW0x7AqggDMnw/8\n73/eu07UBlji1BYk1O12b0jLSQEuTWfn/gH0PAH81KF+848UG3DTDvFr+cYeDi1gTVYGuFzbQSMD\nDmXnnCvwwKMjFR8opND8rQ1wU7v/BwBoY2rjvSTr53Jx/fSc9jkBK/B25nYY2XUkUms8H5RMJsW2\nnl6CAJjNDVbg8qpRfg68sWavQCGW3ykfGZb6Vdta9Bx4hCpw6ffjFtzeQA46he5TgXPKuHWzOqzo\n+3xfxToRFB78PyuCAq3OFZLdu8XFVgYO9N51osYnwF99FUhIAL77zv/5Vquiq1zyhycPpGo4wQmk\n1YhNZz96Gm6THPUbeRyUrUJq1wHWJPGNO1CAWxJSxS00fRTtfw3LHF/gK9m2y4lGMWCeGASsd/0A\nQFyyU5py3XN8DwAgJy1wgHvD0+H5PSf47z71l80QtztNSIC5gQpcLtWUivM6nYeFgxf6hU9D63wH\n1cQAvzDzQgDAoMxBDR4n/xAhD9XstGyc2/5c7+s0RG0FLghC0Ap85Z9WiuPtrBwvKy+iyOBSqhHk\nu652yJxi4tj2X3k4AAAgAElEQVRlGVFeU6485uGHxb/fflvsGBcEuOF5k62oQF2AFSql6XCpAje5\ngPY+C7El2cX7Ac+WmB4OHWA1i/9sUgMU/kkJFhhdQK2Kj4a+y3kCngDXKgO8Z1pPcQW5YIIEeInu\nPmRu/gfQTQAsFiS4NJAvXxPs8iWDzoBts7cFfEwedFqNVl112cQp9Of+9Bye+9NzjR4nr3jl42tr\nboufb/9Z1fdqtAKXrSQnfcDyPQc+5/w5mHO+/3XwDHCiyGAFHkHNnkL3kF/u5TeFbvVMS5lMwL59\n+FcvLXQP6/D9X2YBe/YEnEKv8BSEtq7ietQmJ5Dm03yW5KhfxORoz/rlTO06wJooBnhSgKWhzYkW\nGFSe+g+0ulhqQqo3FKvsVehs6YxEQ6JfBa6o/oIEuEkj++FTUpAwfJTi8aDXXwdpzgIa3/QjIG3L\n/W/W0Ngb0liAS931Xdp0afQcuC+XO3y9IHRmYgNbZET8nWXfvn0YNGgQsrOzUVhYiN27dwc87qef\nfsLw4cPRq1cv9OrVC2+//TYAYPPmzTCbzcjLy/P+qa0Nz7Z+kdbsKXRPF7tiwRXfClwK8KNHgc2b\n8YJnwbJFB8R1wwNNoZ9OADB5MmzXTQYQvAKXAlreye7QAlaTZ2/rAD9ejWfTEjUC7cYln0IH6neU\nkneJAz7T31KAe6aq9zyvxdoJa5Gu9Qzc83s0nZUhfwm/KXQpxIKd2wXqNxkBQgjwMC0ookZDY29I\nYwF+bd9r8ejIR/HW5LcaPQfuixV462YxWVD5QKXq/ddJvYgH+Jw5czB79mz8+uuvmD9/PqZPn+53\nTE1NDS6//HI88sgj+Pnnn7Fr1y4MGTLE+3hOTg527Njh/WM2B9+GMZr2ndiHd35+x3u72RW4XUxQ\n+XKo3nPgr74KPP44aqyn8VJ/wF1SDLRpg0LP/iP/7iH+HeiyrYoEAEYj6gQx+Eyec+BySY76gD5q\nqJ8rt+tkAS6rwDM9a7BUaO3qA7yRKXSg/hIk36lqxfS32xMQnqnqXie0uCn/pvrg9AR4gk5ZofvO\nACToxccbC2Yp7BoN8EsvFf9OC3BteIREqgLXa/VYOGQhMi2Z3v8War+XFOCRWvP8TOO7YU68c7qd\n+Hj/x2GbkaR6EQ3wY8eOYdu2bZg6dSoAYNKkSSgpKcH+/fsVx23YsAEDBw7E4MGDAQA6nQ7p6el+\nrxfrsp/NxsQ3JnpXB2v2OXBPd3nACvz664H77sN9F7tx45XAUwk7AKvV+9bg1ophG2gK/bQnwG0u\n8fVNLv8p9GR5Ba6rf9CuA6o9U/BJDqC3uNMnHhKbyDGqy8hGA1zveXxG/gy/x1JNqYqwDtYkFnD6\nW+8JVCm4Z8wA+vQB1ovbqU7qPUlxuO9rSwHuO3Oi1WgVYaU6wP/1L+DECSCl5aYPI1WBy0kfqlRP\noXsup2ztl5NN7iPOeHVp0yW6A2lhNY4aXPrKpd4NgCh8IhrgJSUl6NSpE/SeN1aNRoOsrCwUFys3\nb9izZw9MJhPGjx+PvLw83HDDDTh+vL5p6cCBAygoKEBhYSFWrFgR9PstX74cmZmZ3j/V1QHapFtA\nrUMMvGZPoXsCXLHk6Y/KHbkOeDay2mE4AVRVKRrebJYk1Br9/xNXmAAYDPUBntIWep9ZziR7fQV+\nXFsf4A4dYNWLHxOS7cD2IqD6UWDOjU/j5P0nMaTHxTAEmTHVarT4aehrqL52J07NP4WpuVP9jvGd\nQpd/XbOwBndecKf4vQM1oOk8x0rnnNPTgV27gIvEHcBGdRuFU/NPwfaQDdUP+P/bkGYE5PuYA4B1\noVWx4YbqADcagXaBdxqLlFCCWC6Uyt1bgYc4hd7aF3R5ddKrKL+vPCJ7n1PrFBNNbE6nE5999hmK\niorwww8/ICMjA7feeisAoKCgAKWlpdi+fTveeecdrFy5Em+88UbA15k7dy5KS0u9f5KTI7NJQmPs\nLjs+2v8Req/o3bwXClCBn/jpf4pDpIVTTmsdwJEjygAfNxp1Cf5vmt4K3LMojOmue6DxmdUzOwM3\nqdllAS51qic5ABgMaGtuC6Sk+FXg0naZFpMFfUdcA1PvfmiT0CbgZhMmvUk5hS772mwwewMq4Jrj\n0pT100/7P+bRJqENjDpjwApeqsB9AzxBn+B9DKgPyVgMpKZOoYdSHUvHqq3ApcBvaEvU1kCr0QZe\napeoiSJ6GVnnzp1RVlYGp9MJvV4PQRBQXFyMrKwsxXFZWVkYMWIEMjLEJqOpU6dizJgxAACLpb7x\nITMzE9dddx22bNmCyZMnR3LozWJ32XHl61c2/4U8AX5Idh221QBgW/0lTt4ATwBw8KAywM0G1OqU\nyZxW4zlWVoEbjGZcuwvY3AXIPwqUZlrQtlasODUCIMhy1qEFrFrxXJaiiU2avjYaYczpDZwSLwGb\nrx2Kt81lOFl7MuA574eHP4zstGx0SumEN3a/gR7teii2MfU99/3gkAfxe9XveGL0E/6/ry5dxPPh\nTWwaCxbgwY5r6U061GjqFHpTFltR+2FhUq9JeP/X93H/RfeH/D3ozKfVaNE7vTcX9ImAiL4DdejQ\nAQUFBVi/fj2mT5+OjRs3IjMzEz169FAcN3nyZKxZswaVlZWwWCz48MMP0b9/fwBAWVkZOnbsCK1W\ni6qqKrz//vuYMcP/3GksqXPWhac68wT43jRxjXGzw3NNeGEhjieKC7EkekL0dAKA3Ydgl+1YWWc2\noFbnhslZv2hLx2rgeBIAnViBm3QmaEwmmJ3A//eu54nnng1ADPBke/12oYB0TbpYmisqdH39PyVj\nYgpwCmhTCyxrMxYfGV4DELjr/M/D/uz9eug5QwEow8T399jW3BavTno18O9Lr29Wx7faAJc+iMRk\ngDexAg+lwUyaElf785sNZrx21WtNGhed+ZKNydh9W+Crj6h5Iv6RqKioCEVFRcjOzsayZcvw4osv\nAgBmzpyJTZs2ARAr8IULF2LQoEHIzc3FF198gZUrxVWdNm7ciH79+qF///4YOHAgLrnkEtx0002R\nHnaz1DnrwvPmLgV4eyCnXJyulirsDvcDnefWXyZ2ygzg0CFlBW7So04nIFWWRx2tYtgLBj1sLpu4\nzKbvQiOy66l9p9HtOsAq2GByaaAo7mUBLlWB0lS6NHUqn4ZuSLAp9KAuu0z8u2vXho9rhLQGe27H\n3AaPk36eWAzwpo4plL2zpV3jAs2oEPmyu+xYvX017K4A5+SoWSL+DpSTk4NvvvnG7/7Vq1crbk+b\nNg3Tpk3zO+6OO+7AHXfcEbHxRUI4A/xUglgxj9snhrTdJ8+kzUdOJwAoL/cL8FoIyLQCxzztAB2s\nYiNanVbrrcD9lvqUXaYnnyZPsYnPdbhtSHZqAchOdhv8u7QNbgBarTfw1L7hB2tiC+qNN4Bjx4DM\nTFWvH8zi4YtxSfdLMCRrSIPHxXKAhxLEiueFUIFLMxTDugxr0vei1qXOWYdZ783C5D6Tm9xkSYHx\npEQE1DnrQmoK+r3yd9z+we1+U7dCXR2uEHfMRE65WNH6Bri0rnmVCfjnQOXjtgQ96vRQVODSFqD3\naT6FzWUTq2LfClwe4J4PzcnGZJh0RnzaHdhs3Y0kl89A5FPoUoC7AGg03qnzQFPogai5jEwhIQHw\n6atoCp1Wh6HnDG00BGM5wJuqKcEv34+diFoeAzwCQq3AJ7w2ASu2rUDRtiLF/cV1f+D/dRG/HnWw\nPsDdsvfaEtkum38dqgzwKqMAQaNcs3yoZ4vQ51zfoLiiOPAUuuwUhVSBpxhTYEitvyTqtO96qYEC\n3A1g0iRv4Kn99B1oIZdYIv08sXRd810X3IWCTgVNfn5BpwKkmdNQNL6o0WPvvOBOTOk3xXt1ARFF\nR/yUEDHE5rKFtE70oVOHAPhXdOV28Vzj0s+AwiP1AS7f3rM4FWinTcYV26qxtgA4YqnfsOOEwQnU\nKivwq/cAL78NTJsIHK0+ij7pfZQBLgjitdMeScNGAQc/Q4opRbEXeaWaAD+3N9CtGxJ/SvS8tLoV\nqBRT6DEUkhJpJiGWVpZ6auxTzXp+sjEZ5feXN34ggKfHBr9Mj8iXTqPD6O6jY/LD+JmOFXgEqK3A\nBUGA/q96nKo7BUDssJY74RDXJ5VWSZMCXH5duNUIJBrMyPGssLpLtkbEdZVrAQBtuvZSvO64fYDW\nc87TpG/4HHiqSSzx2yS0afgSJdk5cKkTWgpyk058fbVLSDbUhR4LEvXifwA25RA1LsmYhI+nftzo\nvvYUOgZ4BKgNcOk6bInvjj3lTvFSLmmjESnAT/icSk7Um5FTK/7PUaf3D8nUEZcqbrerBc7SicFs\n0gWYQpcF+INDHsS8C+fhsVGPKabAv/uhUPmcQBW4J8jle0mrEXIXeguTptCbvdIeUStgc9qwePNi\nxQwehQcDvBn2ndiHl358ye9+tdeBS0uuShQVqtuNE3ZPBS4PcL2yAgcAsy4BOULw85GBlh1N04kf\nFgKeA9fVjz2/Uz7+MfofGN5luDeYh2QNQWGtcrYgYIB7Knapw1ltBS7/3cXi4g9SgLMCJ2qczWXD\nki+X+BUs1Hyx9+54Bsl5Ngc3vnsjyqrKANQHj9oK3Hdxf8W2i1dcgfLtXwHwqcD1WsXuZIAYKN0u\nvzHo9wlU+bbXiyvcBazAO3QAbrwR2LhRcbdUUScZkxSBDaDBCryhcQQS61Po0jlwBjgRRRMDvBmk\nirLWKVbSUnA1GOAHDwLHjwNVVaj5eafiIZdb1hj23nveoJafA3fD7b10TJJoSIRx0V+DdnmXVJb4\n3ZdmEKfQ9Vq9/zlwjQZYtw6YOFFxt/QzJRmSgAkTlM8JcA5cqsClQFa7L/SZMoXOACeiaGIXehgM\nXzccdpfdG+T3fXpf8IO7dwcAvDSpB2b03g/I8sm17TvgUALwpz/h6QHAcxeI98un0AGgzGd3ykSj\nGCjJxmScrD3p9y2lccm1M4oVuNVh9a/Ag5C2hUwyJgGzZwOjRwPduokPBqjApb9DvcY45IVcWhgD\nnEg9g9aAGfkzmrzMLwXHAA+DQBVuY27st9/vPtc//g7s+jvgcOD/xtbfL23PKQW4VIEbXOLKaGaD\neEeSIQkna08qNiAx681YPno5hmYNFbcxXCxuApOsF59Tba9WHeBSYCUZksQqXb50aQNT6KGeAw95\nIZcWxgAnUs9sMGP1hNWNH0gh4xR6CxGqq4EffmjwGJf0X6OsLODjUoAf8wR432Pi32ZPBS5dppEi\n6xV57arXkGHJwO0X3I6r+1ztvT/ZcylUla1KdYBLXdcB9+JuqInNU4E3aQo9BitwaUlYaUaCiIKr\nddRi5qaZfk271HwM8BYiWFKAgoZXynJ5qubyAz9570uWhbExW7ye+3h6IpLsQDfx8nFUOKsBeCpj\nACmywlC6z1eKTgyhanu1/znwIBQVuC/5OXBPcEsV+IWZFwIARncbrer7xPoUukmv7vdFRIDD7cCa\nH9bA4eZll+HGKfQW4tYA2iAzyLkdc7Hzj53eCnzvwa0AgNu/AxZu8Rw0ciSMY88Dvv4ZxzomI7VG\nh84VVQCAkuojAAJX4MEWT0jWisu5NWUKPeCuYg1U4Nf0vQbd23VH/ln5qr5PrE+hc0MGIooFDPAW\nMu1KcR1yqxGY97XysaFZQ7Hzj5144TxP0P8h7p07+gBwdlX9cVJwHK8tx7mJndG5UnzwSJUnwEOq\nwMUp9FACXPoEHTDAAjWxaeuPO//s81V9DyD2p9AZ4EQUCziF3kJe6wfcNh64bzT8WrlSTGJb+f8y\ngVkTgMPVYlNcd5+Gcik43IIbbQwWTN8BXFAKvDLxFQD11XZS//qwDFaBX9Z2AAo6FeD9698PuQIP\nuKRqAxV4qGJ9IRcGOJF6Jp0Ji4Yt8i6pTOHDCjwKTvksheq7hOqRunLACGTIqm9oNIrgSDWmoF0t\n8L/VAFaNAFC/5rje0gbw7EvhV4GPGAH85z+wdMnB92O/r7//nnuAwYMbHLfUxBYwwALtB97Ey0Zi\n/TpwBjiReia9CYuHL472MOJS7JU3rcDvPtdx+3Z1H3afhNmh3EUMUAZHG1MqfPkuXRrotbFpE/D9\n90Av5QYnWL7cb+EWX1IF3tgUuu9CLqGK9ZXYpG000xPTozwSothntVsxZv0YWO3WaA8l7jDAo+CI\nT4BLU+iSXQmVOLsK0NxzT/2daWnKCjwhQIB7glN+eZO07KdXcnKj3fDBqA3wZlfgMb6daI92PbBx\n8kZ8P/v7xg8mauVcggufHPiEl11GAAM8Ckotytu+VfIfCU6cbdWKVbHVCsybB6xcqazAE9r4va4U\nmPJ9qsN5Dln6HzBgMDewEluoYr2JDQAm9pqIzqmdoz0MImrFGOBRcMBn4zDfc+AAcHadJyQTE4F/\n/ANo21ZZgZsDBLhnyjpS21w+MuIRAMCFnS/0fzDQOfBwTKHHYAVORBQLGOBh1qt9L5yTek6Dx+zz\nDXBTgAB3+F9rrajAE9P8Hpcq40gtmPDg0Afh/osbmZZM/we19f+UfBdyCVWsL+RCROol6BOw6rJV\ngdePoGZhF3qYaTXaRqeO9/lkb+KalwCf/T46uX32DIUywFMT/ff/lh6PVAUOqNuYROqGD8sUOitw\nojOaUWfEzIKZ0R5GXGIFHmZajbbRvcB/PEt5W7fuJb9jCmz+FbY8EDukdvJ7XKp87S47XrriJbx+\n1etqhhx2AzIH4P8G/B+u6n1Vk54f613oRKRetb0afVb0EReNorBiBR5magLcl67Gf5H/oa4Mv/vk\nAX52myy/x+VT6NP6TwtpDOFk1Bnxz0v/2eTnx/pCLkSknltwY8/xPao3MyL1GOBhptFoQj73q/P5\nd33uccCQZPE7ThHgqf7noaXmsivPvTKk7x9rOIVORNQ4ljdhJq/Ae7briclu5YIp8o1GHrdehCPD\n34dOtrZq7lFg6yoAKf6NbfIAt5gswDffAIcOee8b2XUk9ty2B0svXhqeHyZK5AvRcAqdiCgwVuBh\nJg/wtMQ0ZDkcgGwJ4LyjwBZPk3qXK29GpzbdcEhWgeecAJLtaDTANRoNMHCg3zG90nv53RdRy5cD\nJSVhfUl5oxwrcKIzW6IhER9N+QiJBv/GXGoeBniYaVA/ha7T6KB11CkCfMhvQF2/XqgxaXFer5FA\nhUZRgeulMG8kwGOGfLW4CGAFTnRm02v1GNNjTLSHEZc4hR5m8gpcV1kNrdOpeDy9Bvhu2Hrsum0X\nurTpAiQmKvYJbyjABSHIhuJxjBU40Zmt0lYJy98sqLRVRnsocYcBHmbyANf/8CO0h39TPJ7gBGCS\nleSJiYomNu/Xmf5NalIFnp2WHc4hxzRW4ERnvip7VeMHUcg4hR5mWo3Wez22TgC0Fcp/uGYHlPtv\nm83KKfTOWcADU4DJk/1eu0+HPnjnmndwUeeLIjH0mMQKnIgoMAZ4mGk0mvoK3A3F9DgAmH0rcK0W\nOoMJgNierhszFhgfvIv8inOvCPOIYxsrcCKiwDiFHmZapxt6pzgPrgsQ4H5T6AB0pvo1gkNdBCbe\ncSEXojNbkiEJu27dhSRDUrSHEneYFmGm/e/XMFQD6AcImiABblR2k+v09Qu/sOJU4hQ60ZlNq9Gi\nc2pnfhiPAP5Gw0wr1DeiCfAPcAD+FbgstFmBK/EDDdGZrcpehdRlqWxkiwAGeJhpAG9TWqAKHIB/\ngAtcuCQY/j6IiAJjgIeZVgA0UoDDP8A1AgCdMpR0sqVDWYErsQInIgqMAR5m8sAOVIF3P+X/HJ3s\n3BADXIkVOBFRYAzwMNMK9aHtW4HvXAF0CxDgGvnuW6w4Ffj7IDqzpRhTULGgAilG/9UlqXlY7oWZ\nRoB3Qty3Au+y7h3gj9MBnsQp9GBYgROd2dyCGyUVJTi3/bn8QB5mTIswU0yh+9zWjxoDGMwBnlQ/\nEcLAUuL/8ERnNqvDir7P90XFggpxG2QKG06hh5miiS23H7R/WeR9LGg4a3kOPBheO0pEFBjfHcNM\nA9kUutkMbft072NBq0l5Bc6KU4EzEkREgTHAw0xRgRv0igoyaDXJc+BB8QMN0ZmPDWyRwbQIM628\niU2vDHCNLKgVNFzIJRj+PojObBaTBZUPcC/wSGAFHmaKClyrUXcONzfX+yUrcCVW4ERnNqfbiY/3\nfwyn2xntocQdBniYKS4jg6AuwNeu9X7JAFdiBU50Zqtx1ODSVy5FjaMm2kOJOwzwMFNU4ILKAE9O\n9n7JilOJvw8iosAY4GGm1etDr8BlWIErsQInIgqMAR5mWnOitwIHQr+OmYGlxAqc6Mym1WjRO703\n13SIAJZ7YaZJToFGnwLgd/VT6DKswJX4Pz3RmS3ZmIzdt+2O9jDiEt8dw0xrSoBm4iQA4hrAIVfg\nrDgVOCNBdGazu+xYvX017C57tIcSdxjgYaY1GBXXe7MCbx5+oCE6s9U56zDrvVmoc9ZFeyhxhwEe\nZlqjERpPGxub2JqPFTgRUWAM8DDTGEzeCrwp58AZWEqswImIAmOAh5nWaEKv9r0AAOeffT4r8Gbi\nBxqiM5tOo8Po7qP5YTwCmBZhpjWacHP+zeiY3BGXdLsEnxz4JKTn8x+5En8fRGe2JGMSPp76cbSH\nEZdYgYeZxmiETqvDhJwJMBvMrMCbiZeREZ3ZbE4bFm9eDJvTFu2hxB2+O4aZNlm5bR7PgTePAKHx\ng4goZtlcNiz5cglsLgZ4uDHAm0gQAgeLNr2D8jYrcCIiioCIB/i+ffswaNAgZGdno7CwELt3B16R\n56effsLw4cPRq1cv9OrVC2+//bb3sTVr1qBnz57o3r07Zs2aBYfDEelhN8otuAPe7xvYXMileQxa\nQ7SHQEQUkyIe4HPmzMHs2bPx66+/Yv78+Zg+fbrfMTU1Nbj88svxyCOP4Oeff8auXbswZMgQAMCh\nQ4fw5z//GVu2bMH+/fvxxx9/4IUXXoj0sBsVLMA10ChuswJvmu2zt+PZsc+irblttIdCRM1g0Bow\nI38GP4xHQEQD/NixY9i2bRumTp0KAJg0aRJKSkqwf/9+xXEbNmzAwIEDMXjwYACATqdDeno6AOCt\nt97ChAkTcNZZZ0Gj0eCWW27Bq6++GslhqxKpCpwBLsrvlI/bL7g92sMgomYyG8xYPWE1zAZztIcS\ndyIa4CUlJejUqRP0ejGUNBoNsrKyUFxcrDhuz549MJlMGD9+PPLy8nDDDTfg+PHjAIDi4mKcc845\n3mO7dOni93zJ8uXLkZmZ6f1TXV0doZ8sglPobGIjojhS66jFzE0zUeuojfZQ4o6qdLnuuuvw9ddf\nR2wQTqcTn332GYqKivDDDz8gIyMDt956a8ivM3fuXJSWlnr/JCcnR2C0IlbgRESNc7gdWPPDGjjc\n0e9dijeq0mXEiBG47bbbUFBQgDVr1qCuTt2i9J07d0ZZWRmcTicAsXO7uLgYWVlZiuOysrIwYsQI\nZGRkQKPRYOrUqfj222+9j/3222/eYw8fPuz3/BZXUwO3PfAlEfKNTAA2sRERUWSoSpfZs2djx44d\nePrpp/H555+ja9euuP/++xXBGkiHDh1QUFCA9evXAwA2btyIzMxM9OjRQ3Hc5MmTsXXrVlRWVgIA\nPvzwQ/Tv3x+AeN5806ZNOHr0KARBwMqVK3HttdeG/IOGVVIS3KMuVtyl8xTkrMCJiKglhJQuOTk5\n6NWrF/R6PX755RcMHjwYjz32WIPPKSoqQlFREbKzs7Fs2TK8+OKLAICZM2di06ZNAMQqe+HChRg0\naBByc3PxxRdfYOXKlQCAbt26YcmSJbjooovQo0cPpKenY86cOU35WcPDc/23+8cdirvDFeA8B05E\n8cSkM2HRsEUw6UzRHkrc0QjBViSR+fbbb/HMM89g8+bNmD59Om677TZkZGTAarWiV69eQZvKoi0z\nMxOlpaXhfVGbDUhIwAkz0H5+/d1mB1BrAB4a8hD+OvKv3vv/V/o/DFwzEAAgLAr+q9YsEafej957\nFB2TO4Z3zEREdEZqKMdUT6GPHDkSBw4cwKOPPoqMjAwAQFJSEh588MHwjfRMYBPPfbuVp7rDVoH7\nnkMnIjqTWe1WjFk/Bla7NdpDiTuqTrju3Lkz6GNRnc6OBk8Dn2+Aaz3FdXMDnIgonrgEFz458Alc\ngivaQ4k7qtJl3LhxOHHihPd2eXk5xo8fH7FBxbRgFXgzA/yREY+gY1JHtDO3a/YQiYgo/qlKlyNH\njiAtLc17u3379jhy5EjEBhXTGplCb+oU+INDH8TReUfZhU5ERKqoCnCXy+W9lhsA7HY77HZ7xAYV\n04IEuHTTt+LmdphE1Jol6BOw6rJVSNAnRHsocUdVgI8dOxZXX301Nm/ejM2bN+Oaa67BuHHjIj22\n2BQkwCV+Ad54kz8RUdwy6oyYWTATRp0x2kOJO6oC/NFHH0VeXh7uv/9+3H///TjvvPPw6KOPRnps\nsSnUAGcFTkStWLW9Gn1W9EG1PXJ7U7RWqk64GgwGLFq0CIsWLYr0eGLbJ58A118PIMAUuienfbcT\nZQVORK2ZW3Bjz/E9QfePoKZT3TH13XffYceOHYp10O+6666IDCpmjRnj/dKvAvfcZgVOREQtQVWA\nL126FG+99RaKi4sxbNgwfPrpp7j44otbV4D7VNL+U+gaAALPgRMRUYtQdQ58w4YN+Prrr5GZmYmN\nGzdi69at0Gpb2QIlv/6quMkKnIiocYmGRHw05SMkGhKjPZS4o6oCT0hIQEJCAtxuNwRBQE5ODg4c\nOBDpscWWH39U3BT8LiMTK3Df68BZgRNRa6bX6jGmx5jGD6SQqQpws9kMh8OBvLw8zJs3D5mZmXC5\nWtmyeDbl/t/ButB9dUrpBAAYkDEg3CMiIop5lbZKZC7PROncUlhMlmgPJ66omgd//vnnYbfb8cQT\nT6CyshL//e9/8fLLL0d6bLHF4VDc9J9CD5zoXdp0wXczv8On0z6N0MCIiGJblb0q2kOIS41W4C6X\nCy+//EUc9UQAACAASURBVDIee+wxJCUlYdWqVS0xrtgjW4kOCLQSm3hHoCnzwozCiA2LiIhap0Yr\ncJ1Oh//85z8tMZbY1kgF7ntOnIiIKJJU70b26KOP4siRI6isrPT+aVUaqcCDTaETEbVmSYYk7Lp1\nF5IMSdEeStxR1cT28MMPAwD+/Oc/Q6PRQBDEbutW1cjWWIATEZEfrUaLzqmdVW+tTOqp+o263W7v\nH5fL5f27VQkyhX7Fz8CnL8nOgfO6byIiryp7FVKXpbKRLQL4kUitIBX4sN+AUQfBKXQiImpRqqbQ\ntVqt3wIlAFpXFR4kwKVNTMD8JiKiFqQqwKuq6qc+amtr8dJLL7Wu8AaCTqFrdXoATjDBiYioJama\nQk9KSvL+ad++PebOnYu33nor0mOLLUEqcK1DvF/jWRueS6cSEdVLMaagYkEFUowp0R5K3GnSOfBf\nfvkF5eXl4R5LbAtWgUt3JPESCSIiX27BjZKKEu4HHgGqptDbtm3rPQfucrkgCAKeeeaZiA4s5gSr\nwO+7H2h/KbB9ShQGRUQU26wOK/o+3xcVCyq4FnqYqQrwHTt21D9Br8dZZ50FnU4XsUHFpGAB3q0H\ncN4IYLt4m5eRERFRS1A1ha7RaNCxY0ecc845yMjIgMPhQElJSaTHFlukKfSVK4ExY+oD3LM4QaAu\nfSIiokhRFeBXXXWV4rYgCH73xT2pAp8yBfj3v+G+43YA4OpCRESNYANbZKhKH7vdjoSEBO9ts9kM\nm8/+2HFPqsD1ekCjgXvspQD8A1zDy8mIiLwsJgsqH6jk+e8IUD2FfuzYMe/to0ePtr7LpaQK3GAA\nAG9HpW+A8xw4EVE9p9uJj/d/DKfb2fjBFBJVTWx33XUXLrzwQkybNg0AsH79eixatCiiA4s5UoB7\nrvf2DXBW3kRE/mocNbj0lUvZhR4BqgL8pptuQteuXfHhhx8CAF588UUMGTIkogOLOQ6HWH17mtWC\nVeBEREQtQVWA19XVYdiwYRg+fDgAcXeyuro6xXnxuOd0iue/PaRTCAxwIiKKBlXpM3LkSFRWVnpv\nV1VVYdSoUREbVEzyCfCg58BbW28AEVEDtBoteqf3ZrETAaoq8JqaGqSmpnpvp6amorq6OmKDiknS\nFLqH3zlwXgdOROQn2ZiM3bftjvYw4pKqj0Rut1sR2JWVlXA6W1lHocoKnIiI6tlddqzevhp2lz3a\nQ4k7qtJnypQpGDVqFNatW4d169Zh9OjRuPHGGyM9ttjicDDAiYhCVOesw6z3ZqHOWRftocQdVVPo\n8+fPx1lnnYUPPvgAGo0Gd955J5Ja2+5bTmeDU+gSXgdOREQtQVWAA8CNN96IAQMGYM2aNbj33nuR\nmZmJK664IpJjiy2NTKHzOnAiImpJjQZ4TU0NXn/9daxZswYHDx5EbW0tvvnmG5x77rktMb7YEWQK\nnc1rRETB6TQ6jO4+GjpNK9vBsgU0GOCzZs3C22+/jaFDh2L+/PkYO3Ysevbs2frCG8BhUy2mjTiK\n6qJ8AMCJmhMAeBkZEVFDkoxJ+Hjqx9EeRlxqsAPrtddeQ25uLubMmYPx48dDr9e32orzm7bV+Kp9\nDY5WH0W1vRomvQnnn30+cjvmAgDevPpNXNT5Itycf3OUR0pEFDtsThsWb14Mm7OVbYDVAhqswMvK\nyvD666/j4YcfxuzZs3HDDTfAIe3K1cq4XS4AwIpxK3Blryv9Hh+QOQBf3fxVSw+LiCim2Vw2LPly\nCeZeOBcmvSnaw4krDVbgycnJmDFjBr7++mt89NFHqKurg91ux6BBg7BixYqWGmNMENxigLfWGQgi\nIootqi9i7t27Nx5//HH8/vvvuPfee/HBBx9EclwxR/BU4Ow2JyKiWBDyKiR6vR6TJk1qfQHOCpyI\nKGQGrQEz8mfAoDU0fjCFRPV14K2d4ObKa0REoTIbzFg9YXW0hxGXmEYqud3i2u+cQiciUq/WUYuZ\nm2ai1lEb7aHEHQa4SlIFzil0IiL1HG4H1vywBg5367yCKZIY4GoIQn2AswInIqIYwABXw+WC4Mlt\nngMnIqJYwDRSw+n07jHGKXQiIvVMOhMWDVsEk46LuIQbu9DVcDjg9uQ2p9CJiNQz6U1YPHxxtIcR\nl1iBq+F0eqfQWYETEalntVsxZv0YWO3WaA8l7jDA1ZBPobMCJyJSzSW48MmBT+ASXNEeStxhgKvh\ncLCJjYiIYgrTSA02sRERUYxhgKvBJjYioiZJ0Cdg1WWrkKBPiPZQ4g4DXI127SBceQUAVuBERKEw\n6oyYWTATRp0x2kOJOwxwNdq2hTB8OABW4EREoai2V6PPij6otldHeyhxJ+IBvm/fPgwaNAjZ2dko\nLCzE7t27/Y7ZvHkzzGYz8vLyvH9qa2sbfawlCZ6z4GxiIyJSzy24sef4HrgFd7SHEncivpDLnDlz\nMHv2bEyfPh1vvfUWpk+fjq1bt/odl5OTgx07dgR8jYYeaymCIAY4p9CJiCgWRLScPHbsGLZt24ap\nU6cCACZNmoSSkhLs378/kt82IqRPj5xCJyKiWBDRAC8pKUGnTp2g14uFvkajQVZWFoqLi/2OPXDg\nAAoKClBYWIgVK1aofqylSFPorMCJiNRLNCTioykfIdGQGO2hxJ2YWAu9oKAApaWlSE1NRWlpKcaN\nG4f27dtj8uTJDT7ma/ny5Vi+fLn3dnV1+JomvFPorMCJiFTTa/UY02NMtIcRlyJagXfu3BllZWVw\nOp0AxBAsLi5GVlaW4jiLxYLU1FQAQGZmJq677jps2bKl0cd8zZ07F6Wlpd4/ycnJYftZ2MRGRBS6\nSlslLH+zoNJWGe2hxJ2IplGHDh1QUFCA9evXAwA2btyIzMxM9OjRQ3FcWVkZ3G7xHHNVVRXef/99\n5OfnN/pYS2ITGxFR01TZq6I9hLgU8XKyqKgIRUVFyM7OxrJly/Diiy8CAGbOnIlNmzYBEIO9X79+\n6N+/PwYOHIhLLrkEN910U6OPtSQ2sRERUSzRCFJpGYcyMzNRWloaltdaumUpHvziQXw/+3sUdCoI\ny2sSEcW7SlslUpelomJBBSwmS7SHc8ZpKMd4QlclNrEREYUuyZCEXbfuQpIhKdpDiTsMcJXYxEZE\nFDqtRovOqZ353hkB/I2qxCY2IqLQVdmrkLoslY1sEcAAV4lNbEREFEsY4CpxJTYiIoolDHCV2MRG\nRESxhAGuEpvYiIhCl2JMQcWCCqQYU6I9lLjDNFKJTWxERKFzC26UVJRwP/AIYICrxCY2IqLQWR1W\n9H2+L6wOa7SHEncY4CqxiY2IiGIJA1wlNrEREVEsYYCrxCY2IqKmYQNbZOijPYAzBZvYiIhCZzFZ\nUPkA9wKPBJaTKrGJjYgodE63Ex/v/xhOtzPaQ4k7DHCV2MRGRBS6GkcNLn3lUtQ4aqI9lLjDAFeJ\nTWxERBRLGOAqsYmNiIhiCdNIJTaxERGFTqvRond6bxY/EcAudJXYxEZEFLpkYzJ237Y72sOIS/xI\npBKb2IiIQmd32bF6+2rYXfZoDyXuMMBVYhMbEVHo6px1mPXeLNQ566I9lLjDAFeJTWxERBRLmEYq\nsYmNiIhiCQNcJTaxERGFTqfRYXT30dBpdNEeStxhF7pKbGIjIgpdkjEJH0/9ONrDiEuswFWSptB5\nDpyISD2b04bFmxfD5rRFeyhxh2mkkrcC5xQ6EZFqNpcNS75cApuLAR5uDHCV2MRGRESxhAGuEpvY\niIgoljDAVWITGxFR6AxaA2bkz4BBa4j2UOIOu9BV4kIuREShMxvMWD1hdbSHEZeYRipxKVUiotDV\nOmoxc9NM1Dpqoz2UuMMAV4lT6EREoXO4HVjzwxo43I5oDyXuMMBVYhMbERHFEga4SryMjIiIYgkD\nXCU2sRERhc6kM2HRsEUw6UzRHkrcYRe6SmxiIyIKnUlvwuLhi6M9jLjEclIlNrEREYXOardizPox\nsNqt0R5K3GGAq8QmNiKi0LkEFz458AlcgivaQ4k7DHCV2MRGRESxhAGuEncjIyKiWMIAV4kVOBFR\n6BL0CVh12Sok6BOiPZS4wy50lQQIrL6JiEJk1Bkxs2BmtIcRl1iBq+QW3Ky+iYhCVG2vRp8VfVBt\nr472UOIOA1wlQWAFTkQUKrfgxp7je7xX8lD4MMBVEiBwFTYiIooZTCSVBEHgFDoREcUMBrhKbGIj\nIgpdoiERH035CImGxGgPJe6wC10lNrEREYVOr9VjTI8x0R5GXGIFrhKb2IiIQldpq4TlbxZU2iqj\nPZS4wwBXiU1sRERNU2WvivYQ4hITSSU2sRERUSxhgKvEJjYiIoolDHCV2MRGRBS6JEMSdt26C0mG\npGgPJe4wwFViExsRUei0Gi06p3ZmD1EE8DeqEpvYiIhCV2WvQuqyVDayRQATSSU2sRERUSxhgKvE\nJjYiIoolDHCV2MRGRESxhAGuEpvYiIhCl2JMQcWCCqQYU6I9lLjDAFeJTWxERKFzC26UVJRwP/AI\niHgi7du3D4MGDUJ2djYKCwuxe/duv2M2b94Ms9mMvLw875/a2lrv42vWrEHPnj3RvXt3zJo1Cw6H\nI9LD9sMmNiKi0FkdVvR9vi+sDmu0hxJ3Ih7gc+bMwezZs/Hrr79i/vz5mD59esDjcnJysGPHDu8f\ns9kMADh06BD+/Oc/Y8uWLdi/fz/++OMPvPDCC5Eeth82sRERUSyJaIAfO3YM27Ztw9SpUwEAkyZN\nQklJCfbv36/6Nd566y1MmDABZ511FjQaDW655Ra8+uqrkRpyUGxiIyKiWBLRAC8pKUGnTp2g14vb\njms0GmRlZaG4uNjv2AMHDqCgoACFhYVYsWKF9/7i4mKcc8453ttdunQJ+HwAWL58OTIzM71/qqur\nw/azsImNiKhp2MAWGfpoDwAACgoKUFpaitTUVJSWlmLcuHFo3749Jk+eHNLrzJ07F3PnzvXezszM\nDNsY2cRGRBQ6i8mCyge4F3gkRDSROnfujLKyMjidTgBiFVtcXIysrCzFcRaLBampqQDE0L3uuuuw\nZcsWAEBWVhZ+++0377GHDx/2e35LYBMbEVHonG4nPt7/MZxuZ7SHEnciGuAdOnRAQUEB1q9fDwDY\nuHEjMjMz0aNHD8VxZWVlcLvFSwyqqqrw/vvvIz8/H4B43nzTpk04evQoBEHAypUrce2110Zy2AGx\niY2IKHQ1jhpc+sqlqHHURHsocSfic8JFRUUoKipCdnY2li1bhhdffBEAMHPmTGzatAmAGOz9+vVD\n//79MXDgQFxyySW46aabAADdunXDkiVLcNFFF6FHjx5IT0/HnDlzIj1sP2xiIyKiWKIRBEGI9iAi\nJTMzE6WlpWF5rYKiAlTYKnDgrgNheT0iotag0laJ1GWpqFhQAYvJEu3hnHEayjF2ZanEKXQiotBp\nNVr0Tu/NJuAIiIku9DMBm9iIiEKXbEzG7tv8V+Ck5uNHIpVYgRMRhc7usmP19tWwu+zRHkrcYYCr\nxCY2IqLQ1TnrMOu9Wahz1kV7KHGHAa6SIHAhFyIiih1MJJU4hU5ERLGEAa4Sm9iIiEKn0+gwuvto\n6DS6aA8l7rALXSUBArT8vENEFJIkYxI+nvpxtIcRl5hIKrGJjYgodDanDYs3L4bNaYv2UOIOA1wl\nNrEREYXO5rJhyZdLYHMxwMONiaQSm9iIiCiWMMBVYhMbERHFEga4SqzAiYhCZ9AaMCN/BgxaQ7SH\nEnfYha4Sm9iIiEJnNpixesLqaA8jLrECV4lNbEREoat11GLmppmoddRGeyhxh4mkEqfQiYhC53A7\nsOaHNXC4HdEeStxhgKvEJjYiIoolDHCVWIETEVEsYYCrxCY2IqLQmXQmLBq2CCadKdpDiTvsQleJ\nTWxERKEz6U1YPHxxtIcRl5hIKnEKnYgodFa7FWPWj4HVbo32UOIOA1wlNrEREYXOJbjwyYFP4BJc\n0R5K3GGAq8QKnIiIYgkDXCU2sRERUSxhgKvEJjYiotAl6BOw6rJVSNAnRHsocYdd6CpxCp2IKHRG\nnREzC2ZGexhxiSWlSmxiIyIKXbW9Gn1W9EG1vTraQ4k7DHCVWIETEYXOLbix5/geuAV3tIcSdxjg\nKrECJyKiWMIAV8ktuNnERkREMYOJpBKn0ImIQpdoSMRHUz5CoiEx2kOJO+xCV4lT6EREodNr9RjT\nY0y0hxGXWIGrxAqciCh0lbZKWP5mQaWtMtpDiTsMcJVYgRMRNU2VvSraQ4hLDHCV2MRGRESxhImk\nEqfQiYgoljDAVeIUOhFR6JIMSdh16y4kGZKiPZS4wwBXiRU4EVHotBotOqd25inICOBvVCVW4ERE\noauyVyF1WSob2SKAAa4Sm9iIiCiWMJFU4hQ6ERHFEgZ4CDiFTkREsYIBroIgCADACpyIKEQpxhRU\nLKhAijEl2kOJOwxwFQSIAc5z4EREoXELbpRUlHA/8AhgIqkg/cPjFDoRUWisDiv6Pt8XVoc12kOJ\nOwxwFTiFTkREsYYBroI0hc4KnIiIYgUDXAVW4ERETccGtsjQR3sAZwI2sRERNY3FZEHlA9wLPBIY\n4CqwiY2IYpXb7fbOEsYip9uJzYc3Y3iX4dBrGTm+NBoNtNqmFYf8barAKXQiijV2ux3FxcVwOBzR\nHkqD3IIbQoWA/Y79nMUMwmAwICsrC0ajMaTnMcBVYBMbEcWa4uJipKSkIC0tLabfm1xuF2r/qEXP\njj2h0+qiPZyYIwgCTpw4geLiYvTo0SOk5zLAVWAFTkSxxO12w+FwIC0tDXp9jL+NawBoAZ1OxwAP\nIi0tDSdPnoTb7Q5pOp3zGSqwiY2IYom3qIjhylsuQZ8Q7SHENOm/Y6i9DDH+0S02eJvYWIETEYVE\np9Whb4e+0R5GXGJJqcKZ9mmXiKgl5eXlIS8vD71794ZOp/Pevuaaa+AW3DhuPa56LfSbbroJW7Zs\nafS45557Dk8++WRzh94sbrcbixcvht1uj8r3ZwWugreJjRU4EZGfHTt2AAAOHz6MvLw8721AbGL7\nreI3tDO3AzSA0+ls8Lz9iy++qOp73n777c0bdBi43W4sWbIE8+bNC7mDPBwY4CqwAieimDZhAnDg\nQGReu3t3YNOmJj/9s88+wx1334FBhYPw448/4i9/+QusViueeeYZOBwOCIKApUuXYty4cQCAwYMH\nY8GCBRg/fjymTp2KlJQU7N27F6Wlpejfvz82bNgAg8GAhx56CHV1dXj88cexevVqvPnmm2jbti12\n794Ns9mMN954A126dAEAPPTQQ3jttdfQtm1bjB49Gq+//jr279/vN9aioiI89dRTMBqNcLvdWLt2\nLc4//3zs3bsXd999N8rLy2Gz2XDrrbfi1ltvxS233AIAGDRoELRaLT7//HOkpaU1+XcVKga4Cmxi\nIyJqugO/HMDqlasxbOgwAEB5eTmmTp0KjUaDgwcPYtCgQSgpKYHBYPB77o8//ojPP/8cRqMRF110\nEd59911cffXVfsf973//w48//ohzzjkH8+bNwz/+8Q8899xz+Ne//oX33nsPO3bsQFJSEm644Yag\n45w7dy4OHTqEDh06wOFwwGazweFw4Prrr8err76K7OxsWK1WXHDBBRgwYABWrlyJNWvW4Ouvv0Zy\ncnL4fmEqMcBVYBMbEcW0ZlTILaFL9y4YPHiw9/bBgwcxZcoU/P7779Dr9Th58iR+++23gNdBT5w4\nEWazGQBQWFiIA0FmGgYPHoxzzjkHAHDhhRdi1apVAIDPP/8ckydP9gbsjBkz8M033wR8jYsvvhhT\np07F+PHjMW7cOPTo0QM7d+7Ezz//jMmTJ3uPq6mpwZ49e5Cbm9uE30b4RDzA9+3bhxtvvBHl5eVI\nTU3FunXr0KdPn4DHCoKAiy++GNu3b8fp06cBiOdUunfvjn79+nmP27hxI7p37x7poSvGBXAKnYgo\nVDqtDu1S2ymuAZ88eTL++c9/4oorrgAAWCwW1NXVBXx+QkL9JWg6nQ5Op7NZxzX0Pv6vf/0L27Zt\nw+bNmzF69Gg89thjyM7ORvv27RXn9SXBvkdLific8Jw5czB79mz8+uuvmD9/PqZPnx702CeffDJg\nMKekpGDHjh3ePy0Z3gCb2IiImsotuOFwOxRd6KdPn0bXrl0BAOvWrUNVVVXEvv/IkSPx1ltvwWq1\nQhAErF27NuBxDocDBw8eRGFhIe677z5MnDgRW7duRe/evWE2m/Hyyy97j923bx9Onz4NvV6PxMRE\nVFRURGz8DYlogB87dgzbtm3D1KlTAQCTJk1CSUlJwOaB3bt3491338WCBQsiOaQmYQVORNQ0giDA\n4XIoFil56qmncPnll6OgoAB79uxBRkZGxL7/FVdcgbFjxyIvLw+FhYVIS0tDmzZt/I5zOByYPn06\n+vXrh7y8POzcuRN33303DAYD3n//fbz++uvIzc1Fnz59MGvWLNTW1gIA7r33XowYMQJ5eXk4ceJE\nxH6OgIQI2rZtm5Cdna24r7CwUPj8888V99ntduHCCy8U9uzZIxw6dEhITU31Pnbo0CFBr9cL559/\nvpCfny8sWbJEcDqdAb/fE088IWRkZHj/yF+nOX6v/F3AYgj/9+//C8vrERE1h9PpFPbs2RP0vTCW\nOF1OYevvWwWnK3pjraysFARBENxut3DnnXcKd9xxR9TGEkhD/z0zMjKCPi8m2qqXLFmCiRMnolev\nXn6PderUCb///ju2bt2Kzz77DFu2bMETTzwR8HXmzp2L0tJS759wdQWyiY2I6Mw1ZcoU5Ofno3fv\n3jh69CiWLFkS7SGFRUSb2Dp37oyysjLvhfuCIKC4uBhZWVmK47788ksUFxfj2WefhdPpRGVlJbp0\n6YKtW7ciPT0dHTp0AAC0a9cON998MzZs2ID7778/kkNXEDiFTkTUJBpo0D6xfVQLoE0x3qXfVBGt\nwDt06ICCggKsX78egNg9npmZ6XepwJYtW/Dbb7/h8OHD+Oqrr2CxWHD48GGkp6fj2LFj3v1ubTYb\n3n77beTn50dy2H7YxEZE1DRarRZd2nQJaZctUifiv9GioiIUFRUhOzsby5Yt8y6TN3PmTFWfir76\n6ivk5+ejf//+KCgowFlnnYUHH3ww0sNWYAVORNQ0brcbh08fhtutbi10Ui/i14Hn5OQEvGh+9erV\nAY/v0qWL9xpwQLyIf+LEiREbnxpciY2IqGkECCivKUdnS+doDyXuMJFUYBMbERHFGga4CpxCJyIK\nbty4cXj22Wf97u/fvz/eefudBp+7ePFi3H333QDEZrN77rkn4HG7du3ybk7SkMOHD2PlypV+49u7\nd2+jz42kdevW4ZdffgnrazLAVUhPSscL41/AxF7RnconIopFM2bM8NsGdNu2bSgrK8NlEy7D2Sln\nqyqAJkyY0Ow9vgMF+IcffoicnJxmvW5zMcCjxGKyYNZ5s3BBxgXRHgoRUcyZMGECSkpKsHPnTu99\na9euxQ033ACT0YQTh09g6JChKCgoQO/evfHII48EfJ1169Z510cHxOq8Z8+eOO+88/Daa69573c6\nnRgzZgzOP/989OnTB9dffz2sVisA4JZbbsHevXuRl5eHCRMmABB7q6S1zPfv349Ro0YhNzcXeXl5\nePfdd72vq9FosHTpUlxwwQXo2rVr0L3Jv/32W5x33nnIy8tD37598fzzzwMAqqqqMGvWLFxwwQXI\nzc3F7NmzYbfbsXr1amzbtg333HMP8vLy8OGHHzbl1+yHu5EREZ3hJrw6AQdORWY/8O5tu2PTdQ1f\nMWQwGDBt2jSsXbsW//znP1FXV4dXX30VX3/9NQAxQD///HOYTCbU1tZi0KBBGDVqFAYOHBj0NT/4\n4AO8+eab+P7775GSkoJp06Z5H9PpdNiwYQPS0tIgCAJuu+02PPPMM1iwYAFWrlyJu+++O+DmI4C4\nqMvNN9+MOXPmYN++fRg4cCDy8/O9O5mZTCZ89913+OWXX1BYWIhp06ZBr1dG5d/+9jfMmzcP1113\nHQDg1KlTAMRlVYcMGYJVq1ZBEATMmjULTz31FO677z6sX78ed999t+IDSnMxwImIqNlmzJiBYcOG\n4e9//zvefvtt9OrVy7u6Zm1tLW677Tbs2LEDWq0WJSUl2LFjR4MBLm0DarFYAIgbY3311VcAxL6k\nJ598Eh988AGcTicqKiowaNCgRsdYVVWF7du347///S8AoGfPnhg8eDC2bNniDfApU6YAAM4991zo\n9XocPXoUmZmZitcZMWIE/vrXv2Lfvn0YOXKkd6vUd999F9988w2WL1/u/bl1Oh0ihQFORHSGa6xC\nbgm9e/dGjx498N5772Ht2rWYMWOG97GFCxeiffv2+OGHH6DX6zFx4sSg24cGIz+HvmHDBnzxxRf4\n8ssvYbFY8PTTT+OLL75o0rh9z82r2Zb07rvvxuWXX47PPvsMCxcuRN++fbFixQoIgoCNGzciOzu7\nSWMJFc+BExFRWMyYMQNLly7Fd999h2uuucZ7/6lTp5CZmQm9Xo+9e/fi008/bfS1Ro0ahTfffBNV\nVVUQBAEvvPCC4vXat28Pi8WCqqoqrFu3zvuYxWIJur1nSkoKCgoKvOe29+/fj6+++gpDhw4N6efc\nu3cvunbtilmzZmHhwoX49ttvAYg7nz322GPe0D916pR3982GxtVUDHAiIgqLa665Bnv37sXVV1+t\n2EzqoYcewosvvojc3FwsWLAAI0eObPS1xo0bh6uuugoFBQU4//zzFXto3HDDDaipqUFOTg7Gjh2L\nIUOGeB+Ttvzs27evt4lN7pVXXsHrr7+O/v3746qrrsLq1av99udozLPPPos+ffogPz8fDz30kHeD\nrSeffBJmsxl5eXnIzc3FxRdfjMOHDwMAZs+ejaVLl4a1iU0jCLJNWuNMZmbm/9/evYRE9bdxAP+e\ndBKNQJmyZHQcSo3CuXiLMbEhxIqgkiiCFkoEgptq18awJGpThrWJNgOpmJnmphtCeYGKjAzTiLQ8\nDn3v4AAAB5dJREFUONWYYiqkXbw87yKa99+/3lEn347H+X5WzvnNGZ/zzMM8nN+54e3bt1qHQUQ0\nr6ampvDq1SskJSX9X4+x0t/h7/v018e4B05ERKRDbOBEREQ6xAZORESkQ2zgREQ68+PSp0V8ClNQ\nCfR5G7wOnIhIZ5YsWQKDwYChoSEYjUY+aEnHRARDQ0MwGAxYsmRu+9Rs4EREOmQ2m9HX14ePHz9q\nHQr9IYPBMOdL2QA2cCIiXVq6dCkSEhIwPT3NqXQdUxRlznveP7CBExHpWKA//qR//OaJiIh0iA2c\niIhIh9jAiYiIdGhR3ws9LCwMK1eunJfP+vTp008356fZY+4Cx9wFjrkLHHMXuPnO3eDgIL5+/frb\nsUXdwOcTH4wSOOYucMxd4Ji7wDF3gfubueMUOhERkQ6xgRMREelQyIkTJ05oHYReZGZmah2CbjF3\ngWPuAsfcBY65C9zfyh2PgRMREekQp9CJiIh0iA2ciIhIh9jAZ9Dd3Y1NmzYhKSkJGRkZ6Orq0jqk\nBeXw4cOwWCxQFAXPnj3zLfeXN+YU+PLlC/Ly8pCUlAS73Y7c3Fz09PQAAAYGBrB9+3YkJiYiOTkZ\nLS0tvvX8jQWTrVu3wmazweFwIDs7G+3t7QBYd3PhdruhKAoaGhoAsO5my2KxYN26dXA4HHA4HKip\nqQGgUe0J+bVlyxZxu90iIlJbWyvp6enaBrTANDc3i8fjkfj4eGlvb/ct95c35lTk8+fPcvPmTZme\nnhYRkYsXL4rL5RIRkYMHD0pJSYmIiDx+/FhMJpN8+/ZtxrFgMjw87Pu7vr5ebDabiLDuZqu3t1cy\nMzPF6XTKjRs3RIR1N1v//q37QYvaYwP348OHD7J8+XKZmJgQEZHp6WlZtWqVdHd3axzZwvPPovaX\nN+b099ra2iQ+Pl5ERJYtWyZer9c3lpGRIY2NjTOOBSu32y12u511N0tTU1OSk5MjT548EZfL5Wvg\nrLvZ+V0D16r2OIXuh8fjQUxMDEJDvz91VVEUmM1m9PX1aRzZwuYvb8zp75WXl2P37t0YGhrCxMQE\nVq9e7RuzWCzo6+vzOxaM8vPzERcXh+PHj6OiooJ1N0tlZWXIyspCWlqabxnrbm7y8/NhtVpx6NAh\nDA4OalZ7bOBEGjt9+jR6enpw5swZrUPRlStXrsDj8eDUqVM4duyY1uHoQmdnJ+rq6lBcXKx1KLrV\n0tKCjo4OPH36FCtWrEBBQYFmsbCB+xEXFwev14vJyUkAgIigr68PZrNZ48gWNn95Y05/dvbsWdTX\n1+P27duIiIiA0WhEaGgo+vv7fe9RVRVms9nvWDArKCjA/fv3ERsby7qbQWtrK1RVRWJiIiwWCx49\neoTCwkJcu3aNdTdLP7bbYDDg6NGjaG1t1ew3jw3cj+joaKSmpqKyshIAUFdXh9jYWCQkJGgc2cLm\nL2/M6X+VlZWhuroajY2NiIyM9C3ft28fLl26BABoa2vDu3fv4HK5ZhwLFiMjI3j//r3vdUNDA4xG\nI+tuFoqKiuD1eqGqKlRVhdPpxOXLl1FUVMS6m4WxsTGMjIz4XldXVyMlJUW72vvjo+iL3MuXL8Xp\ndEpiYqKkpaVJR0eH1iEtKIWFhWIymSQkJESio6Nl7dq1IuI/b8ypiMfjEQCyZs0asdvtYrfbZePG\njSIi0t/fL7m5uZKQkCAbNmyQe/fu+dbzNxYsVFWVjIwMSU5OFpvNJjk5Ob6Tilh3c/PPk9hYdzN7\n/fq1OBwOsVqtkpycLLt27ZLe3l4R0ab2eCtVIiIiHeIUOhERkQ6xgRMREekQGzgREZEOsYETERHp\nEBs4ERGRDrGBExER6VCo1gEQkXYsFgvCwsIQHh7uW1ZRUQGr1Tpv/0NVVTgcjp9ugEFEf44NnCjI\n1dTUwOFwaB0GEc0Rp9CJ6BeKoqC4uBgpKSlISkpCVVWVb+zu3btITU2FzWaDy+XCixcvfGNutxsO\nhwN2ux3p6elQVdU3VlJSgrS0NCQkJODWrVt/c3OIFiXugRMFuf379/80hf7w4UMA35t4e3s73rx5\ng/T0dGRlZSEiIgIHDhxAU1MTrFYrqqqqsHfvXnR1daG5uRmlpaV48OABYmJiMD4+DgAYGBjA6Ogo\nbDYbTp48iTt37uDIkSPYsWOHJttLtFjwVqpEQcxisaChoeGXKXRFUaCqKuLj4wEAeXl52LNnD6Ki\nonDu3Dk0NTX53hsZGYnOzk6Ul5cjPDwcpaWlP32WqqpYv349xsfHoSgKRkdHYTQafU9nIqLAcAqd\niGZFUZSA1w0LC/OtHxISgqmpqfkKiyhosYET0W+53W4A3/egW1tbkZ2dDafTiefPn6OzsxMAcPXq\nVZhMJphMJuzcuROVlZXwer0AgPHxcd80OhHNPx4DJwpy/z4Gfv78eQDA1NQUUlJSMDY2hgsXLsBi\nsQAAqqqqkJ+fj8nJSURFRaG2thaKomDz5s0oKSnBtm3boCgKli5diuvXr2uxSURBgcfAiegXiqJg\neHgYkZGRWodCRP8Dp9CJiIh0iFPoRPQLTswRLXzcAyciItIhNnAiIiIdYgMnIiLSITZwIiIiHWID\nJyIi0iE2cCIiIh36D2GPuSJRVujuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHnCAYAAACcxKXnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEDWU/wctX6+tF+hmEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJP9NbI95AbP",
        "colab_type": "text"
      },
      "source": [
        "## Run 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dc61de2c-46da-4fb0-c9ba-322e085c8470",
        "id": "uLwPqcil42Vr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Model 2\n",
        "\n",
        "model_2 = models.Sequential()\n",
        "model_2.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model_2.add(layers.MaxPooling2D((2, 2)))\n",
        "model_2.add(layers.Flatten())\n",
        "model_2.add(layers.Dense(64, activation='relu'))\n",
        "model_2.add(layers.Dropout(0.5))\n",
        "model_2.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 148, 148, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 72, 72, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 36, 36, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 34, 34, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 73984)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4735040   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,104,769\n",
            "Trainable params: 5,104,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsnGIIyK5Zxp",
        "colab_type": "code",
        "outputId": "af2b95b4-aa07-424d-e025-e3728fd14fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Early stopping (stop training after the validation loss reaches the minimum)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=80, verbose=1)\n",
        "\n",
        "# Callback for checkpointing\n",
        "checkpoint = ModelCheckpoint('model_2_benmal_best.h5', \n",
        "        monitor='val_loss', mode='min', verbose=1, \n",
        "        save_best_only=True, save_freq='epoch'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(optimizer=RMSprop(learning_rate=0.001, decay=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history_2 = model_2.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=int(0.8*n_train_img) // 128,\n",
        "        epochs=500,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint],\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        initial_epoch=0\n",
        ")\n",
        "\n",
        "# Save\n",
        "models.save_model(model_2, 'model_2_benmal_end.h5')\n",
        "!cp model* \"/content/gdrive/My Drive/models/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.9689 - acc: 0.5734Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6909 - acc: 0.5605\n",
            "Epoch 00001: val_loss improved from inf to 0.68548, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 7s 424ms/step - loss: 1.8900 - acc: 0.5738 - val_loss: 0.6855 - val_acc: 0.5664\n",
            "Epoch 2/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6853 - acc: 0.5910Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6888 - acc: 0.5605\n",
            "Epoch 00002: val_loss did not improve from 0.68548\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6855 - acc: 0.5902 - val_loss: 0.6866 - val_acc: 0.5664\n",
            "Epoch 3/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6881 - acc: 0.5814Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6878 - acc: 0.5605\n",
            "Epoch 00003: val_loss improved from 0.68548 to 0.68239, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6879 - acc: 0.5797 - val_loss: 0.6824 - val_acc: 0.5664\n",
            "Epoch 4/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7217 - acc: 0.5828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6895 - acc: 0.5605\n",
            "Epoch 00004: val_loss improved from 0.68239 to 0.67863, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.7183 - acc: 0.5859 - val_loss: 0.6786 - val_acc: 0.5664\n",
            "Epoch 5/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6812 - acc: 0.5942Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6847 - acc: 0.5605\n",
            "Epoch 00005: val_loss improved from 0.67863 to 0.67359, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.6809 - acc: 0.5971 - val_loss: 0.6736 - val_acc: 0.5664\n",
            "Epoch 6/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6925 - acc: 0.5681Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6857 - acc: 0.5605\n",
            "Epoch 00006: val_loss did not improve from 0.67359\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6916 - acc: 0.5698 - val_loss: 0.6814 - val_acc: 0.5664\n",
            "Epoch 7/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6805 - acc: 0.6042Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6823 - acc: 0.5605\n",
            "Epoch 00007: val_loss did not improve from 0.67359\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6807 - acc: 0.6031 - val_loss: 0.6770 - val_acc: 0.5664\n",
            "Epoch 8/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6894 - acc: 0.5802Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.7021 - acc: 0.5605\n",
            "Epoch 00008: val_loss did not improve from 0.67359\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.6887 - acc: 0.5835 - val_loss: 0.6865 - val_acc: 0.5664\n",
            "Epoch 9/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6868 - acc: 0.5873Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6761 - acc: 0.5605\n",
            "Epoch 00009: val_loss improved from 0.67359 to 0.66774, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6859 - acc: 0.5877 - val_loss: 0.6677 - val_acc: 0.5664\n",
            "Epoch 10/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6769 - acc: 0.6021Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6731 - acc: 0.5605\n",
            "Epoch 00010: val_loss improved from 0.66774 to 0.66533, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6773 - acc: 0.5996 - val_loss: 0.6653 - val_acc: 0.5664\n",
            "Epoch 11/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6690 - acc: 0.5926Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6685 - acc: 0.5801\n",
            "Epoch 00011: val_loss improved from 0.66533 to 0.66500, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6688 - acc: 0.5917 - val_loss: 0.6650 - val_acc: 0.5832\n",
            "Epoch 12/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6677 - acc: 0.5935Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6684 - acc: 0.5723\n",
            "Epoch 00012: val_loss did not improve from 0.66500\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6659 - acc: 0.5961 - val_loss: 0.6677 - val_acc: 0.5738\n",
            "Epoch 13/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6620 - acc: 0.6010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6524 - acc: 0.6406\n",
            "Epoch 00013: val_loss improved from 0.66500 to 0.64616, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6629 - acc: 0.5996 - val_loss: 0.6462 - val_acc: 0.6449\n",
            "Epoch 14/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6551 - acc: 0.6005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6630 - acc: 0.6094\n",
            "Epoch 00014: val_loss did not improve from 0.64616\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6512 - acc: 0.6060 - val_loss: 0.6665 - val_acc: 0.6075\n",
            "Epoch 15/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6635 - acc: 0.6085Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6472 - acc: 0.6367\n",
            "Epoch 00015: val_loss did not improve from 0.64616\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6639 - acc: 0.6076 - val_loss: 0.6481 - val_acc: 0.6393\n",
            "Epoch 16/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6565 - acc: 0.6122Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6489 - acc: 0.6367\n",
            "Epoch 00016: val_loss did not improve from 0.64616\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.6567 - acc: 0.6085 - val_loss: 0.6473 - val_acc: 0.6411\n",
            "Epoch 17/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6487 - acc: 0.6339Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.7082 - acc: 0.5664\n",
            "Epoch 00017: val_loss did not improve from 0.64616\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.6455 - acc: 0.6369 - val_loss: 0.6932 - val_acc: 0.5701\n",
            "Epoch 18/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6498 - acc: 0.6249Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6446 - acc: 0.6328\n",
            "Epoch 00018: val_loss improved from 0.64616 to 0.64169, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.6516 - acc: 0.6210 - val_loss: 0.6417 - val_acc: 0.6374\n",
            "Epoch 19/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6443 - acc: 0.6240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6377 - acc: 0.6270\n",
            "Epoch 00019: val_loss improved from 0.64169 to 0.62993, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6449 - acc: 0.6226 - val_loss: 0.6299 - val_acc: 0.6299\n",
            "Epoch 20/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6445 - acc: 0.6329Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6459 - acc: 0.6035\n",
            "Epoch 00020: val_loss did not improve from 0.62993\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6452 - acc: 0.6324 - val_loss: 0.6402 - val_acc: 0.6037\n",
            "Epoch 21/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6591 - acc: 0.6324Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6461 - acc: 0.6621\n",
            "Epoch 00021: val_loss did not improve from 0.62993\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6585 - acc: 0.6299 - val_loss: 0.6432 - val_acc: 0.6654\n",
            "Epoch 22/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6305 - acc: 0.6377Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6357 - acc: 0.6348\n",
            "Epoch 00022: val_loss did not improve from 0.62993\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6307 - acc: 0.6403 - val_loss: 0.6306 - val_acc: 0.6393\n",
            "Epoch 23/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6505 - acc: 0.6234Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6359 - acc: 0.6387\n",
            "Epoch 00023: val_loss improved from 0.62993 to 0.62766, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 396ms/step - loss: 0.6490 - acc: 0.6270 - val_loss: 0.6277 - val_acc: 0.6449\n",
            "Epoch 24/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6427 - acc: 0.6191Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6342 - acc: 0.6191\n",
            "Epoch 00024: val_loss did not improve from 0.62766\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6420 - acc: 0.6185 - val_loss: 0.6306 - val_acc: 0.6224\n",
            "Epoch 25/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6311 - acc: 0.6395Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6318 - acc: 0.6426\n",
            "Epoch 00025: val_loss improved from 0.62766 to 0.62562, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6368 - acc: 0.6360 - val_loss: 0.6256 - val_acc: 0.6467\n",
            "Epoch 26/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6256 - acc: 0.6375Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6299 - acc: 0.6328\n",
            "Epoch 00026: val_loss did not improve from 0.62562\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.6256 - acc: 0.6387 - val_loss: 0.6350 - val_acc: 0.6318\n",
            "Epoch 27/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6400 - acc: 0.6162Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6387 - acc: 0.6289\n",
            "Epoch 00027: val_loss did not improve from 0.62562\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6409 - acc: 0.6168 - val_loss: 0.6563 - val_acc: 0.6224\n",
            "Epoch 28/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6331 - acc: 0.6344Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6226 - acc: 0.6484\n",
            "Epoch 00028: val_loss did not improve from 0.62562\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6331 - acc: 0.6343 - val_loss: 0.6367 - val_acc: 0.6467\n",
            "Epoch 29/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6373 - acc: 0.6297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6238 - acc: 0.6562\n",
            "Epoch 00029: val_loss did not improve from 0.62562\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.6362 - acc: 0.6339 - val_loss: 0.6286 - val_acc: 0.6523\n",
            "Epoch 30/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6107 - acc: 0.6509Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6563 - acc: 0.5938\n",
            "Epoch 00030: val_loss did not improve from 0.62562\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.6098 - acc: 0.6517 - val_loss: 0.6704 - val_acc: 0.5907\n",
            "Epoch 31/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6285 - acc: 0.6349Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6186 - acc: 0.6328\n",
            "Epoch 00031: val_loss improved from 0.62562 to 0.62315, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6261 - acc: 0.6328 - val_loss: 0.6231 - val_acc: 0.6318\n",
            "Epoch 32/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6231 - acc: 0.6451Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6303 - acc: 0.6230\n",
            "Epoch 00032: val_loss did not improve from 0.62315\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6243 - acc: 0.6478 - val_loss: 0.6380 - val_acc: 0.6206\n",
            "Epoch 33/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6322 - acc: 0.6451Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6286 - acc: 0.5996\n",
            "Epoch 00033: val_loss did not improve from 0.62315\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.6328 - acc: 0.6453 - val_loss: 0.6412 - val_acc: 0.5925\n",
            "Epoch 34/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6193 - acc: 0.6462Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6037 - acc: 0.6660\n",
            "Epoch 00034: val_loss improved from 0.62315 to 0.61464, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6201 - acc: 0.6468 - val_loss: 0.6146 - val_acc: 0.6617\n",
            "Epoch 35/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6200 - acc: 0.6292Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6115 - acc: 0.6270\n",
            "Epoch 00035: val_loss did not improve from 0.61464\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.6201 - acc: 0.6294 - val_loss: 0.6155 - val_acc: 0.6243\n",
            "Epoch 36/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6197 - acc: 0.6271Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6133 - acc: 0.6445\n",
            "Epoch 00036: val_loss did not improve from 0.61464\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6209 - acc: 0.6249 - val_loss: 0.6278 - val_acc: 0.6430\n",
            "Epoch 37/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6168 - acc: 0.6387Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6042 - acc: 0.6270\n",
            "Epoch 00037: val_loss improved from 0.61464 to 0.60686, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6171 - acc: 0.6349 - val_loss: 0.6069 - val_acc: 0.6280\n",
            "Epoch 38/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6264 - acc: 0.6520Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6066 - acc: 0.6582\n",
            "Epoch 00038: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6254 - acc: 0.6473 - val_loss: 0.6117 - val_acc: 0.6579\n",
            "Epoch 39/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6127 - acc: 0.6493Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6056 - acc: 0.6426\n",
            "Epoch 00039: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.6100 - acc: 0.6523 - val_loss: 0.6200 - val_acc: 0.6411\n",
            "Epoch 40/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6133 - acc: 0.6453Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6101 - acc: 0.6484\n",
            "Epoch 00040: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6151 - acc: 0.6458 - val_loss: 0.6138 - val_acc: 0.6486\n",
            "Epoch 41/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6237 - acc: 0.6307Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6066 - acc: 0.6426\n",
            "Epoch 00041: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6244 - acc: 0.6270 - val_loss: 0.6105 - val_acc: 0.6393\n",
            "Epoch 42/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6126 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6049 - acc: 0.6465\n",
            "Epoch 00042: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6157 - acc: 0.6508 - val_loss: 0.6079 - val_acc: 0.6449\n",
            "Epoch 43/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6211 - acc: 0.6408Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6121 - acc: 0.6445\n",
            "Epoch 00043: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6210 - acc: 0.6418 - val_loss: 0.6217 - val_acc: 0.6411\n",
            "Epoch 44/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6143 - acc: 0.6446Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6296 - acc: 0.6211\n",
            "Epoch 00044: val_loss did not improve from 0.60686\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6110 - acc: 0.6478 - val_loss: 0.6440 - val_acc: 0.6150\n",
            "Epoch 45/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6098 - acc: 0.6408Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6047 - acc: 0.6484\n",
            "Epoch 00045: val_loss improved from 0.60686 to 0.60591, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6085 - acc: 0.6448 - val_loss: 0.6059 - val_acc: 0.6467\n",
            "Epoch 46/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6192 - acc: 0.6472Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6040 - acc: 0.6660\n",
            "Epoch 00046: val_loss did not improve from 0.60591\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6216 - acc: 0.6468 - val_loss: 0.6184 - val_acc: 0.6654\n",
            "Epoch 47/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6086 - acc: 0.6446Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6018 - acc: 0.6543\n",
            "Epoch 00047: val_loss did not improve from 0.60591\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6103 - acc: 0.6453 - val_loss: 0.6205 - val_acc: 0.6542\n",
            "Epoch 48/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6081 - acc: 0.6525Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6171 - acc: 0.6406\n",
            "Epoch 00048: val_loss did not improve from 0.60591\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6086 - acc: 0.6498 - val_loss: 0.6346 - val_acc: 0.6393\n",
            "Epoch 49/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6080 - acc: 0.6525Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6124 - acc: 0.6348\n",
            "Epoch 00049: val_loss did not improve from 0.60591\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6053 - acc: 0.6542 - val_loss: 0.6150 - val_acc: 0.6318\n",
            "Epoch 50/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6128 - acc: 0.6424Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6092 - acc: 0.6328\n",
            "Epoch 00050: val_loss improved from 0.60591 to 0.59675, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.6128 - acc: 0.6433 - val_loss: 0.5968 - val_acc: 0.6318\n",
            "Epoch 51/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6151 - acc: 0.6552Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5975 - acc: 0.6406\n",
            "Epoch 00051: val_loss improved from 0.59675 to 0.59278, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.6128 - acc: 0.6572 - val_loss: 0.5928 - val_acc: 0.6449\n",
            "Epoch 52/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6164 - acc: 0.6493Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6057 - acc: 0.6582\n",
            "Epoch 00052: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.6157 - acc: 0.6478 - val_loss: 0.6020 - val_acc: 0.6598\n",
            "Epoch 53/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6095 - acc: 0.6520Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6074 - acc: 0.6484\n",
            "Epoch 00053: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6096 - acc: 0.6538 - val_loss: 0.6056 - val_acc: 0.6467\n",
            "Epoch 54/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6101 - acc: 0.6419Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6321 - acc: 0.6094\n",
            "Epoch 00054: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6110 - acc: 0.6428 - val_loss: 0.6339 - val_acc: 0.6093\n",
            "Epoch 55/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5953 - acc: 0.6531Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6003 - acc: 0.6523\n",
            "Epoch 00055: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5932 - acc: 0.6543 - val_loss: 0.5970 - val_acc: 0.6523\n",
            "Epoch 56/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6262 - acc: 0.6281Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5980 - acc: 0.6387\n",
            "Epoch 00056: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.6227 - acc: 0.6349 - val_loss: 0.5956 - val_acc: 0.6393\n",
            "Epoch 57/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6017 - acc: 0.6573Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5983 - acc: 0.6641\n",
            "Epoch 00057: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6063 - acc: 0.6517 - val_loss: 0.5989 - val_acc: 0.6636\n",
            "Epoch 58/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6056 - acc: 0.6505Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6017 - acc: 0.6445\n",
            "Epoch 00058: val_loss did not improve from 0.59278\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.6071 - acc: 0.6470 - val_loss: 0.6004 - val_acc: 0.6467\n",
            "Epoch 59/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6089 - acc: 0.6573Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5910 - acc: 0.6523\n",
            "Epoch 00059: val_loss improved from 0.59278 to 0.58880, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6081 - acc: 0.6572 - val_loss: 0.5888 - val_acc: 0.6542\n",
            "Epoch 60/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6136 - acc: 0.6504Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5950 - acc: 0.6543\n",
            "Epoch 00060: val_loss did not improve from 0.58880\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6119 - acc: 0.6562 - val_loss: 0.6039 - val_acc: 0.6523\n",
            "Epoch 61/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6003 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6025 - acc: 0.6523\n",
            "Epoch 00061: val_loss did not improve from 0.58880\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5986 - acc: 0.6633 - val_loss: 0.6140 - val_acc: 0.6449\n",
            "Epoch 62/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6120 - acc: 0.6484Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6283 - acc: 0.5781\n",
            "Epoch 00062: val_loss did not improve from 0.58880\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6122 - acc: 0.6453 - val_loss: 0.6182 - val_acc: 0.5850\n",
            "Epoch 63/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6005 - acc: 0.6521Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6025 - acc: 0.6582\n",
            "Epoch 00063: val_loss improved from 0.58880 to 0.58445, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5995 - acc: 0.6548 - val_loss: 0.5845 - val_acc: 0.6617\n",
            "Epoch 64/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5979 - acc: 0.6594Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5957 - acc: 0.6523\n",
            "Epoch 00064: val_loss did not improve from 0.58445\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5963 - acc: 0.6582 - val_loss: 0.6157 - val_acc: 0.6523\n",
            "Epoch 65/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6119 - acc: 0.6578Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6060 - acc: 0.6309\n",
            "Epoch 00065: val_loss did not improve from 0.58445\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6142 - acc: 0.6542 - val_loss: 0.6015 - val_acc: 0.6374\n",
            "Epoch 66/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5883 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6047 - acc: 0.6484\n",
            "Epoch 00066: val_loss did not improve from 0.58445\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5912 - acc: 0.6692 - val_loss: 0.6064 - val_acc: 0.6523\n",
            "Epoch 67/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6048 - acc: 0.6589Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5980 - acc: 0.6523\n",
            "Epoch 00067: val_loss did not improve from 0.58445\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6059 - acc: 0.6577 - val_loss: 0.5963 - val_acc: 0.6561\n",
            "Epoch 68/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6024 - acc: 0.6483Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5869 - acc: 0.6680\n",
            "Epoch 00068: val_loss did not improve from 0.58445\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5985 - acc: 0.6518 - val_loss: 0.5853 - val_acc: 0.6654\n",
            "Epoch 69/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6035 - acc: 0.6488Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5964 - acc: 0.6562\n",
            "Epoch 00069: val_loss improved from 0.58445 to 0.57842, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.6024 - acc: 0.6528 - val_loss: 0.5784 - val_acc: 0.6579\n",
            "Epoch 70/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5994 - acc: 0.6605Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5964 - acc: 0.6367\n",
            "Epoch 00070: val_loss improved from 0.57842 to 0.57457, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5977 - acc: 0.6617 - val_loss: 0.5746 - val_acc: 0.6430\n",
            "Epoch 71/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5954 - acc: 0.6719Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6160 - acc: 0.6406\n",
            "Epoch 00071: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5988 - acc: 0.6689 - val_loss: 0.6071 - val_acc: 0.6467\n",
            "Epoch 72/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6051 - acc: 0.6551Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5937 - acc: 0.6504\n",
            "Epoch 00072: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6025 - acc: 0.6587 - val_loss: 0.5994 - val_acc: 0.6505\n",
            "Epoch 73/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5973 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6010 - acc: 0.6543\n",
            "Epoch 00073: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5975 - acc: 0.6692 - val_loss: 0.6042 - val_acc: 0.6523\n",
            "Epoch 74/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6033 - acc: 0.6647Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5916 - acc: 0.6523\n",
            "Epoch 00074: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.6043 - acc: 0.6687 - val_loss: 0.6081 - val_acc: 0.6505\n",
            "Epoch 75/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5973 - acc: 0.6451Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5962 - acc: 0.6680\n",
            "Epoch 00075: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5971 - acc: 0.6468 - val_loss: 0.6135 - val_acc: 0.6617\n",
            "Epoch 76/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6076 - acc: 0.6599Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5859 - acc: 0.6621\n",
            "Epoch 00076: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6063 - acc: 0.6631 - val_loss: 0.5939 - val_acc: 0.6579\n",
            "Epoch 77/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6032 - acc: 0.6525Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5937 - acc: 0.6562\n",
            "Epoch 00077: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6029 - acc: 0.6523 - val_loss: 0.6009 - val_acc: 0.6523\n",
            "Epoch 78/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5914 - acc: 0.6599Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5993 - acc: 0.6602\n",
            "Epoch 00078: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5909 - acc: 0.6582 - val_loss: 0.6085 - val_acc: 0.6561\n",
            "Epoch 79/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6090 - acc: 0.6638Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5852 - acc: 0.6621\n",
            "Epoch 00079: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6055 - acc: 0.6623 - val_loss: 0.5964 - val_acc: 0.6617\n",
            "Epoch 80/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5985 - acc: 0.6642Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5933 - acc: 0.6660\n",
            "Epoch 00080: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5980 - acc: 0.6667 - val_loss: 0.5985 - val_acc: 0.6636\n",
            "Epoch 81/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5868 - acc: 0.6760Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5875 - acc: 0.6445\n",
            "Epoch 00081: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5873 - acc: 0.6782 - val_loss: 0.5980 - val_acc: 0.6467\n",
            "Epoch 82/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5990 - acc: 0.6426\n",
            "Epoch 00082: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5983 - acc: 0.6592 - val_loss: 0.6212 - val_acc: 0.6355\n",
            "Epoch 83/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5965 - acc: 0.6653Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5772 - acc: 0.6738\n",
            "Epoch 00083: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5959 - acc: 0.6672 - val_loss: 0.6187 - val_acc: 0.6692\n",
            "Epoch 84/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5992 - acc: 0.6647Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6543\n",
            "Epoch 00084: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5967 - acc: 0.6667 - val_loss: 0.6041 - val_acc: 0.6486\n",
            "Epoch 85/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6005 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5829 - acc: 0.6562\n",
            "Epoch 00085: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5989 - acc: 0.6756 - val_loss: 0.6035 - val_acc: 0.6486\n",
            "Epoch 86/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5895 - acc: 0.6753Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6063 - acc: 0.6387\n",
            "Epoch 00086: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5906 - acc: 0.6731 - val_loss: 0.6017 - val_acc: 0.6374\n",
            "Epoch 87/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5966 - acc: 0.6642Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5958 - acc: 0.6484\n",
            "Epoch 00087: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5937 - acc: 0.6677 - val_loss: 0.5870 - val_acc: 0.6523\n",
            "Epoch 88/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5924 - acc: 0.6674Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6047 - acc: 0.6504\n",
            "Epoch 00088: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5929 - acc: 0.6672 - val_loss: 0.6091 - val_acc: 0.6486\n",
            "Epoch 89/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5888 - acc: 0.6646Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6017 - acc: 0.6523\n",
            "Epoch 00089: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5906 - acc: 0.6650 - val_loss: 0.5799 - val_acc: 0.6561\n",
            "Epoch 90/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5928 - acc: 0.6800Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5982 - acc: 0.6504\n",
            "Epoch 00090: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5956 - acc: 0.6775 - val_loss: 0.5805 - val_acc: 0.6561\n",
            "Epoch 91/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5999 - acc: 0.6727Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6015 - acc: 0.6523\n",
            "Epoch 00091: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5981 - acc: 0.6726 - val_loss: 0.5875 - val_acc: 0.6523\n",
            "Epoch 92/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5956 - acc: 0.6716Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6042 - acc: 0.6445\n",
            "Epoch 00092: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5922 - acc: 0.6766 - val_loss: 0.5866 - val_acc: 0.6505\n",
            "Epoch 93/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5866 - acc: 0.6753Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5939 - acc: 0.6660\n",
            "Epoch 00093: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5928 - acc: 0.6741 - val_loss: 0.6132 - val_acc: 0.6617\n",
            "Epoch 94/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5888 - acc: 0.6663Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6059 - acc: 0.6562\n",
            "Epoch 00094: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5886 - acc: 0.6682 - val_loss: 0.6289 - val_acc: 0.6523\n",
            "Epoch 95/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5890 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5908 - acc: 0.6602\n",
            "Epoch 00095: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5873 - acc: 0.6781 - val_loss: 0.5994 - val_acc: 0.6542\n",
            "Epoch 96/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5915 - acc: 0.6562Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5868 - acc: 0.6465\n",
            "Epoch 00096: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5880 - acc: 0.6632 - val_loss: 0.5993 - val_acc: 0.6411\n",
            "Epoch 97/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5943 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5958 - acc: 0.6621\n",
            "Epoch 00097: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5925 - acc: 0.6675 - val_loss: 0.5979 - val_acc: 0.6598\n",
            "Epoch 98/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5894 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6002 - acc: 0.6250\n",
            "Epoch 00098: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5949 - acc: 0.6627 - val_loss: 0.5956 - val_acc: 0.6280\n",
            "Epoch 99/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5888 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5835 - acc: 0.6621\n",
            "Epoch 00099: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5898 - acc: 0.6726 - val_loss: 0.5883 - val_acc: 0.6579\n",
            "Epoch 100/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5901 - acc: 0.6721Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5927 - acc: 0.6602\n",
            "Epoch 00100: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5910 - acc: 0.6692 - val_loss: 0.5939 - val_acc: 0.6523\n",
            "Epoch 101/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5792 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5978 - acc: 0.6602\n",
            "Epoch 00101: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5748 - acc: 0.6811 - val_loss: 0.6209 - val_acc: 0.6561\n",
            "Epoch 102/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5796 - acc: 0.6740Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6028 - acc: 0.6738\n",
            "Epoch 00102: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5829 - acc: 0.6746 - val_loss: 0.5938 - val_acc: 0.6729\n",
            "Epoch 103/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5820 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5873 - acc: 0.6758\n",
            "Epoch 00103: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5813 - acc: 0.6711 - val_loss: 0.5972 - val_acc: 0.6748\n",
            "Epoch 104/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5861 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5857 - acc: 0.6699\n",
            "Epoch 00104: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5856 - acc: 0.6657 - val_loss: 0.5916 - val_acc: 0.6617\n",
            "Epoch 105/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5762 - acc: 0.6743Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5872 - acc: 0.6523\n",
            "Epoch 00105: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5814 - acc: 0.6721 - val_loss: 0.5889 - val_acc: 0.6486\n",
            "Epoch 106/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5824 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6165 - acc: 0.6309\n",
            "Epoch 00106: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5823 - acc: 0.6781 - val_loss: 0.6244 - val_acc: 0.6336\n",
            "Epoch 107/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5884 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5912 - acc: 0.6641\n",
            "Epoch 00107: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5892 - acc: 0.6772 - val_loss: 0.5901 - val_acc: 0.6598\n",
            "Epoch 108/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5960 - acc: 0.6686Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5847 - acc: 0.6621\n",
            "Epoch 00108: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5930 - acc: 0.6709 - val_loss: 0.5860 - val_acc: 0.6579\n",
            "Epoch 109/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5849 - acc: 0.6801Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5940 - acc: 0.6426\n",
            "Epoch 00109: val_loss did not improve from 0.57457\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5847 - acc: 0.6781 - val_loss: 0.5865 - val_acc: 0.6486\n",
            "Epoch 110/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5919 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5893 - acc: 0.6445\n",
            "Epoch 00110: val_loss improved from 0.57457 to 0.57231, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5952 - acc: 0.6731 - val_loss: 0.5723 - val_acc: 0.6505\n",
            "Epoch 111/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5777 - acc: 0.6667Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5928 - acc: 0.6406\n",
            "Epoch 00111: val_loss improved from 0.57231 to 0.57096, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5765 - acc: 0.6655 - val_loss: 0.5710 - val_acc: 0.6449\n",
            "Epoch 112/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5929 - acc: 0.6716Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6018 - acc: 0.6504\n",
            "Epoch 00112: val_loss did not improve from 0.57096\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5940 - acc: 0.6682 - val_loss: 0.5837 - val_acc: 0.6523\n",
            "Epoch 113/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5838 - acc: 0.6716Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5933 - acc: 0.6621\n",
            "Epoch 00113: val_loss did not improve from 0.57096\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5874 - acc: 0.6687 - val_loss: 0.5784 - val_acc: 0.6673\n",
            "Epoch 114/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5781 - acc: 0.6789Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5877 - acc: 0.6562\n",
            "Epoch 00114: val_loss improved from 0.57096 to 0.56981, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5808 - acc: 0.6754 - val_loss: 0.5698 - val_acc: 0.6598\n",
            "Epoch 115/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5760 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6126 - acc: 0.6562\n",
            "Epoch 00115: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5750 - acc: 0.6763 - val_loss: 0.5922 - val_acc: 0.6598\n",
            "Epoch 116/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5861 - acc: 0.6631Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5974 - acc: 0.6465\n",
            "Epoch 00116: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5868 - acc: 0.6617 - val_loss: 0.5843 - val_acc: 0.6486\n",
            "Epoch 117/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5895 - acc: 0.6679Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5986 - acc: 0.6445\n",
            "Epoch 00117: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5924 - acc: 0.6642 - val_loss: 0.5895 - val_acc: 0.6523\n",
            "Epoch 118/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5799 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5941 - acc: 0.6504\n",
            "Epoch 00118: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5786 - acc: 0.6741 - val_loss: 0.5766 - val_acc: 0.6523\n",
            "Epoch 119/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5930 - acc: 0.6679Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5934 - acc: 0.6680\n",
            "Epoch 00119: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5896 - acc: 0.6711 - val_loss: 0.5768 - val_acc: 0.6692\n",
            "Epoch 120/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5825 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5866 - acc: 0.6621\n",
            "Epoch 00120: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5842 - acc: 0.6776 - val_loss: 0.5932 - val_acc: 0.6598\n",
            "Epoch 121/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5835 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5929 - acc: 0.6523\n",
            "Epoch 00121: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5819 - acc: 0.6811 - val_loss: 0.6009 - val_acc: 0.6505\n",
            "Epoch 122/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5878 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5845 - acc: 0.6660\n",
            "Epoch 00122: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5858 - acc: 0.6771 - val_loss: 0.5968 - val_acc: 0.6617\n",
            "Epoch 123/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5768 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5966 - acc: 0.6660\n",
            "Epoch 00123: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5804 - acc: 0.6772 - val_loss: 0.6115 - val_acc: 0.6710\n",
            "Epoch 124/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5799 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5866 - acc: 0.6602\n",
            "Epoch 00124: val_loss did not improve from 0.56981\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5784 - acc: 0.6871 - val_loss: 0.6025 - val_acc: 0.6579\n",
            "Epoch 125/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5935 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5736 - acc: 0.6660\n",
            "Epoch 00125: val_loss improved from 0.56981 to 0.56839, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5926 - acc: 0.6726 - val_loss: 0.5684 - val_acc: 0.6692\n",
            "Epoch 126/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5790 - acc: 0.6807Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6398 - acc: 0.6191\n",
            "Epoch 00126: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5781 - acc: 0.6797 - val_loss: 0.6559 - val_acc: 0.6168\n",
            "Epoch 127/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5866 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6188 - acc: 0.6562\n",
            "Epoch 00127: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5814 - acc: 0.6731 - val_loss: 0.6040 - val_acc: 0.6598\n",
            "Epoch 128/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5863 - acc: 0.6697Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6050 - acc: 0.6504\n",
            "Epoch 00128: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5855 - acc: 0.6734 - val_loss: 0.5993 - val_acc: 0.6542\n",
            "Epoch 129/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5821 - acc: 0.6755Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5814 - acc: 0.6582\n",
            "Epoch 00129: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5801 - acc: 0.6738 - val_loss: 0.5706 - val_acc: 0.6636\n",
            "Epoch 130/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5837 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6095 - acc: 0.6309\n",
            "Epoch 00130: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5869 - acc: 0.6781 - val_loss: 0.5918 - val_acc: 0.6336\n",
            "Epoch 131/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5810 - acc: 0.6769Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5901 - acc: 0.6582\n",
            "Epoch 00131: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5793 - acc: 0.6781 - val_loss: 0.5773 - val_acc: 0.6579\n",
            "Epoch 132/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5766 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5927 - acc: 0.6758\n",
            "Epoch 00132: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5782 - acc: 0.6850 - val_loss: 0.5826 - val_acc: 0.6766\n",
            "Epoch 133/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5818 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5907 - acc: 0.6602\n",
            "Epoch 00133: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5785 - acc: 0.6781 - val_loss: 0.5782 - val_acc: 0.6654\n",
            "Epoch 134/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5869 - acc: 0.6700Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5993 - acc: 0.6133\n",
            "Epoch 00134: val_loss did not improve from 0.56839\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5825 - acc: 0.6776 - val_loss: 0.5882 - val_acc: 0.6206\n",
            "Epoch 135/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5804 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5803 - acc: 0.6660\n",
            "Epoch 00135: val_loss improved from 0.56839 to 0.56541, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5792 - acc: 0.6836 - val_loss: 0.5654 - val_acc: 0.6654\n",
            "Epoch 136/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5808 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5891 - acc: 0.6738\n",
            "Epoch 00136: val_loss improved from 0.56541 to 0.56290, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5784 - acc: 0.6905 - val_loss: 0.5629 - val_acc: 0.6766\n",
            "Epoch 137/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5770 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5858 - acc: 0.6621\n",
            "Epoch 00137: val_loss did not improve from 0.56290\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.5777 - acc: 0.6890 - val_loss: 0.5798 - val_acc: 0.6617\n",
            "Epoch 138/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5780 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5934 - acc: 0.6621\n",
            "Epoch 00138: val_loss did not improve from 0.56290\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5790 - acc: 0.6836 - val_loss: 0.5654 - val_acc: 0.6692\n",
            "Epoch 139/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5760 - acc: 0.6885Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5836 - acc: 0.6719\n",
            "Epoch 00139: val_loss improved from 0.56290 to 0.55908, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5758 - acc: 0.6890 - val_loss: 0.5591 - val_acc: 0.6748\n",
            "Epoch 140/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5799 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6816\n",
            "Epoch 00140: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5778 - acc: 0.6835 - val_loss: 0.5927 - val_acc: 0.6766\n",
            "Epoch 141/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5716 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5710 - acc: 0.6855\n",
            "Epoch 00141: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5697 - acc: 0.6831 - val_loss: 0.5718 - val_acc: 0.6766\n",
            "Epoch 142/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5851 - acc: 0.6700Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5815 - acc: 0.6699\n",
            "Epoch 00142: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5821 - acc: 0.6736 - val_loss: 0.5855 - val_acc: 0.6617\n",
            "Epoch 143/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5741 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5817 - acc: 0.6816\n",
            "Epoch 00143: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5700 - acc: 0.6826 - val_loss: 0.5879 - val_acc: 0.6766\n",
            "Epoch 144/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5781 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5841 - acc: 0.6699\n",
            "Epoch 00144: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5748 - acc: 0.6880 - val_loss: 0.5884 - val_acc: 0.6673\n",
            "Epoch 145/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5885 - acc: 0.6730Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5895 - acc: 0.6504\n",
            "Epoch 00145: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5863 - acc: 0.6729 - val_loss: 0.5910 - val_acc: 0.6449\n",
            "Epoch 146/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5778 - acc: 0.6966Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6895\n",
            "Epoch 00146: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5767 - acc: 0.6950 - val_loss: 0.5820 - val_acc: 0.6841\n",
            "Epoch 147/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6901Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5943 - acc: 0.6621\n",
            "Epoch 00147: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5703 - acc: 0.6895 - val_loss: 0.5797 - val_acc: 0.6617\n",
            "Epoch 148/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5910 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5764 - acc: 0.6758\n",
            "Epoch 00148: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5906 - acc: 0.6726 - val_loss: 0.5608 - val_acc: 0.6822\n",
            "Epoch 149/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5657 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5836 - acc: 0.6797\n",
            "Epoch 00149: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5656 - acc: 0.6930 - val_loss: 0.5676 - val_acc: 0.6822\n",
            "Epoch 150/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5794 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5924 - acc: 0.6543\n",
            "Epoch 00150: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5797 - acc: 0.6850 - val_loss: 0.5795 - val_acc: 0.6561\n",
            "Epoch 151/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5804 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5880 - acc: 0.6699\n",
            "Epoch 00151: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5790 - acc: 0.6910 - val_loss: 0.5763 - val_acc: 0.6748\n",
            "Epoch 152/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5750 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5785 - acc: 0.6719\n",
            "Epoch 00152: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5748 - acc: 0.6826 - val_loss: 0.5609 - val_acc: 0.6748\n",
            "Epoch 153/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5628 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5873 - acc: 0.6504\n",
            "Epoch 00153: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5685 - acc: 0.6955 - val_loss: 0.5722 - val_acc: 0.6561\n",
            "Epoch 154/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5712 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5839 - acc: 0.6582\n",
            "Epoch 00154: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.5699 - acc: 0.6990 - val_loss: 0.5627 - val_acc: 0.6636\n",
            "Epoch 155/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5722 - acc: 0.6797\n",
            "Epoch 00155: val_loss did not improve from 0.55908\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5710 - acc: 0.6880 - val_loss: 0.5596 - val_acc: 0.6841\n",
            "Epoch 156/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5730 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5708 - acc: 0.6836\n",
            "Epoch 00156: val_loss improved from 0.55908 to 0.54744, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5704 - acc: 0.6890 - val_loss: 0.5474 - val_acc: 0.6916\n",
            "Epoch 157/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5698 - acc: 0.6927Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5725 - acc: 0.6836\n",
            "Epoch 00157: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5712 - acc: 0.6924 - val_loss: 0.5564 - val_acc: 0.6860\n",
            "Epoch 158/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5618 - acc: 0.6903Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5774 - acc: 0.6797\n",
            "Epoch 00158: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5618 - acc: 0.6906 - val_loss: 0.5613 - val_acc: 0.6822\n",
            "Epoch 159/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.6907Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5921 - acc: 0.6719\n",
            "Epoch 00159: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5646 - acc: 0.6910 - val_loss: 0.5757 - val_acc: 0.6729\n",
            "Epoch 160/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5826 - acc: 0.6562Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5906 - acc: 0.6758\n",
            "Epoch 00160: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5790 - acc: 0.6602 - val_loss: 0.5708 - val_acc: 0.6804\n",
            "Epoch 161/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5672 - acc: 0.6962Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6738\n",
            "Epoch 00161: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5639 - acc: 0.6997 - val_loss: 0.5779 - val_acc: 0.6748\n",
            "Epoch 162/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5786 - acc: 0.6823Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5679 - acc: 0.6895\n",
            "Epoch 00162: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5776 - acc: 0.6821 - val_loss: 0.5598 - val_acc: 0.6935\n",
            "Epoch 163/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5658 - acc: 0.6805Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5913 - acc: 0.6660\n",
            "Epoch 00163: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5660 - acc: 0.6800 - val_loss: 0.5754 - val_acc: 0.6617\n",
            "Epoch 164/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5760 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5895 - acc: 0.6758\n",
            "Epoch 00164: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5766 - acc: 0.6812 - val_loss: 0.5690 - val_acc: 0.6729\n",
            "Epoch 165/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5630 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6030 - acc: 0.6523\n",
            "Epoch 00165: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5609 - acc: 0.6965 - val_loss: 0.6046 - val_acc: 0.6561\n",
            "Epoch 166/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5694 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6738\n",
            "Epoch 00166: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5689 - acc: 0.6900 - val_loss: 0.5859 - val_acc: 0.6766\n",
            "Epoch 167/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5708 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5976 - acc: 0.6621\n",
            "Epoch 00167: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5684 - acc: 0.6786 - val_loss: 0.6072 - val_acc: 0.6636\n",
            "Epoch 168/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5703 - acc: 0.6918Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5949 - acc: 0.6660\n",
            "Epoch 00168: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 350ms/step - loss: 0.5714 - acc: 0.6926 - val_loss: 0.5792 - val_acc: 0.6710\n",
            "Epoch 169/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5647 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5680 - acc: 0.6719\n",
            "Epoch 00169: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5665 - acc: 0.6978 - val_loss: 0.5737 - val_acc: 0.6673\n",
            "Epoch 170/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5713 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6010 - acc: 0.6543\n",
            "Epoch 00170: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5670 - acc: 0.6875 - val_loss: 0.5878 - val_acc: 0.6617\n",
            "Epoch 171/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5734 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5716 - acc: 0.6855\n",
            "Epoch 00171: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5764 - acc: 0.6816 - val_loss: 0.5769 - val_acc: 0.6879\n",
            "Epoch 172/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5628 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5751 - acc: 0.6816\n",
            "Epoch 00172: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5647 - acc: 0.6970 - val_loss: 0.5852 - val_acc: 0.6822\n",
            "Epoch 173/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5593 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5733 - acc: 0.6719\n",
            "Epoch 00173: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5611 - acc: 0.6816 - val_loss: 0.6130 - val_acc: 0.6617\n",
            "Epoch 174/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5575 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5747 - acc: 0.6777\n",
            "Epoch 00174: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5607 - acc: 0.6980 - val_loss: 0.5753 - val_acc: 0.6785\n",
            "Epoch 175/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5689 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5620 - acc: 0.6738\n",
            "Epoch 00175: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5687 - acc: 0.6865 - val_loss: 0.5745 - val_acc: 0.6729\n",
            "Epoch 176/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5697 - acc: 0.6771Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5798 - acc: 0.6680\n",
            "Epoch 00176: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5713 - acc: 0.6766 - val_loss: 0.5793 - val_acc: 0.6710\n",
            "Epoch 177/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5589 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5739 - acc: 0.6895\n",
            "Epoch 00177: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5634 - acc: 0.6995 - val_loss: 0.5776 - val_acc: 0.6897\n",
            "Epoch 178/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5677 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5715 - acc: 0.6543\n",
            "Epoch 00178: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5631 - acc: 0.6895 - val_loss: 0.5881 - val_acc: 0.6579\n",
            "Epoch 179/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5591 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5996 - acc: 0.6504\n",
            "Epoch 00179: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5572 - acc: 0.6880 - val_loss: 0.6113 - val_acc: 0.6467\n",
            "Epoch 180/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5863 - acc: 0.6686Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5729 - acc: 0.6738\n",
            "Epoch 00180: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5829 - acc: 0.6739 - val_loss: 0.5858 - val_acc: 0.6729\n",
            "Epoch 181/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5581 - acc: 0.6960Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5806 - acc: 0.6621\n",
            "Epoch 00181: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5581 - acc: 0.6970 - val_loss: 0.6133 - val_acc: 0.6579\n",
            "Epoch 182/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5623 - acc: 0.6938Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5829 - acc: 0.6660\n",
            "Epoch 00182: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5628 - acc: 0.6929 - val_loss: 0.5974 - val_acc: 0.6654\n",
            "Epoch 183/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5811 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5955 - acc: 0.6660\n",
            "Epoch 00183: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5760 - acc: 0.6826 - val_loss: 0.5556 - val_acc: 0.6710\n",
            "Epoch 184/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5563 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5883 - acc: 0.6699\n",
            "Epoch 00184: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5550 - acc: 0.6985 - val_loss: 0.5521 - val_acc: 0.6748\n",
            "Epoch 185/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5682 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5816 - acc: 0.6621\n",
            "Epoch 00185: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5671 - acc: 0.6821 - val_loss: 0.5516 - val_acc: 0.6710\n",
            "Epoch 186/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5615 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5924 - acc: 0.6641\n",
            "Epoch 00186: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5592 - acc: 0.6895 - val_loss: 0.5525 - val_acc: 0.6692\n",
            "Epoch 187/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5664 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5899 - acc: 0.6602\n",
            "Epoch 00187: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5643 - acc: 0.6935 - val_loss: 0.5553 - val_acc: 0.6673\n",
            "Epoch 188/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5606 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5730 - acc: 0.6855\n",
            "Epoch 00188: val_loss did not improve from 0.54744\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5614 - acc: 0.6900 - val_loss: 0.5481 - val_acc: 0.6879\n",
            "Epoch 189/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5544 - acc: 0.6987Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5749 - acc: 0.6836\n",
            "Epoch 00189: val_loss improved from 0.54744 to 0.54409, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5568 - acc: 0.6970 - val_loss: 0.5441 - val_acc: 0.6897\n",
            "Epoch 190/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5603 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5925 - acc: 0.6543\n",
            "Epoch 00190: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5590 - acc: 0.6945 - val_loss: 0.5566 - val_acc: 0.6598\n",
            "Epoch 191/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5698 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5779 - acc: 0.6699\n",
            "Epoch 00191: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5669 - acc: 0.6860 - val_loss: 0.5845 - val_acc: 0.6710\n",
            "Epoch 192/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5632 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5690 - acc: 0.6875\n",
            "Epoch 00192: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5659 - acc: 0.6955 - val_loss: 0.5652 - val_acc: 0.6879\n",
            "Epoch 193/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5837 - acc: 0.6719\n",
            "Epoch 00193: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5531 - acc: 0.6985 - val_loss: 0.5855 - val_acc: 0.6748\n",
            "Epoch 194/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5540 - acc: 0.7016Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5941 - acc: 0.6660\n",
            "Epoch 00194: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5601 - acc: 0.6943 - val_loss: 0.5912 - val_acc: 0.6654\n",
            "Epoch 195/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5589 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5783 - acc: 0.6797\n",
            "Epoch 00195: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5575 - acc: 0.6821 - val_loss: 0.5496 - val_acc: 0.6879\n",
            "Epoch 196/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5708 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5774 - acc: 0.6797\n",
            "Epoch 00196: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5675 - acc: 0.6967 - val_loss: 0.5621 - val_acc: 0.6841\n",
            "Epoch 197/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5823 - acc: 0.6680\n",
            "Epoch 00197: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5573 - acc: 0.6992 - val_loss: 0.5752 - val_acc: 0.6766\n",
            "Epoch 198/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5545 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5634 - acc: 0.6855\n",
            "Epoch 00198: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5522 - acc: 0.6906 - val_loss: 0.5733 - val_acc: 0.6860\n",
            "Epoch 199/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.7042Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5707 - acc: 0.6777\n",
            "Epoch 00199: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5519 - acc: 0.7061 - val_loss: 0.5791 - val_acc: 0.6785\n",
            "Epoch 200/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5548 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5722 - acc: 0.7012\n",
            "Epoch 00200: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5553 - acc: 0.6985 - val_loss: 0.5752 - val_acc: 0.7009\n",
            "Epoch 201/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5635 - acc: 0.6912Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5707 - acc: 0.6777\n",
            "Epoch 00201: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5681 - acc: 0.6895 - val_loss: 0.5700 - val_acc: 0.6822\n",
            "Epoch 202/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5601 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5729 - acc: 0.6973\n",
            "Epoch 00202: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5618 - acc: 0.7000 - val_loss: 0.5815 - val_acc: 0.6972\n",
            "Epoch 203/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5466 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5848 - acc: 0.6758\n",
            "Epoch 00203: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5485 - acc: 0.7069 - val_loss: 0.5909 - val_acc: 0.6766\n",
            "Epoch 204/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5586 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5634 - acc: 0.6660\n",
            "Epoch 00204: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5564 - acc: 0.6935 - val_loss: 0.5689 - val_acc: 0.6710\n",
            "Epoch 205/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.7156Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5759 - acc: 0.6621\n",
            "Epoch 00205: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5562 - acc: 0.7134 - val_loss: 0.5813 - val_acc: 0.6636\n",
            "Epoch 206/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5533 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5956 - acc: 0.6680\n",
            "Epoch 00206: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5581 - acc: 0.6920 - val_loss: 0.5944 - val_acc: 0.6692\n",
            "Epoch 207/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5661 - acc: 0.6896Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5888 - acc: 0.6660\n",
            "Epoch 00207: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5684 - acc: 0.6865 - val_loss: 0.5904 - val_acc: 0.6673\n",
            "Epoch 208/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.6962Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5652 - acc: 0.6914\n",
            "Epoch 00208: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5550 - acc: 0.6962 - val_loss: 0.5763 - val_acc: 0.6897\n",
            "Epoch 209/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5537 - acc: 0.7016Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6914\n",
            "Epoch 00209: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5476 - acc: 0.7095 - val_loss: 0.5801 - val_acc: 0.6935\n",
            "Epoch 210/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5599 - acc: 0.7032Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5706 - acc: 0.6816\n",
            "Epoch 00210: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5600 - acc: 0.7017 - val_loss: 0.5761 - val_acc: 0.6841\n",
            "Epoch 211/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5591 - acc: 0.6990Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5590 - acc: 0.6973\n",
            "Epoch 00211: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5584 - acc: 0.7002 - val_loss: 0.5670 - val_acc: 0.6972\n",
            "Epoch 212/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5535 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5729 - acc: 0.6719\n",
            "Epoch 00212: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5556 - acc: 0.7113 - val_loss: 0.5760 - val_acc: 0.6692\n",
            "Epoch 213/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5477 - acc: 0.7089Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5561 - acc: 0.7109\n",
            "Epoch 00213: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5453 - acc: 0.7104 - val_loss: 0.5851 - val_acc: 0.7065\n",
            "Epoch 214/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5480 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5710 - acc: 0.6895\n",
            "Epoch 00214: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5446 - acc: 0.7129 - val_loss: 0.5774 - val_acc: 0.6897\n",
            "Epoch 215/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.6984Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5610 - acc: 0.6973\n",
            "Epoch 00215: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5526 - acc: 0.7007 - val_loss: 0.5857 - val_acc: 0.6953\n",
            "Epoch 216/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5584 - acc: 0.7063Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5610 - acc: 0.6934\n",
            "Epoch 00216: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5579 - acc: 0.7039 - val_loss: 0.5961 - val_acc: 0.6897\n",
            "Epoch 217/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5497 - acc: 0.6953Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5639 - acc: 0.6719\n",
            "Epoch 00217: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5498 - acc: 0.6982 - val_loss: 0.5849 - val_acc: 0.6673\n",
            "Epoch 218/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5659 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5669 - acc: 0.6738\n",
            "Epoch 00218: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5629 - acc: 0.6900 - val_loss: 0.5972 - val_acc: 0.6654\n",
            "Epoch 219/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5653 - acc: 0.6934\n",
            "Epoch 00219: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5497 - acc: 0.6995 - val_loss: 0.5909 - val_acc: 0.6860\n",
            "Epoch 220/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5489 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5838 - acc: 0.6914\n",
            "Epoch 00220: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5471 - acc: 0.7034 - val_loss: 0.6290 - val_acc: 0.6860\n",
            "Epoch 221/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5553 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5662 - acc: 0.6914\n",
            "Epoch 00221: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5533 - acc: 0.7064 - val_loss: 0.5888 - val_acc: 0.6897\n",
            "Epoch 222/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5485 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5715 - acc: 0.6895\n",
            "Epoch 00222: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.5524 - acc: 0.6980 - val_loss: 0.6042 - val_acc: 0.6804\n",
            "Epoch 223/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5574 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5688 - acc: 0.6895\n",
            "Epoch 00223: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5584 - acc: 0.6990 - val_loss: 0.5955 - val_acc: 0.6822\n",
            "Epoch 224/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5477 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5761 - acc: 0.6816\n",
            "Epoch 00224: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5503 - acc: 0.7009 - val_loss: 0.5956 - val_acc: 0.6766\n",
            "Epoch 225/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5492 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5842 - acc: 0.6914\n",
            "Epoch 00225: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5461 - acc: 0.7014 - val_loss: 0.6092 - val_acc: 0.6841\n",
            "Epoch 226/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5556 - acc: 0.6997Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5768 - acc: 0.6836\n",
            "Epoch 00226: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5527 - acc: 0.7059 - val_loss: 0.5807 - val_acc: 0.6785\n",
            "Epoch 227/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5389 - acc: 0.6966Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5674 - acc: 0.6875\n",
            "Epoch 00227: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5368 - acc: 0.7014 - val_loss: 0.5740 - val_acc: 0.6822\n",
            "Epoch 228/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5623 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5819 - acc: 0.6836\n",
            "Epoch 00228: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5626 - acc: 0.6870 - val_loss: 0.5798 - val_acc: 0.6822\n",
            "Epoch 229/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7063Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5508 - acc: 0.7070\n",
            "Epoch 00229: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 0.5520 - acc: 0.7095 - val_loss: 0.5565 - val_acc: 0.7028\n",
            "Epoch 230/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5347 - acc: 0.7216Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5987 - acc: 0.6875\n",
            "Epoch 00230: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5351 - acc: 0.7189 - val_loss: 0.6011 - val_acc: 0.6841\n",
            "Epoch 231/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5603 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5653 - acc: 0.6953\n",
            "Epoch 00231: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5593 - acc: 0.6880 - val_loss: 0.5770 - val_acc: 0.6897\n",
            "Epoch 232/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5541 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5686 - acc: 0.6895\n",
            "Epoch 00232: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5530 - acc: 0.7103 - val_loss: 0.5782 - val_acc: 0.6841\n",
            "Epoch 233/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5546 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5629 - acc: 0.6992\n",
            "Epoch 00233: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5556 - acc: 0.7014 - val_loss: 0.5791 - val_acc: 0.6972\n",
            "Epoch 234/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5456 - acc: 0.7031Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5679 - acc: 0.6758\n",
            "Epoch 00234: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5447 - acc: 0.6997 - val_loss: 0.5839 - val_acc: 0.6710\n",
            "Epoch 235/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5497 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5528 - acc: 0.6895\n",
            "Epoch 00235: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5490 - acc: 0.7014 - val_loss: 0.5634 - val_acc: 0.6897\n",
            "Epoch 236/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5532 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5571 - acc: 0.7012\n",
            "Epoch 00236: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5484 - acc: 0.7173 - val_loss: 0.5764 - val_acc: 0.6953\n",
            "Epoch 237/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5469 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5696 - acc: 0.6797\n",
            "Epoch 00237: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5468 - acc: 0.7084 - val_loss: 0.5761 - val_acc: 0.6822\n",
            "Epoch 238/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5515 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5625 - acc: 0.6895\n",
            "Epoch 00238: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5491 - acc: 0.7039 - val_loss: 0.5797 - val_acc: 0.6897\n",
            "Epoch 239/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5463 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5635 - acc: 0.6895\n",
            "Epoch 00239: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 0.5472 - acc: 0.6990 - val_loss: 0.5727 - val_acc: 0.6860\n",
            "Epoch 240/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5482 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5625 - acc: 0.6914\n",
            "Epoch 00240: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5517 - acc: 0.7069 - val_loss: 0.5668 - val_acc: 0.6897\n",
            "Epoch 241/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5455 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5697 - acc: 0.7090\n",
            "Epoch 00241: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5443 - acc: 0.7154 - val_loss: 0.5557 - val_acc: 0.7084\n",
            "Epoch 242/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7167Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5768 - acc: 0.6836\n",
            "Epoch 00242: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5386 - acc: 0.7163 - val_loss: 0.5656 - val_acc: 0.6841\n",
            "Epoch 243/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5479 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5705 - acc: 0.6953\n",
            "Epoch 00243: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5486 - acc: 0.7098 - val_loss: 0.5580 - val_acc: 0.6953\n",
            "Epoch 244/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5493 - acc: 0.7026Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5691 - acc: 0.6719\n",
            "Epoch 00244: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5470 - acc: 0.7031 - val_loss: 0.5680 - val_acc: 0.6729\n",
            "Epoch 245/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5423 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5645 - acc: 0.6797\n",
            "Epoch 00245: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5447 - acc: 0.6865 - val_loss: 0.5646 - val_acc: 0.6822\n",
            "Epoch 246/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5403 - acc: 0.7076Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5721 - acc: 0.7012\n",
            "Epoch 00246: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5431 - acc: 0.7073 - val_loss: 0.5671 - val_acc: 0.7009\n",
            "Epoch 247/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.7146Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5834 - acc: 0.6602\n",
            "Epoch 00247: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5518 - acc: 0.7144 - val_loss: 0.5820 - val_acc: 0.6598\n",
            "Epoch 248/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5472 - acc: 0.7070Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5779 - acc: 0.6953\n",
            "Epoch 00248: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5449 - acc: 0.7088 - val_loss: 0.5644 - val_acc: 0.6953\n",
            "Epoch 249/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5359 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5620 - acc: 0.6914\n",
            "Epoch 00249: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5396 - acc: 0.7034 - val_loss: 0.5710 - val_acc: 0.6879\n",
            "Epoch 250/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7125Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6109 - acc: 0.6523\n",
            "Epoch 00250: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5328 - acc: 0.7173 - val_loss: 0.5859 - val_acc: 0.6561\n",
            "Epoch 251/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5517 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5774 - acc: 0.6895\n",
            "Epoch 00251: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5524 - acc: 0.6970 - val_loss: 0.5701 - val_acc: 0.6879\n",
            "Epoch 252/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5401 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6362 - acc: 0.6562\n",
            "Epoch 00252: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5400 - acc: 0.7134 - val_loss: 0.6112 - val_acc: 0.6579\n",
            "Epoch 253/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5496 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5523 - acc: 0.7051\n",
            "Epoch 00253: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5535 - acc: 0.7049 - val_loss: 0.5870 - val_acc: 0.6991\n",
            "Epoch 254/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5594 - acc: 0.7109\n",
            "Epoch 00254: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5403 - acc: 0.7104 - val_loss: 0.5978 - val_acc: 0.7084\n",
            "Epoch 255/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5420 - acc: 0.7167Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5516 - acc: 0.7109\n",
            "Epoch 00255: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5393 - acc: 0.7203 - val_loss: 0.6054 - val_acc: 0.7009\n",
            "Epoch 256/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5471 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5641 - acc: 0.6973\n",
            "Epoch 00256: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 5s 314ms/step - loss: 0.5469 - acc: 0.7014 - val_loss: 0.5931 - val_acc: 0.6935\n",
            "Epoch 257/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5361 - acc: 0.7183Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5536 - acc: 0.6934\n",
            "Epoch 00257: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5411 - acc: 0.7124 - val_loss: 0.5996 - val_acc: 0.6841\n",
            "Epoch 258/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5511 - acc: 0.7129\n",
            "Epoch 00258: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5458 - acc: 0.7029 - val_loss: 0.5785 - val_acc: 0.7028\n",
            "Epoch 259/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5460 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5670 - acc: 0.6973\n",
            "Epoch 00259: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5479 - acc: 0.7149 - val_loss: 0.6236 - val_acc: 0.6879\n",
            "Epoch 260/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5563 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6007 - acc: 0.6660\n",
            "Epoch 00260: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5493 - acc: 0.6975 - val_loss: 0.5971 - val_acc: 0.6673\n",
            "Epoch 261/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5477 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5515 - acc: 0.7129\n",
            "Epoch 00261: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5474 - acc: 0.7044 - val_loss: 0.5543 - val_acc: 0.7121\n",
            "Epoch 262/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5373 - acc: 0.7115Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5728 - acc: 0.6816\n",
            "Epoch 00262: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5375 - acc: 0.7114 - val_loss: 0.5682 - val_acc: 0.6841\n",
            "Epoch 263/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5658 - acc: 0.6855\n",
            "Epoch 00263: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5482 - acc: 0.7109 - val_loss: 0.5692 - val_acc: 0.6860\n",
            "Epoch 264/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5366 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5586 - acc: 0.6953\n",
            "Epoch 00264: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5386 - acc: 0.7049 - val_loss: 0.5568 - val_acc: 0.6953\n",
            "Epoch 265/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.7034Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5610 - acc: 0.6895\n",
            "Epoch 00265: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5434 - acc: 0.7019 - val_loss: 0.5632 - val_acc: 0.6879\n",
            "Epoch 266/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5339 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5901 - acc: 0.6641\n",
            "Epoch 00266: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5355 - acc: 0.7193 - val_loss: 0.5888 - val_acc: 0.6692\n",
            "Epoch 267/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5478 - acc: 0.7066Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5709 - acc: 0.6895\n",
            "Epoch 00267: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5463 - acc: 0.7069 - val_loss: 0.5891 - val_acc: 0.6879\n",
            "Epoch 268/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5410 - acc: 0.7146Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5544 - acc: 0.7012\n",
            "Epoch 00268: val_loss did not improve from 0.54409\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5389 - acc: 0.7168 - val_loss: 0.5451 - val_acc: 0.7028\n",
            "Epoch 269/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5424 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5623 - acc: 0.6777\n",
            "Epoch 00269: val_loss improved from 0.54409 to 0.54400, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5403 - acc: 0.7124 - val_loss: 0.5440 - val_acc: 0.6822\n",
            "Epoch 270/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5418 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5760 - acc: 0.6758\n",
            "Epoch 00270: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5409 - acc: 0.7144 - val_loss: 0.5745 - val_acc: 0.6766\n",
            "Epoch 271/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5470 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5994 - acc: 0.6582\n",
            "Epoch 00271: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5442 - acc: 0.7114 - val_loss: 0.5843 - val_acc: 0.6654\n",
            "Epoch 272/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5305 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5619 - acc: 0.6836\n",
            "Epoch 00272: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5329 - acc: 0.7228 - val_loss: 0.5564 - val_acc: 0.6841\n",
            "Epoch 273/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5357 - acc: 0.7182Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5844 - acc: 0.6895\n",
            "Epoch 00273: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5375 - acc: 0.7173 - val_loss: 0.5692 - val_acc: 0.6935\n",
            "Epoch 274/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5505 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5600 - acc: 0.6953\n",
            "Epoch 00274: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5470 - acc: 0.7094 - val_loss: 0.5584 - val_acc: 0.6953\n",
            "Epoch 275/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5337 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5554 - acc: 0.6992\n",
            "Epoch 00275: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5325 - acc: 0.7233 - val_loss: 0.5802 - val_acc: 0.6935\n",
            "Epoch 276/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.7003Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5489 - acc: 0.7207\n",
            "Epoch 00276: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5480 - acc: 0.7024 - val_loss: 0.5685 - val_acc: 0.7159\n",
            "Epoch 277/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5333 - acc: 0.7182Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5529 - acc: 0.6816\n",
            "Epoch 00277: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5360 - acc: 0.7173 - val_loss: 0.5892 - val_acc: 0.6748\n",
            "Epoch 278/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5390 - acc: 0.7162Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5690 - acc: 0.6914\n",
            "Epoch 00278: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5407 - acc: 0.7168 - val_loss: 0.5888 - val_acc: 0.6860\n",
            "Epoch 279/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5330 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5631 - acc: 0.7070\n",
            "Epoch 00279: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5357 - acc: 0.7079 - val_loss: 0.5805 - val_acc: 0.7028\n",
            "Epoch 280/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5369 - acc: 0.7183Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5529 - acc: 0.7070\n",
            "Epoch 00280: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5421 - acc: 0.7139 - val_loss: 0.5733 - val_acc: 0.7028\n",
            "Epoch 281/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5350 - acc: 0.7215Epoch 1/500\n",
            " 5/16 [========>.....................] - ETA: 3s - loss: 0.5989 - acc: 0.6953\n",
            "Epoch 00281: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5349 - acc: 0.7203 - val_loss: 0.5989 - val_acc: 0.6953\n",
            "Epoch 282/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5385 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5780 - acc: 0.6855\n",
            "Epoch 00282: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5365 - acc: 0.7064 - val_loss: 0.6229 - val_acc: 0.6729\n",
            "Epoch 283/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5426 - acc: 0.7063Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5615 - acc: 0.6895\n",
            "Epoch 00283: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5423 - acc: 0.7036 - val_loss: 0.6015 - val_acc: 0.6822\n",
            "Epoch 284/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5361 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5564 - acc: 0.6895\n",
            "Epoch 00284: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5351 - acc: 0.7158 - val_loss: 0.5734 - val_acc: 0.6879\n",
            "Epoch 285/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5360 - acc: 0.7146Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5517 - acc: 0.6914\n",
            "Epoch 00285: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5356 - acc: 0.7139 - val_loss: 0.5873 - val_acc: 0.6860\n",
            "Epoch 286/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5331 - acc: 0.7316Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5636 - acc: 0.7031\n",
            "Epoch 00286: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5285 - acc: 0.7347 - val_loss: 0.5911 - val_acc: 0.6991\n",
            "Epoch 287/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5506 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5455 - acc: 0.7070\n",
            "Epoch 00287: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5505 - acc: 0.7119 - val_loss: 0.5977 - val_acc: 0.7009\n",
            "Epoch 288/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5255 - acc: 0.7247Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5651 - acc: 0.6934\n",
            "Epoch 00288: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5229 - acc: 0.7255 - val_loss: 0.5835 - val_acc: 0.6879\n",
            "Epoch 289/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5459 - acc: 0.7063Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5454 - acc: 0.7109\n",
            "Epoch 00289: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5433 - acc: 0.7109 - val_loss: 0.5598 - val_acc: 0.7084\n",
            "Epoch 290/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5357 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5509 - acc: 0.7090\n",
            "Epoch 00290: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5356 - acc: 0.7119 - val_loss: 0.5553 - val_acc: 0.7084\n",
            "Epoch 291/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5259 - acc: 0.7292Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5567 - acc: 0.6992\n",
            "Epoch 00291: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5252 - acc: 0.7288 - val_loss: 0.5678 - val_acc: 0.6972\n",
            "Epoch 292/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5300 - acc: 0.7204Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5555 - acc: 0.6855\n",
            "Epoch 00292: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5314 - acc: 0.7173 - val_loss: 0.5612 - val_acc: 0.6841\n",
            "Epoch 293/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5521 - acc: 0.6973\n",
            "Epoch 00293: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5471 - acc: 0.7104 - val_loss: 0.5617 - val_acc: 0.6972\n",
            "Epoch 294/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5255 - acc: 0.7199Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5620 - acc: 0.6973\n",
            "Epoch 00294: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5242 - acc: 0.7198 - val_loss: 0.5653 - val_acc: 0.7009\n",
            "Epoch 295/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5411 - acc: 0.7172Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5609 - acc: 0.6914\n",
            "Epoch 00295: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5405 - acc: 0.7188 - val_loss: 0.5692 - val_acc: 0.6935\n",
            "Epoch 296/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5548 - acc: 0.7070\n",
            "Epoch 00296: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5384 - acc: 0.7032 - val_loss: 0.5637 - val_acc: 0.7065\n",
            "Epoch 297/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5509 - acc: 0.7286Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5541 - acc: 0.7109\n",
            "Epoch 00297: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5470 - acc: 0.7300 - val_loss: 0.5725 - val_acc: 0.7103\n",
            "Epoch 298/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5461 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5820 - acc: 0.7012\n",
            "Epoch 00298: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5447 - acc: 0.7099 - val_loss: 0.5930 - val_acc: 0.6972\n",
            "Epoch 299/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5336 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5739 - acc: 0.7031\n",
            "Epoch 00299: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5329 - acc: 0.7154 - val_loss: 0.5812 - val_acc: 0.6991\n",
            "Epoch 300/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5358 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5534 - acc: 0.7031\n",
            "Epoch 00300: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5369 - acc: 0.7124 - val_loss: 0.5672 - val_acc: 0.7047\n",
            "Epoch 301/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5455 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5618 - acc: 0.7188\n",
            "Epoch 00301: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5451 - acc: 0.7059 - val_loss: 0.5739 - val_acc: 0.7159\n",
            "Epoch 302/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5275 - acc: 0.7220Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5576 - acc: 0.7070\n",
            "Epoch 00302: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5300 - acc: 0.7233 - val_loss: 0.5691 - val_acc: 0.7065\n",
            "Epoch 303/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5218 - acc: 0.7220Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5472 - acc: 0.7051\n",
            "Epoch 00303: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5247 - acc: 0.7218 - val_loss: 0.5580 - val_acc: 0.7028\n",
            "Epoch 304/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5367 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5605 - acc: 0.6895\n",
            "Epoch 00304: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5362 - acc: 0.7109 - val_loss: 0.5753 - val_acc: 0.6860\n",
            "Epoch 305/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5403 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5595 - acc: 0.7148\n",
            "Epoch 00305: val_loss did not improve from 0.54400\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5389 - acc: 0.7094 - val_loss: 0.5660 - val_acc: 0.7140\n",
            "Epoch 306/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5348 - acc: 0.7199Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6797\n",
            "Epoch 00306: val_loss improved from 0.54400 to 0.53652, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5332 - acc: 0.7203 - val_loss: 0.5365 - val_acc: 0.6822\n",
            "Epoch 307/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5362 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5788 - acc: 0.6914\n",
            "Epoch 00307: val_loss did not improve from 0.53652\n",
            "16/16 [==============================] - 5s 315ms/step - loss: 0.5354 - acc: 0.7129 - val_loss: 0.5417 - val_acc: 0.6953\n",
            "Epoch 308/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5393 - acc: 0.7146Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5653 - acc: 0.6836\n",
            "Epoch 00308: val_loss improved from 0.53652 to 0.52546, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5373 - acc: 0.7149 - val_loss: 0.5255 - val_acc: 0.6897\n",
            "Epoch 309/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5349 - acc: 0.7241Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5708 - acc: 0.7012\n",
            "Epoch 00309: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5316 - acc: 0.7268 - val_loss: 0.5611 - val_acc: 0.6991\n",
            "Epoch 310/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5300 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5613 - acc: 0.7070\n",
            "Epoch 00310: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5316 - acc: 0.7228 - val_loss: 0.5511 - val_acc: 0.7065\n",
            "Epoch 311/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.7273Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5628 - acc: 0.6973\n",
            "Epoch 00311: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5440 - acc: 0.7253 - val_loss: 0.5517 - val_acc: 0.6991\n",
            "Epoch 312/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5286 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5779 - acc: 0.6855\n",
            "Epoch 00312: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5281 - acc: 0.7293 - val_loss: 0.6042 - val_acc: 0.6785\n",
            "Epoch 313/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5317 - acc: 0.7089Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5533 - acc: 0.7012\n",
            "Epoch 00313: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5373 - acc: 0.7031 - val_loss: 0.5428 - val_acc: 0.7065\n",
            "Epoch 314/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5387 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5568 - acc: 0.6895\n",
            "Epoch 00314: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5377 - acc: 0.7203 - val_loss: 0.5593 - val_acc: 0.6935\n",
            "Epoch 315/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5306 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5594 - acc: 0.6992\n",
            "Epoch 00315: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5357 - acc: 0.7089 - val_loss: 0.5600 - val_acc: 0.7047\n",
            "Epoch 316/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5243 - acc: 0.7459Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5486 - acc: 0.7109\n",
            "Epoch 00316: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5226 - acc: 0.7486 - val_loss: 0.5717 - val_acc: 0.7103\n",
            "Epoch 317/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.7276Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5638 - acc: 0.7031\n",
            "Epoch 00317: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5226 - acc: 0.7189 - val_loss: 0.5406 - val_acc: 0.7103\n",
            "Epoch 318/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5364 - acc: 0.7214Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5593 - acc: 0.6992\n",
            "Epoch 00318: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5366 - acc: 0.7231 - val_loss: 0.5717 - val_acc: 0.7028\n",
            "Epoch 319/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5336 - acc: 0.7321Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5815 - acc: 0.6738\n",
            "Epoch 00319: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5330 - acc: 0.7303 - val_loss: 0.5909 - val_acc: 0.6710\n",
            "Epoch 320/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7231Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5570 - acc: 0.6992\n",
            "Epoch 00320: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5279 - acc: 0.7238 - val_loss: 0.5565 - val_acc: 0.6991\n",
            "Epoch 321/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5424 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5610 - acc: 0.7266\n",
            "Epoch 00321: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5442 - acc: 0.7134 - val_loss: 0.5549 - val_acc: 0.7271\n",
            "Epoch 322/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5853 - acc: 0.6680\n",
            "Epoch 00322: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5304 - acc: 0.7198 - val_loss: 0.5807 - val_acc: 0.6654\n",
            "Epoch 323/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5209 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5893 - acc: 0.6816\n",
            "Epoch 00323: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5204 - acc: 0.7288 - val_loss: 0.5773 - val_acc: 0.6841\n",
            "Epoch 324/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5307 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5622 - acc: 0.7031\n",
            "Epoch 00324: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 5s 304ms/step - loss: 0.5363 - acc: 0.7094 - val_loss: 0.5639 - val_acc: 0.6991\n",
            "Epoch 325/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5310 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5583 - acc: 0.7051\n",
            "Epoch 00325: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5357 - acc: 0.7213 - val_loss: 0.5639 - val_acc: 0.7047\n",
            "Epoch 326/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5318 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5542 - acc: 0.6992\n",
            "Epoch 00326: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5283 - acc: 0.7233 - val_loss: 0.5698 - val_acc: 0.6972\n",
            "Epoch 327/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5316 - acc: 0.7162Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5562 - acc: 0.7012\n",
            "Epoch 00327: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5322 - acc: 0.7149 - val_loss: 0.5697 - val_acc: 0.7009\n",
            "Epoch 328/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5284 - acc: 0.7194Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5573 - acc: 0.6816\n",
            "Epoch 00328: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5266 - acc: 0.7223 - val_loss: 0.5791 - val_acc: 0.6766\n",
            "Epoch 329/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5267 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5801 - acc: 0.6699\n",
            "Epoch 00329: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5247 - acc: 0.7223 - val_loss: 0.6009 - val_acc: 0.6673\n",
            "Epoch 330/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5231 - acc: 0.7328Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5566 - acc: 0.7129\n",
            "Epoch 00330: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 0.5242 - acc: 0.7329 - val_loss: 0.5717 - val_acc: 0.7140\n",
            "Epoch 331/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5415 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5484 - acc: 0.7285\n",
            "Epoch 00331: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5428 - acc: 0.7213 - val_loss: 0.5628 - val_acc: 0.7290\n",
            "Epoch 332/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5290 - acc: 0.7273Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5552 - acc: 0.7012\n",
            "Epoch 00332: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5304 - acc: 0.7223 - val_loss: 0.5683 - val_acc: 0.6991\n",
            "Epoch 333/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5272 - acc: 0.7385Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5553 - acc: 0.7129\n",
            "Epoch 00333: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5302 - acc: 0.7357 - val_loss: 0.5607 - val_acc: 0.7103\n",
            "Epoch 334/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5317 - acc: 0.7195Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5602 - acc: 0.7031\n",
            "Epoch 00334: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5294 - acc: 0.7199 - val_loss: 0.5781 - val_acc: 0.7047\n",
            "Epoch 335/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5249 - acc: 0.7193Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5487 - acc: 0.7227\n",
            "Epoch 00335: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5235 - acc: 0.7227 - val_loss: 0.5518 - val_acc: 0.7252\n",
            "Epoch 336/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5419 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5546 - acc: 0.7012\n",
            "Epoch 00336: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5411 - acc: 0.7039 - val_loss: 0.5447 - val_acc: 0.7028\n",
            "Epoch 337/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5380 - acc: 0.7172Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5541 - acc: 0.7031\n",
            "Epoch 00337: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5347 - acc: 0.7198 - val_loss: 0.5455 - val_acc: 0.7047\n",
            "Epoch 338/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6738\n",
            "Epoch 00338: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5256 - acc: 0.7193 - val_loss: 0.5622 - val_acc: 0.6729\n",
            "Epoch 339/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5181 - acc: 0.7305Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5545 - acc: 0.6934\n",
            "Epoch 00339: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5202 - acc: 0.7293 - val_loss: 0.5436 - val_acc: 0.6972\n",
            "Epoch 340/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7220Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5544 - acc: 0.7090\n",
            "Epoch 00340: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5322 - acc: 0.7203 - val_loss: 0.5379 - val_acc: 0.7103\n",
            "Epoch 341/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5199 - acc: 0.7294Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5685 - acc: 0.6953\n",
            "Epoch 00341: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5233 - acc: 0.7283 - val_loss: 0.5468 - val_acc: 0.6972\n",
            "Epoch 342/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7268Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5550 - acc: 0.6836\n",
            "Epoch 00342: val_loss did not improve from 0.52546\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5309 - acc: 0.7273 - val_loss: 0.5448 - val_acc: 0.6879\n",
            "Epoch 343/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7198Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5510 - acc: 0.6914\n",
            "Epoch 00343: val_loss improved from 0.52546 to 0.52286, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5271 - acc: 0.7222 - val_loss: 0.5229 - val_acc: 0.6953\n",
            "Epoch 344/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5348 - acc: 0.7259Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5567 - acc: 0.7148\n",
            "Epoch 00344: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5332 - acc: 0.7250 - val_loss: 0.5421 - val_acc: 0.7140\n",
            "Epoch 345/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5330 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5640 - acc: 0.6953\n",
            "Epoch 00345: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5314 - acc: 0.7188 - val_loss: 0.5572 - val_acc: 0.6972\n",
            "Epoch 346/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7199Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5861 - acc: 0.6992\n",
            "Epoch 00346: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5364 - acc: 0.7193 - val_loss: 0.5745 - val_acc: 0.7009\n",
            "Epoch 347/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5322 - acc: 0.7211Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5642 - acc: 0.6875\n",
            "Epoch 00347: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5314 - acc: 0.7240 - val_loss: 0.5571 - val_acc: 0.6916\n",
            "Epoch 348/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5340 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6040 - acc: 0.6836\n",
            "Epoch 00348: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5290 - acc: 0.7293 - val_loss: 0.5894 - val_acc: 0.6841\n",
            "Epoch 349/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5296 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5587 - acc: 0.7051\n",
            "Epoch 00349: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5284 - acc: 0.7198 - val_loss: 0.5680 - val_acc: 0.7028\n",
            "Epoch 350/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5268 - acc: 0.7172Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5420 - acc: 0.7148\n",
            "Epoch 00350: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5296 - acc: 0.7183 - val_loss: 0.5505 - val_acc: 0.7140\n",
            "Epoch 351/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7273Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5538 - acc: 0.6973\n",
            "Epoch 00351: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5342 - acc: 0.7233 - val_loss: 0.5561 - val_acc: 0.6953\n",
            "Epoch 352/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5276 - acc: 0.7243Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5491 - acc: 0.7207\n",
            "Epoch 00352: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5292 - acc: 0.7214 - val_loss: 0.5639 - val_acc: 0.7178\n",
            "Epoch 353/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5314 - acc: 0.7247Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5791 - acc: 0.6992\n",
            "Epoch 00353: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5257 - acc: 0.7317 - val_loss: 0.6017 - val_acc: 0.6935\n",
            "Epoch 354/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5238 - acc: 0.7245Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5613 - acc: 0.7266\n",
            "Epoch 00354: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5248 - acc: 0.7261 - val_loss: 0.5690 - val_acc: 0.7234\n",
            "Epoch 355/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5538 - acc: 0.6992\n",
            "Epoch 00355: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5298 - acc: 0.7064 - val_loss: 0.5244 - val_acc: 0.7047\n",
            "Epoch 356/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5222 - acc: 0.7204Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5569 - acc: 0.6973\n",
            "Epoch 00356: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5221 - acc: 0.7178 - val_loss: 0.5317 - val_acc: 0.7047\n",
            "Epoch 357/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5182 - acc: 0.7284Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5655 - acc: 0.7012\n",
            "Epoch 00357: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5165 - acc: 0.7278 - val_loss: 0.5320 - val_acc: 0.7084\n",
            "Epoch 358/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5311 - acc: 0.7234Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5947 - acc: 0.6875\n",
            "Epoch 00358: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 5s 317ms/step - loss: 0.5264 - acc: 0.7271 - val_loss: 0.5627 - val_acc: 0.6935\n",
            "Epoch 359/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5299 - acc: 0.7241Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5577 - acc: 0.7090\n",
            "Epoch 00359: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5297 - acc: 0.7250 - val_loss: 0.5514 - val_acc: 0.7103\n",
            "Epoch 360/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5325 - acc: 0.7204Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5469 - acc: 0.7109\n",
            "Epoch 00360: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5340 - acc: 0.7188 - val_loss: 0.5439 - val_acc: 0.7103\n",
            "Epoch 361/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5243 - acc: 0.7203Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5743 - acc: 0.6953\n",
            "Epoch 00361: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5224 - acc: 0.7246 - val_loss: 0.5612 - val_acc: 0.6953\n",
            "Epoch 362/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7038Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5627 - acc: 0.7012\n",
            "Epoch 00362: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5395 - acc: 0.7058 - val_loss: 0.5460 - val_acc: 0.7047\n",
            "Epoch 363/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5191 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5542 - acc: 0.7012\n",
            "Epoch 00363: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5187 - acc: 0.7263 - val_loss: 0.5631 - val_acc: 0.7009\n",
            "Epoch 364/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5429 - acc: 0.7194Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5475 - acc: 0.7051\n",
            "Epoch 00364: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5380 - acc: 0.7213 - val_loss: 0.5482 - val_acc: 0.7047\n",
            "Epoch 365/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5226 - acc: 0.7333Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5554 - acc: 0.6934\n",
            "Epoch 00365: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5266 - acc: 0.7271 - val_loss: 0.5549 - val_acc: 0.6953\n",
            "Epoch 366/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5683 - acc: 0.6895\n",
            "Epoch 00366: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5266 - acc: 0.7233 - val_loss: 0.5565 - val_acc: 0.6879\n",
            "Epoch 367/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7389Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5513 - acc: 0.6973\n",
            "Epoch 00367: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5225 - acc: 0.7361 - val_loss: 0.5417 - val_acc: 0.6991\n",
            "Epoch 368/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5218 - acc: 0.7203Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5416 - acc: 0.7227\n",
            "Epoch 00368: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5211 - acc: 0.7217 - val_loss: 0.5399 - val_acc: 0.7196\n",
            "Epoch 369/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5219 - acc: 0.7194Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5605 - acc: 0.6973\n",
            "Epoch 00369: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5226 - acc: 0.7213 - val_loss: 0.5631 - val_acc: 0.6972\n",
            "Epoch 370/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5197 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5710 - acc: 0.6992\n",
            "Epoch 00370: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5211 - acc: 0.7263 - val_loss: 0.5757 - val_acc: 0.7009\n",
            "Epoch 371/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5260 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5654 - acc: 0.6855\n",
            "Epoch 00371: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5268 - acc: 0.7213 - val_loss: 0.5614 - val_acc: 0.6860\n",
            "Epoch 372/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5181 - acc: 0.7273Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5710 - acc: 0.7090\n",
            "Epoch 00372: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5178 - acc: 0.7263 - val_loss: 0.5718 - val_acc: 0.7065\n",
            "Epoch 373/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5185 - acc: 0.7279Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5623 - acc: 0.6895\n",
            "Epoch 00373: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5224 - acc: 0.7248 - val_loss: 0.5667 - val_acc: 0.6860\n",
            "Epoch 374/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5109 - acc: 0.7310Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.7109\n",
            "Epoch 00374: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5108 - acc: 0.7317 - val_loss: 0.5637 - val_acc: 0.7103\n",
            "Epoch 375/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5253 - acc: 0.7210Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5561 - acc: 0.7090\n",
            "Epoch 00375: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5262 - acc: 0.7188 - val_loss: 0.5624 - val_acc: 0.7065\n",
            "Epoch 376/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5210 - acc: 0.7220Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5490 - acc: 0.7168\n",
            "Epoch 00376: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5190 - acc: 0.7243 - val_loss: 0.5521 - val_acc: 0.7159\n",
            "Epoch 377/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7321Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5660 - acc: 0.7031\n",
            "Epoch 00377: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5280 - acc: 0.7308 - val_loss: 0.5617 - val_acc: 0.6991\n",
            "Epoch 378/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5695 - acc: 0.7090\n",
            "Epoch 00378: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5238 - acc: 0.7288 - val_loss: 0.5619 - val_acc: 0.7065\n",
            "Epoch 379/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5269 - acc: 0.7161Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5498 - acc: 0.7109\n",
            "Epoch 00379: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5279 - acc: 0.7168 - val_loss: 0.5364 - val_acc: 0.7084\n",
            "Epoch 380/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5246 - acc: 0.7208Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5597 - acc: 0.7031\n",
            "Epoch 00380: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5278 - acc: 0.7183 - val_loss: 0.5550 - val_acc: 0.7009\n",
            "Epoch 381/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5341 - acc: 0.7265Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5696 - acc: 0.6934\n",
            "Epoch 00381: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5317 - acc: 0.7290 - val_loss: 0.5729 - val_acc: 0.6897\n",
            "Epoch 382/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5219 - acc: 0.7300Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5514 - acc: 0.7168\n",
            "Epoch 00382: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5175 - acc: 0.7332 - val_loss: 0.5640 - val_acc: 0.7140\n",
            "Epoch 383/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5310 - acc: 0.7245Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5715 - acc: 0.7129\n",
            "Epoch 00383: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5282 - acc: 0.7275 - val_loss: 0.5671 - val_acc: 0.7103\n",
            "Epoch 384/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5310 - acc: 0.7310Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5458 - acc: 0.7109\n",
            "Epoch 00384: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5312 - acc: 0.7317 - val_loss: 0.5514 - val_acc: 0.7065\n",
            "Epoch 385/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5281 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5481 - acc: 0.7012\n",
            "Epoch 00385: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5281 - acc: 0.7193 - val_loss: 0.5610 - val_acc: 0.6972\n",
            "Epoch 386/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5115 - acc: 0.7389Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5852 - acc: 0.6953\n",
            "Epoch 00386: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5097 - acc: 0.7396 - val_loss: 0.5846 - val_acc: 0.6935\n",
            "Epoch 387/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5232 - acc: 0.7260Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5551 - acc: 0.6934\n",
            "Epoch 00387: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5225 - acc: 0.7241 - val_loss: 0.5538 - val_acc: 0.6897\n",
            "Epoch 388/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5124 - acc: 0.7332Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5717 - acc: 0.7051\n",
            "Epoch 00388: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5108 - acc: 0.7322 - val_loss: 0.5683 - val_acc: 0.7028\n",
            "Epoch 389/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5488 - acc: 0.7227\n",
            "Epoch 00389: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5283 - acc: 0.7273 - val_loss: 0.5442 - val_acc: 0.7234\n",
            "Epoch 390/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5266 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5492 - acc: 0.6875\n",
            "Epoch 00390: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5271 - acc: 0.7223 - val_loss: 0.5642 - val_acc: 0.6860\n",
            "Epoch 391/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5197 - acc: 0.7294Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5577 - acc: 0.7031\n",
            "Epoch 00391: val_loss did not improve from 0.52286\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5219 - acc: 0.7273 - val_loss: 0.5538 - val_acc: 0.7047\n",
            "Epoch 392/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5129 - acc: 0.7369Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5384 - acc: 0.6875\n",
            "Epoch 00392: val_loss improved from 0.52286 to 0.52223, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.5146 - acc: 0.7372 - val_loss: 0.5222 - val_acc: 0.6935\n",
            "Epoch 393/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5314 - acc: 0.7252Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5708 - acc: 0.6953\n",
            "Epoch 00393: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5282 - acc: 0.7268 - val_loss: 0.5427 - val_acc: 0.7028\n",
            "Epoch 394/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5166 - acc: 0.7363Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5974 - acc: 0.6758\n",
            "Epoch 00394: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5126 - acc: 0.7392 - val_loss: 0.5598 - val_acc: 0.6822\n",
            "Epoch 395/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5320 - acc: 0.7183Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5536 - acc: 0.6992\n",
            "Epoch 00395: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5312 - acc: 0.7188 - val_loss: 0.5309 - val_acc: 0.7065\n",
            "Epoch 396/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5223 - acc: 0.7198Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5629 - acc: 0.6914\n",
            "Epoch 00396: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5221 - acc: 0.7212 - val_loss: 0.5481 - val_acc: 0.6953\n",
            "Epoch 397/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5177 - acc: 0.7308Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5586 - acc: 0.6934\n",
            "Epoch 00397: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5183 - acc: 0.7300 - val_loss: 0.5413 - val_acc: 0.6991\n",
            "Epoch 398/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5118 - acc: 0.7401Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5501 - acc: 0.7012\n",
            "Epoch 00398: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5137 - acc: 0.7393 - val_loss: 0.5429 - val_acc: 0.7065\n",
            "Epoch 399/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7204Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5474 - acc: 0.6973\n",
            "Epoch 00399: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5323 - acc: 0.7183 - val_loss: 0.5344 - val_acc: 0.7028\n",
            "Epoch 400/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5154 - acc: 0.7406Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5847 - acc: 0.6699\n",
            "Epoch 00400: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5116 - acc: 0.7427 - val_loss: 0.5518 - val_acc: 0.6766\n",
            "Epoch 401/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5227 - acc: 0.7289Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5587 - acc: 0.6914\n",
            "Epoch 00401: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5338 - acc: 0.7223 - val_loss: 0.5520 - val_acc: 0.6972\n",
            "Epoch 402/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5075 - acc: 0.7454Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5558 - acc: 0.6895\n",
            "Epoch 00402: val_loss did not improve from 0.52223\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5060 - acc: 0.7442 - val_loss: 0.5411 - val_acc: 0.6991\n",
            "Epoch 403/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5229 - acc: 0.7391Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5339 - acc: 0.7109\n",
            "Epoch 00403: val_loss improved from 0.52223 to 0.51856, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5250 - acc: 0.7358 - val_loss: 0.5186 - val_acc: 0.7159\n",
            "Epoch 404/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5248 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5637 - acc: 0.7070\n",
            "Epoch 00404: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5253 - acc: 0.7079 - val_loss: 0.5773 - val_acc: 0.6972\n",
            "Epoch 405/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5126 - acc: 0.7390Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5565 - acc: 0.7090\n",
            "Epoch 00405: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5114 - acc: 0.7382 - val_loss: 0.5685 - val_acc: 0.7028\n",
            "Epoch 406/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5121 - acc: 0.7289Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5478 - acc: 0.7188\n",
            "Epoch 00406: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5168 - acc: 0.7258 - val_loss: 0.5645 - val_acc: 0.7084\n",
            "Epoch 407/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5125 - acc: 0.7337Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5592 - acc: 0.7051\n",
            "Epoch 00407: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5125 - acc: 0.7327 - val_loss: 0.5701 - val_acc: 0.6972\n",
            "Epoch 408/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5115 - acc: 0.7379Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5427 - acc: 0.7227\n",
            "Epoch 00408: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5159 - acc: 0.7357 - val_loss: 0.5737 - val_acc: 0.7140\n",
            "Epoch 409/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5129 - acc: 0.7300Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5372 - acc: 0.7168\n",
            "Epoch 00409: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5140 - acc: 0.7273 - val_loss: 0.5575 - val_acc: 0.7103\n",
            "Epoch 410/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7332Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5760 - acc: 0.7012\n",
            "Epoch 00410: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5225 - acc: 0.7357 - val_loss: 0.6319 - val_acc: 0.6972\n",
            "Epoch 411/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.7077Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5543 - acc: 0.7090\n",
            "Epoch 00411: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5335 - acc: 0.7134 - val_loss: 0.5835 - val_acc: 0.7084\n",
            "Epoch 412/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5099 - acc: 0.7297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5507 - acc: 0.7070\n",
            "Epoch 00412: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5095 - acc: 0.7324 - val_loss: 0.5794 - val_acc: 0.7028\n",
            "Epoch 413/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5259 - acc: 0.7337Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5609 - acc: 0.7051\n",
            "Epoch 00413: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5265 - acc: 0.7322 - val_loss: 0.5623 - val_acc: 0.7047\n",
            "Epoch 414/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5253 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5640 - acc: 0.6895\n",
            "Epoch 00414: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5248 - acc: 0.7233 - val_loss: 0.5602 - val_acc: 0.6879\n",
            "Epoch 415/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7438Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5854 - acc: 0.7129\n",
            "Epoch 00415: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5177 - acc: 0.7466 - val_loss: 0.5736 - val_acc: 0.7121\n",
            "Epoch 416/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5226 - acc: 0.7135Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5560 - acc: 0.7031\n",
            "Epoch 00416: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5204 - acc: 0.7149 - val_loss: 0.5671 - val_acc: 0.6991\n",
            "Epoch 417/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5204 - acc: 0.7347Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5451 - acc: 0.7129\n",
            "Epoch 00417: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5142 - acc: 0.7402 - val_loss: 0.5430 - val_acc: 0.7121\n",
            "Epoch 418/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5242 - acc: 0.7300Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5662 - acc: 0.7051\n",
            "Epoch 00418: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5184 - acc: 0.7341 - val_loss: 0.5791 - val_acc: 0.7009\n",
            "Epoch 419/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5258 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5420 - acc: 0.7051\n",
            "Epoch 00419: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5250 - acc: 0.7228 - val_loss: 0.5435 - val_acc: 0.7028\n",
            "Epoch 420/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5245 - acc: 0.7161Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5635 - acc: 0.7090\n",
            "Epoch 00420: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5204 - acc: 0.7231 - val_loss: 0.5680 - val_acc: 0.7065\n",
            "Epoch 421/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5110 - acc: 0.7363Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5564 - acc: 0.7012\n",
            "Epoch 00421: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5148 - acc: 0.7317 - val_loss: 0.5593 - val_acc: 0.6953\n",
            "Epoch 422/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5144 - acc: 0.7310Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5550 - acc: 0.7031\n",
            "Epoch 00422: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5157 - acc: 0.7327 - val_loss: 0.5535 - val_acc: 0.6991\n",
            "Epoch 423/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5218 - acc: 0.7300Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5398 - acc: 0.6973\n",
            "Epoch 00423: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5260 - acc: 0.7278 - val_loss: 0.5481 - val_acc: 0.6935\n",
            "Epoch 424/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5126 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5469 - acc: 0.7168\n",
            "Epoch 00424: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5157 - acc: 0.7248 - val_loss: 0.5499 - val_acc: 0.7159\n",
            "Epoch 425/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5121 - acc: 0.7305Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5578 - acc: 0.6895\n",
            "Epoch 00425: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5135 - acc: 0.7278 - val_loss: 0.5567 - val_acc: 0.6879\n",
            "Epoch 426/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5174 - acc: 0.7416Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5698 - acc: 0.7207\n",
            "Epoch 00426: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5178 - acc: 0.7402 - val_loss: 0.5673 - val_acc: 0.7196\n",
            "Epoch 427/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5215 - acc: 0.7229Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5715 - acc: 0.6895\n",
            "Epoch 00427: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5248 - acc: 0.7231 - val_loss: 0.5657 - val_acc: 0.6879\n",
            "Epoch 428/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5161 - acc: 0.7341Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5381 - acc: 0.7070\n",
            "Epoch 00428: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5131 - acc: 0.7361 - val_loss: 0.5416 - val_acc: 0.7047\n",
            "Epoch 429/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5135 - acc: 0.7252Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5510 - acc: 0.7109\n",
            "Epoch 00429: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5137 - acc: 0.7228 - val_loss: 0.5440 - val_acc: 0.7084\n",
            "Epoch 430/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5111 - acc: 0.7359Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5561 - acc: 0.7051\n",
            "Epoch 00430: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5116 - acc: 0.7363 - val_loss: 0.5461 - val_acc: 0.7009\n",
            "Epoch 431/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5208 - acc: 0.7303Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5617 - acc: 0.7148\n",
            "Epoch 00431: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5189 - acc: 0.7326 - val_loss: 0.5544 - val_acc: 0.7084\n",
            "Epoch 432/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5153 - acc: 0.7459Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5640 - acc: 0.6992\n",
            "Epoch 00432: val_loss did not improve from 0.51856\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5146 - acc: 0.7432 - val_loss: 0.5379 - val_acc: 0.7028\n",
            "Epoch 433/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5167 - acc: 0.7379Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5494 - acc: 0.7090\n",
            "Epoch 00433: val_loss improved from 0.51856 to 0.51232, saving model to model_2_benmal_best.h5\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5175 - acc: 0.7382 - val_loss: 0.5123 - val_acc: 0.7159\n",
            "Epoch 434/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5184 - acc: 0.7328Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5576 - acc: 0.6934\n",
            "Epoch 00434: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5211 - acc: 0.7310 - val_loss: 0.5282 - val_acc: 0.7009\n",
            "Epoch 435/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5172 - acc: 0.7411Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5560 - acc: 0.6992\n",
            "Epoch 00435: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5208 - acc: 0.7377 - val_loss: 0.5222 - val_acc: 0.7065\n",
            "Epoch 436/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5003 - acc: 0.7497Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5494 - acc: 0.7109\n",
            "Epoch 00436: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5029 - acc: 0.7477 - val_loss: 0.5157 - val_acc: 0.7178\n",
            "Epoch 437/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5211 - acc: 0.7370Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5569 - acc: 0.6777\n",
            "Epoch 00437: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5230 - acc: 0.7357 - val_loss: 0.5270 - val_acc: 0.6860\n",
            "Epoch 438/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5198 - acc: 0.7344Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5596 - acc: 0.6836\n",
            "Epoch 00438: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5203 - acc: 0.7354 - val_loss: 0.5274 - val_acc: 0.6897\n",
            "Epoch 439/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5146 - acc: 0.7270Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5611 - acc: 0.6973\n",
            "Epoch 00439: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5146 - acc: 0.7290 - val_loss: 0.5353 - val_acc: 0.7028\n",
            "Epoch 440/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5148 - acc: 0.7370Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5641 - acc: 0.6953\n",
            "Epoch 00440: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5107 - acc: 0.7383 - val_loss: 0.5385 - val_acc: 0.7047\n",
            "Epoch 441/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5240 - acc: 0.7231Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5510 - acc: 0.7012\n",
            "Epoch 00441: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5203 - acc: 0.7263 - val_loss: 0.5271 - val_acc: 0.7047\n",
            "Epoch 442/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5148 - acc: 0.7332Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5453 - acc: 0.6973\n",
            "Epoch 00442: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5153 - acc: 0.7322 - val_loss: 0.5145 - val_acc: 0.7047\n",
            "Epoch 443/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5133 - acc: 0.7385Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5655 - acc: 0.7129\n",
            "Epoch 00443: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5198 - acc: 0.7357 - val_loss: 0.5394 - val_acc: 0.7178\n",
            "Epoch 444/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5085 - acc: 0.7406Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5582 - acc: 0.6758\n",
            "Epoch 00444: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5092 - acc: 0.7397 - val_loss: 0.5275 - val_acc: 0.6822\n",
            "Epoch 445/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5078 - acc: 0.7396Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5629 - acc: 0.7090\n",
            "Epoch 00445: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5068 - acc: 0.7393 - val_loss: 0.5248 - val_acc: 0.7140\n",
            "Epoch 446/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7342Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5506 - acc: 0.7070\n",
            "Epoch 00446: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5186 - acc: 0.7337 - val_loss: 0.5172 - val_acc: 0.7121\n",
            "Epoch 447/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5117 - acc: 0.7303Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5700 - acc: 0.6973\n",
            "Epoch 00447: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5150 - acc: 0.7305 - val_loss: 0.5489 - val_acc: 0.7028\n",
            "Epoch 448/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5090 - acc: 0.7427Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5441 - acc: 0.7012\n",
            "Epoch 00448: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5092 - acc: 0.7417 - val_loss: 0.5446 - val_acc: 0.7009\n",
            "Epoch 449/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.7378Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5755 - acc: 0.6953\n",
            "Epoch 00449: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5201 - acc: 0.7336 - val_loss: 0.5760 - val_acc: 0.6935\n",
            "Epoch 450/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5203 - acc: 0.7276Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5457 - acc: 0.7129\n",
            "Epoch 00450: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5185 - acc: 0.7305 - val_loss: 0.5536 - val_acc: 0.7140\n",
            "Epoch 451/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5046 - acc: 0.7363Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5993 - acc: 0.6973\n",
            "Epoch 00451: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5097 - acc: 0.7303 - val_loss: 0.5954 - val_acc: 0.6972\n",
            "Epoch 452/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4931 - acc: 0.7411Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5518 - acc: 0.7070\n",
            "Epoch 00452: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.4928 - acc: 0.7412 - val_loss: 0.5518 - val_acc: 0.7084\n",
            "Epoch 453/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5212 - acc: 0.7341Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5523 - acc: 0.7012\n",
            "Epoch 00453: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5191 - acc: 0.7356 - val_loss: 0.5541 - val_acc: 0.7047\n",
            "Epoch 454/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5217 - acc: 0.7240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6914\n",
            "Epoch 00454: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5225 - acc: 0.7251 - val_loss: 0.5697 - val_acc: 0.6935\n",
            "Epoch 455/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5157 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5593 - acc: 0.7129\n",
            "Epoch 00455: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5162 - acc: 0.7208 - val_loss: 0.5566 - val_acc: 0.7121\n",
            "Epoch 456/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4981 - acc: 0.7528Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5497 - acc: 0.7129\n",
            "Epoch 00456: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.4985 - acc: 0.7536 - val_loss: 0.5482 - val_acc: 0.7103\n",
            "Epoch 457/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5218 - acc: 0.7252Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5888 - acc: 0.6895\n",
            "Epoch 00457: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5211 - acc: 0.7273 - val_loss: 0.6016 - val_acc: 0.6860\n",
            "Epoch 458/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5202 - acc: 0.7279Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5659 - acc: 0.7051\n",
            "Epoch 00458: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5182 - acc: 0.7322 - val_loss: 0.5982 - val_acc: 0.7047\n",
            "Epoch 459/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5132 - acc: 0.7374Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5420 - acc: 0.6914\n",
            "Epoch 00459: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5101 - acc: 0.7397 - val_loss: 0.5489 - val_acc: 0.6953\n",
            "Epoch 460/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5106 - acc: 0.7391Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5526 - acc: 0.7070\n",
            "Epoch 00460: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5136 - acc: 0.7367 - val_loss: 0.5578 - val_acc: 0.7084\n",
            "Epoch 461/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5176 - acc: 0.7266Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5543 - acc: 0.6992\n",
            "Epoch 00461: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5195 - acc: 0.7248 - val_loss: 0.5780 - val_acc: 0.6953\n",
            "Epoch 462/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5068 - acc: 0.7284Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5521 - acc: 0.7148\n",
            "Epoch 00462: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5049 - acc: 0.7298 - val_loss: 0.5746 - val_acc: 0.7084\n",
            "Epoch 463/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5134 - acc: 0.7401Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5474 - acc: 0.7168\n",
            "Epoch 00463: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5127 - acc: 0.7368 - val_loss: 0.5578 - val_acc: 0.7159\n",
            "Epoch 464/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5067 - acc: 0.7411Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6973\n",
            "Epoch 00464: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5079 - acc: 0.7376 - val_loss: 0.5670 - val_acc: 0.6953\n",
            "Epoch 465/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5077 - acc: 0.7422Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5690 - acc: 0.6992\n",
            "Epoch 00465: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5072 - acc: 0.7442 - val_loss: 0.6111 - val_acc: 0.6972\n",
            "Epoch 466/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5180 - acc: 0.7323Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5525 - acc: 0.7188\n",
            "Epoch 00466: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5142 - acc: 0.7363 - val_loss: 0.5387 - val_acc: 0.7159\n",
            "Epoch 467/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5006 - acc: 0.7475Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5372 - acc: 0.7168\n",
            "Epoch 00467: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5066 - acc: 0.7397 - val_loss: 0.5798 - val_acc: 0.7196\n",
            "Epoch 468/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5132 - acc: 0.7395Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5672 - acc: 0.7051\n",
            "Epoch 00468: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5160 - acc: 0.7371 - val_loss: 0.6151 - val_acc: 0.7009\n",
            "Epoch 469/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5152 - acc: 0.7294Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5389 - acc: 0.7129\n",
            "Epoch 00469: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5168 - acc: 0.7268 - val_loss: 0.5336 - val_acc: 0.7159\n",
            "Epoch 470/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5102 - acc: 0.7354Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5556 - acc: 0.7090\n",
            "Epoch 00470: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5077 - acc: 0.7417 - val_loss: 0.5691 - val_acc: 0.7103\n",
            "Epoch 471/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5143 - acc: 0.7358Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5521 - acc: 0.6992\n",
            "Epoch 00471: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5133 - acc: 0.7356 - val_loss: 0.5676 - val_acc: 0.6991\n",
            "Epoch 472/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5243 - acc: 0.7240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5462 - acc: 0.7070\n",
            "Epoch 00472: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5212 - acc: 0.7251 - val_loss: 0.5604 - val_acc: 0.7084\n",
            "Epoch 473/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5129 - acc: 0.7448Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5592 - acc: 0.7148\n",
            "Epoch 00473: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5128 - acc: 0.7442 - val_loss: 0.5815 - val_acc: 0.7084\n",
            "Epoch 474/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5072 - acc: 0.7453Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5564 - acc: 0.7090\n",
            "Epoch 00474: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5098 - acc: 0.7432 - val_loss: 0.5788 - val_acc: 0.7009\n",
            "Epoch 475/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5016 - acc: 0.7438Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5455 - acc: 0.7207\n",
            "Epoch 00475: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5051 - acc: 0.7397 - val_loss: 0.5556 - val_acc: 0.7121\n",
            "Epoch 476/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5111 - acc: 0.7406Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5583 - acc: 0.6953\n",
            "Epoch 00476: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5095 - acc: 0.7427 - val_loss: 0.5786 - val_acc: 0.6879\n",
            "Epoch 477/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5159 - acc: 0.7241Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5679 - acc: 0.7148\n",
            "Epoch 00477: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5164 - acc: 0.7233 - val_loss: 0.6039 - val_acc: 0.7047\n",
            "Epoch 478/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5131 - acc: 0.7432Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5624 - acc: 0.6934\n",
            "Epoch 00478: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5104 - acc: 0.7437 - val_loss: 0.5400 - val_acc: 0.6953\n",
            "Epoch 479/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5170 - acc: 0.7401Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5451 - acc: 0.7051\n",
            "Epoch 00479: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5151 - acc: 0.7392 - val_loss: 0.5577 - val_acc: 0.7065\n",
            "Epoch 480/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4940 - acc: 0.7384Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5440 - acc: 0.7227\n",
            "Epoch 00480: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5024 - acc: 0.7361 - val_loss: 0.5475 - val_acc: 0.7234\n",
            "Epoch 481/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5192 - acc: 0.7229Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5330 - acc: 0.7090\n",
            "Epoch 00481: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5184 - acc: 0.7261 - val_loss: 0.5381 - val_acc: 0.7103\n",
            "Epoch 482/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5114 - acc: 0.7257Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5714 - acc: 0.7207\n",
            "Epoch 00482: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5100 - acc: 0.7255 - val_loss: 0.5716 - val_acc: 0.7178\n",
            "Epoch 483/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5135 - acc: 0.7297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5595 - acc: 0.7031\n",
            "Epoch 00483: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5102 - acc: 0.7334 - val_loss: 0.6196 - val_acc: 0.7028\n",
            "Epoch 484/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5121 - acc: 0.7363Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5558 - acc: 0.6953\n",
            "Epoch 00484: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5117 - acc: 0.7347 - val_loss: 0.5576 - val_acc: 0.6972\n",
            "Epoch 485/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5090 - acc: 0.7335Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5376 - acc: 0.7188\n",
            "Epoch 00485: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5106 - acc: 0.7305 - val_loss: 0.5606 - val_acc: 0.7159\n",
            "Epoch 486/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5136 - acc: 0.7326Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5679 - acc: 0.6895\n",
            "Epoch 00486: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5090 - acc: 0.7382 - val_loss: 0.5675 - val_acc: 0.6879\n",
            "Epoch 487/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5088 - acc: 0.7240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5704 - acc: 0.6875\n",
            "Epoch 00487: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5093 - acc: 0.7241 - val_loss: 0.5782 - val_acc: 0.6860\n",
            "Epoch 488/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4997 - acc: 0.7485Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5521 - acc: 0.7207\n",
            "Epoch 00488: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5047 - acc: 0.7447 - val_loss: 0.5615 - val_acc: 0.7215\n",
            "Epoch 489/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5050 - acc: 0.7326Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5470 - acc: 0.7051\n",
            "Epoch 00489: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5088 - acc: 0.7332 - val_loss: 0.5549 - val_acc: 0.7065\n",
            "Epoch 490/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5006 - acc: 0.7422Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5589 - acc: 0.6973\n",
            "Epoch 00490: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5025 - acc: 0.7427 - val_loss: 0.5391 - val_acc: 0.7028\n",
            "Epoch 491/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5067 - acc: 0.7379Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5508 - acc: 0.7070\n",
            "Epoch 00491: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5112 - acc: 0.7391 - val_loss: 0.5390 - val_acc: 0.7121\n",
            "Epoch 492/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5086 - acc: 0.7349Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5501 - acc: 0.6953\n",
            "Epoch 00492: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5059 - acc: 0.7363 - val_loss: 0.5419 - val_acc: 0.6972\n",
            "Epoch 493/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5250 - acc: 0.7310Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5317 - acc: 0.7148\n",
            "Epoch 00493: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 350ms/step - loss: 0.5216 - acc: 0.7362 - val_loss: 0.5226 - val_acc: 0.7178\n",
            "Epoch 494/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5239 - acc: 0.7347Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5381 - acc: 0.7051\n",
            "Epoch 00494: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5191 - acc: 0.7362 - val_loss: 0.5354 - val_acc: 0.7103\n",
            "Epoch 495/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5163 - acc: 0.7260Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5431 - acc: 0.7109\n",
            "Epoch 00495: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5154 - acc: 0.7285 - val_loss: 0.5215 - val_acc: 0.7178\n",
            "Epoch 496/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5060 - acc: 0.7411Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5578 - acc: 0.7168\n",
            "Epoch 00496: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5054 - acc: 0.7412 - val_loss: 0.6095 - val_acc: 0.7084\n",
            "Epoch 497/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5168 - acc: 0.7316Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5789 - acc: 0.7051\n",
            "Epoch 00497: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5135 - acc: 0.7332 - val_loss: 0.6226 - val_acc: 0.7009\n",
            "Epoch 498/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5109 - acc: 0.7276Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5280 - acc: 0.7168\n",
            "Epoch 00498: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5083 - acc: 0.7295 - val_loss: 0.5648 - val_acc: 0.7159\n",
            "Epoch 499/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5060 - acc: 0.7368Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5354 - acc: 0.7227\n",
            "Epoch 00499: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5058 - acc: 0.7381 - val_loss: 0.5845 - val_acc: 0.7178\n",
            "Epoch 500/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5141 - acc: 0.7286Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5433 - acc: 0.7207\n",
            "Epoch 00500: val_loss did not improve from 0.51232\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5125 - acc: 0.7310 - val_loss: 0.5931 - val_acc: 0.7159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOlZWb_R6bwU",
        "colab_type": "code",
        "outputId": "01e0cf8c-1b27-4f50-faa1-64d5c2ac6cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# History of accuracy and loss\n",
        "tra_loss_2 = history_2.history['loss']\n",
        "tra_acc_2 = history_2.history['acc']\n",
        "val_loss_2 = history_2.history['val_loss']\n",
        "val_acc_2 = history_2.history['val_acc']\n",
        "\n",
        "# Total number of epochs training\n",
        "epochs_2 = range(1, len(tra_acc_2)+1)\n",
        "end_epoch_2 = len(tra_acc_2)\n",
        "\n",
        "# Epoch when reached the validation loss minimum\n",
        "opt_epoch_2 = val_loss_2.index(min(val_loss_2)) + 1\n",
        "\n",
        "# Loss and accuracy on the validation set\n",
        "end_val_loss_2 = val_loss_2[-1]\n",
        "end_val_acc_2 = val_acc_2[-1]\n",
        "opt_val_loss_2 = val_loss_2[opt_epoch_2-1]\n",
        "opt_val_acc_2 = val_acc_2[opt_epoch_2-1]\n",
        "\n",
        "# Loss and accuracy on the test set\n",
        "opt_model_2 = models.load_model('model_2_benmal_best.h5')\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_test_loss_2, opt_test_acc_2 = opt_model_2.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_pred_2 = opt_model_2.predict([test_images, test_labels])\n",
        "pred_classes_2 = np.rint(opt_pred_2)\n",
        "\n",
        "print(\"Model 2\\n\")\n",
        "\n",
        "print(\"Epoch [end]: %d\" % end_epoch_2)\n",
        "print(\"Epoch [opt]: %d\" % opt_epoch_2)\n",
        "print(\"Valid accuracy [end]: %.4f\" % end_val_acc_2)\n",
        "print(\"Valid accuracy [opt]: %.4f\" % opt_val_acc_2)\n",
        "print(\"Test accuracy [end]:  %.4f\" % test_acc_2)\n",
        "print(\"Test accuracy [opt]:  %.4f\" % opt_test_acc_2)\n",
        "print(\"Valid loss [end]: %.4f\" % end_val_loss_2)\n",
        "print(\"Valid loss [opt]: %.4f\" % opt_val_loss_2)\n",
        "print(\"Test loss [end]:  %.4f\" % test_loss_2)\n",
        "print(\"Test loss [opt]:  %.4f\" % opt_test_loss_2)\n",
        "\n",
        "print(classification_report(test_labels, pred_classes_2, digits=4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 2\n",
            "\n",
            "Epoch [end]: 500\n",
            "Epoch [opt]: 433\n",
            "Valid accuracy [end]: 0.7159\n",
            "Valid accuracy [opt]: 0.7159\n",
            "Test accuracy [end]:  0.6577\n",
            "Test accuracy [opt]:  0.6399\n",
            "Valid loss [end]: 0.5931\n",
            "Valid loss [opt]: 0.5123\n",
            "Test loss [end]:  0.6354\n",
            "Test loss [opt]:  0.6562\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7692    0.6393    0.6983       219\n",
            "           1     0.4870    0.6410    0.5535       117\n",
            "\n",
            "    accuracy                         0.6399       336\n",
            "   macro avg     0.6281    0.6401    0.6259       336\n",
            "weighted avg     0.6710    0.6399    0.6479       336\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYM_gGtE_wZ",
        "colab_type": "code",
        "outputId": "c41e6dfc-1171-464d-a468-5d8e98f297dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "# Model accuracy\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 2 accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(epochs_2, tra_acc_2, 'r', label='Training set')\n",
        "plt.plot(epochs_2, val_acc_2, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_2, val_acc_2[opt_epoch_2-1], 'go')\n",
        "plt.vlines(opt_epoch_2, min(val_acc_2), opt_val_acc_2, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_acc_2, 1, opt_epoch_2, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Model loss\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 2 loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0.48,0.9)\n",
        "plt.plot(epochs_2, tra_loss_2, 'r', label='Training set')\n",
        "plt.plot(epochs_2, val_loss_2, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_2, val_loss_2[opt_epoch_2-1], 'go')\n",
        "plt.vlines(opt_epoch_2, min(val_loss_2), opt_val_loss_2, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_loss_2, 1, opt_epoch_2, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAHnCAYAAABUqE8xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAHnCAYAAABHfw/FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVTBqdDAbrYUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDpg--UCSo6d",
        "colab_type": "text"
      },
      "source": [
        "## Run 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6ArpdseSsUT",
        "colab_type": "code",
        "outputId": "3390f6be-1fec-4f71-a562-6a55214ddc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Model 3\n",
        "\n",
        "model_3 = models.Sequential()\n",
        "model_3.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model_3.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model_3.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model_3.add(layers.MaxPooling2D((2, 2)))\n",
        "model_3.add(layers.Flatten())\n",
        "model_3.add(layers.Dense(64, activation='relu'))\n",
        "model_3.add(layers.Dropout(0.5))\n",
        "model_3.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_3.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 148, 148, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 72, 72, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 36, 36, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 34, 34, 256)       295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 73984)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                4735040   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,104,769\n",
            "Trainable params: 5,104,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHrBGOp0Sy11",
        "colab_type": "code",
        "outputId": "f700629e-126b-425c-a49e-49708a63b575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Early stopping (stop training after the validation loss reaches the minimum)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=80, verbose=1)\n",
        "\n",
        "# Callback for checkpointing\n",
        "checkpoint = ModelCheckpoint('model_3_benmal_best.h5', \n",
        "        monitor='val_loss', mode='min', verbose=1, \n",
        "        save_best_only=True, save_freq='epoch'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model_3.compile(optimizer=RMSprop(learning_rate=0.001, decay=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history_3 = model_3.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=int(0.8*n_train_img) // 128,\n",
        "        epochs=500,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint],\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        initial_epoch=0\n",
        ")\n",
        "\n",
        "# Save\n",
        "models.save_model(model_3, 'model_3_benmal_end.h5')\n",
        "!cp model* \"/content/gdrive/My Drive/models/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 1.1637 - acc: 0.5523Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.7022 - acc: 0.5703\n",
            "Epoch 00001: val_loss improved from inf to 0.71209, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 7s 459ms/step - loss: 1.1331 - acc: 0.5559 - val_loss: 0.7121 - val_acc: 0.5664\n",
            "Epoch 2/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.7080 - acc: 0.5734Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6877 - acc: 0.5703\n",
            "Epoch 00002: val_loss improved from 0.71209 to 0.68978, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.7068 - acc: 0.5733 - val_loss: 0.6898 - val_acc: 0.5664\n",
            "Epoch 3/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6948 - acc: 0.5841Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6861 - acc: 0.5703\n",
            "Epoch 00003: val_loss did not improve from 0.68978\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6939 - acc: 0.5857 - val_loss: 0.6899 - val_acc: 0.5664\n",
            "Epoch 4/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6838 - acc: 0.5809Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6770 - acc: 0.5703\n",
            "Epoch 00004: val_loss improved from 0.68978 to 0.68644, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 400ms/step - loss: 0.6834 - acc: 0.5822 - val_loss: 0.6864 - val_acc: 0.5664\n",
            "Epoch 5/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6840 - acc: 0.5786Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6759 - acc: 0.5703\n",
            "Epoch 00005: val_loss improved from 0.68644 to 0.68134, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 402ms/step - loss: 0.6843 - acc: 0.5767 - val_loss: 0.6813 - val_acc: 0.5664\n",
            "Epoch 6/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6734 - acc: 0.5876Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6703 - acc: 0.5703\n",
            "Epoch 00006: val_loss improved from 0.68134 to 0.68030, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.6726 - acc: 0.5905 - val_loss: 0.6803 - val_acc: 0.5664\n",
            "Epoch 7/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6769 - acc: 0.5899Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6668 - acc: 0.5918\n",
            "Epoch 00007: val_loss improved from 0.68030 to 0.67297, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.6766 - acc: 0.5902 - val_loss: 0.6730 - val_acc: 0.5888\n",
            "Epoch 8/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6738 - acc: 0.5772Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6591 - acc: 0.6426\n",
            "Epoch 00008: val_loss improved from 0.67297 to 0.67145, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6714 - acc: 0.5782 - val_loss: 0.6715 - val_acc: 0.6374\n",
            "Epoch 9/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6773 - acc: 0.5825Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6722 - acc: 0.6523\n",
            "Epoch 00009: val_loss did not improve from 0.67145\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6780 - acc: 0.5782 - val_loss: 0.6777 - val_acc: 0.6505\n",
            "Epoch 10/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6763 - acc: 0.6000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6678 - acc: 0.5742\n",
            "Epoch 00010: val_loss did not improve from 0.67145\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6755 - acc: 0.5986 - val_loss: 0.6759 - val_acc: 0.5701\n",
            "Epoch 11/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6675 - acc: 0.5881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.7010 - acc: 0.4316\n",
            "Epoch 00011: val_loss did not improve from 0.67145\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6730 - acc: 0.5859 - val_loss: 0.6980 - val_acc: 0.4355\n",
            "Epoch 12/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6708 - acc: 0.5948Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6572 - acc: 0.5996\n",
            "Epoch 00012: val_loss improved from 0.67145 to 0.66925, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.6710 - acc: 0.5977 - val_loss: 0.6693 - val_acc: 0.5963\n",
            "Epoch 13/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6744 - acc: 0.6157Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6487 - acc: 0.6016\n",
            "Epoch 00013: val_loss improved from 0.66925 to 0.66287, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6731 - acc: 0.6153 - val_loss: 0.6629 - val_acc: 0.5981\n",
            "Epoch 14/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6539 - acc: 0.6161Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6465 - acc: 0.6016\n",
            "Epoch 00014: val_loss did not improve from 0.66287\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6518 - acc: 0.6200 - val_loss: 0.6632 - val_acc: 0.5963\n",
            "Epoch 15/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6501 - acc: 0.6198Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6577 - acc: 0.5781\n",
            "Epoch 00015: val_loss did not improve from 0.66287\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6488 - acc: 0.6216 - val_loss: 0.6767 - val_acc: 0.5738\n",
            "Epoch 16/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6643 - acc: 0.6101Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6545 - acc: 0.5898\n",
            "Epoch 00016: val_loss did not improve from 0.66287\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.6624 - acc: 0.6115 - val_loss: 0.6663 - val_acc: 0.5869\n",
            "Epoch 17/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6491 - acc: 0.6244Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6333 - acc: 0.6387\n",
            "Epoch 00017: val_loss improved from 0.66287 to 0.64784, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.6467 - acc: 0.6304 - val_loss: 0.6478 - val_acc: 0.6318\n",
            "Epoch 18/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6462 - acc: 0.6255Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6344 - acc: 0.6484\n",
            "Epoch 00018: val_loss did not improve from 0.64784\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.6458 - acc: 0.6244 - val_loss: 0.6581 - val_acc: 0.6486\n",
            "Epoch 19/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6573 - acc: 0.6164Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6406 - acc: 0.6523\n",
            "Epoch 00019: val_loss did not improve from 0.64784\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6599 - acc: 0.6145 - val_loss: 0.6532 - val_acc: 0.6486\n",
            "Epoch 20/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6396 - acc: 0.6245Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6187 - acc: 0.6426\n",
            "Epoch 00020: val_loss improved from 0.64784 to 0.63549, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6394 - acc: 0.6230 - val_loss: 0.6355 - val_acc: 0.6393\n",
            "Epoch 21/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6461 - acc: 0.6297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6321 - acc: 0.6387\n",
            "Epoch 00021: val_loss did not improve from 0.63549\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.6464 - acc: 0.6304 - val_loss: 0.6489 - val_acc: 0.6374\n",
            "Epoch 22/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6430 - acc: 0.6266Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6338 - acc: 0.6309\n",
            "Epoch 00022: val_loss did not improve from 0.63549\n",
            "16/16 [==============================] - 6s 392ms/step - loss: 0.6430 - acc: 0.6260 - val_loss: 0.6540 - val_acc: 0.6280\n",
            "Epoch 23/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6318 - acc: 0.6476Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6768 - acc: 0.5801\n",
            "Epoch 00023: val_loss did not improve from 0.63549\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.6309 - acc: 0.6476 - val_loss: 0.7087 - val_acc: 0.5757\n",
            "Epoch 24/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6356 - acc: 0.6292Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6193 - acc: 0.6504\n",
            "Epoch 00024: val_loss improved from 0.63549 to 0.62265, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 0.6359 - acc: 0.6328 - val_loss: 0.6226 - val_acc: 0.6505\n",
            "Epoch 25/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6314 - acc: 0.6195Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6244 - acc: 0.6426\n",
            "Epoch 00025: val_loss did not improve from 0.62265\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6307 - acc: 0.6203 - val_loss: 0.6228 - val_acc: 0.6393\n",
            "Epoch 26/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6306 - acc: 0.6260Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6351 - acc: 0.6523\n",
            "Epoch 00026: val_loss did not improve from 0.62265\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6413 - acc: 0.6270 - val_loss: 0.6345 - val_acc: 0.6542\n",
            "Epoch 27/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6295 - acc: 0.6459Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6125 - acc: 0.6543\n",
            "Epoch 00027: val_loss improved from 0.62265 to 0.62036, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6337 - acc: 0.6436 - val_loss: 0.6204 - val_acc: 0.6542\n",
            "Epoch 28/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6135 - acc: 0.6448Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6290 - acc: 0.6406\n",
            "Epoch 00028: val_loss did not improve from 0.62036\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6181 - acc: 0.6416 - val_loss: 0.6258 - val_acc: 0.6411\n",
            "Epoch 29/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6471 - acc: 0.6292Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6121 - acc: 0.6387\n",
            "Epoch 00029: val_loss improved from 0.62036 to 0.61956, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6462 - acc: 0.6269 - val_loss: 0.6196 - val_acc: 0.6355\n",
            "Epoch 30/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6207 - acc: 0.6324Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6104 - acc: 0.6445\n",
            "Epoch 00030: val_loss improved from 0.61956 to 0.61489, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.6194 - acc: 0.6339 - val_loss: 0.6149 - val_acc: 0.6430\n",
            "Epoch 31/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6226 - acc: 0.6350Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6073 - acc: 0.6445\n",
            "Epoch 00031: val_loss improved from 0.61489 to 0.61157, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6222 - acc: 0.6374 - val_loss: 0.6116 - val_acc: 0.6449\n",
            "Epoch 32/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6253 - acc: 0.6324Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6280 - acc: 0.6367\n",
            "Epoch 00032: val_loss did not improve from 0.61157\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.6252 - acc: 0.6339 - val_loss: 0.6234 - val_acc: 0.6374\n",
            "Epoch 33/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6199 - acc: 0.6350Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6263 - acc: 0.6406\n",
            "Epoch 00033: val_loss did not improve from 0.61157\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.6237 - acc: 0.6369 - val_loss: 0.6197 - val_acc: 0.6449\n",
            "Epoch 34/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6216 - acc: 0.6276Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5923 - acc: 0.6582\n",
            "Epoch 00034: val_loss improved from 0.61157 to 0.60941, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.6213 - acc: 0.6299 - val_loss: 0.6094 - val_acc: 0.6523\n",
            "Epoch 35/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6334 - acc: 0.6371Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6167 - acc: 0.6543\n",
            "Epoch 00035: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.6323 - acc: 0.6379 - val_loss: 0.6454 - val_acc: 0.6486\n",
            "Epoch 36/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6235 - acc: 0.6297Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6078 - acc: 0.6660\n",
            "Epoch 00036: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6217 - acc: 0.6334 - val_loss: 0.6252 - val_acc: 0.6617\n",
            "Epoch 37/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6179 - acc: 0.6180Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6014 - acc: 0.6621\n",
            "Epoch 00037: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.6172 - acc: 0.6200 - val_loss: 0.6287 - val_acc: 0.6561\n",
            "Epoch 38/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6230 - acc: 0.6424Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6078 - acc: 0.6348\n",
            "Epoch 00038: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6234 - acc: 0.6408 - val_loss: 0.6256 - val_acc: 0.6336\n",
            "Epoch 39/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6259 - acc: 0.6324Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6043 - acc: 0.6387\n",
            "Epoch 00039: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.6254 - acc: 0.6344 - val_loss: 0.6300 - val_acc: 0.6336\n",
            "Epoch 40/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6176 - acc: 0.6318Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5977 - acc: 0.6406\n",
            "Epoch 00040: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6158 - acc: 0.6329 - val_loss: 0.6167 - val_acc: 0.6374\n",
            "Epoch 41/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6191 - acc: 0.6354Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6031 - acc: 0.6738\n",
            "Epoch 00041: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.6189 - acc: 0.6318 - val_loss: 0.6193 - val_acc: 0.6673\n",
            "Epoch 42/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6252 - acc: 0.6286Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6054 - acc: 0.6445\n",
            "Epoch 00042: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6238 - acc: 0.6294 - val_loss: 0.6210 - val_acc: 0.6393\n",
            "Epoch 43/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6118 - acc: 0.6281Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6093 - acc: 0.6406\n",
            "Epoch 00043: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6117 - acc: 0.6289 - val_loss: 0.6326 - val_acc: 0.6374\n",
            "Epoch 44/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6109 - acc: 0.6340Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6011 - acc: 0.6328\n",
            "Epoch 00044: val_loss did not improve from 0.60941\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6107 - acc: 0.6354 - val_loss: 0.6147 - val_acc: 0.6336\n",
            "Epoch 45/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6196 - acc: 0.6500Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5980 - acc: 0.6484\n",
            "Epoch 00045: val_loss improved from 0.60941 to 0.60882, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6153 - acc: 0.6508 - val_loss: 0.6088 - val_acc: 0.6449\n",
            "Epoch 46/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6175 - acc: 0.6451Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6331 - acc: 0.6016\n",
            "Epoch 00046: val_loss did not improve from 0.60882\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.6198 - acc: 0.6428 - val_loss: 0.6421 - val_acc: 0.5981\n",
            "Epoch 47/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6168 - acc: 0.6456Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6027 - acc: 0.6523\n",
            "Epoch 00047: val_loss improved from 0.60882 to 0.58689, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.6138 - acc: 0.6468 - val_loss: 0.5869 - val_acc: 0.6542\n",
            "Epoch 48/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6074 - acc: 0.6333Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6022 - acc: 0.6504\n",
            "Epoch 00048: val_loss improved from 0.58689 to 0.58467, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.6078 - acc: 0.6313 - val_loss: 0.5847 - val_acc: 0.6523\n",
            "Epoch 49/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6133 - acc: 0.6424Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6129 - acc: 0.6309\n",
            "Epoch 00049: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6088 - acc: 0.6473 - val_loss: 0.5881 - val_acc: 0.6355\n",
            "Epoch 50/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6163 - acc: 0.6408Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6074 - acc: 0.6484\n",
            "Epoch 00050: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6166 - acc: 0.6384 - val_loss: 0.5864 - val_acc: 0.6561\n",
            "Epoch 51/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6118 - acc: 0.6446Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6040 - acc: 0.6270\n",
            "Epoch 00051: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.6102 - acc: 0.6463 - val_loss: 0.5890 - val_acc: 0.6299\n",
            "Epoch 52/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6150 - acc: 0.6334Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6210 - acc: 0.6191\n",
            "Epoch 00052: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.6154 - acc: 0.6344 - val_loss: 0.6201 - val_acc: 0.6150\n",
            "Epoch 53/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6103 - acc: 0.6391Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6187 - acc: 0.6543\n",
            "Epoch 00053: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.6097 - acc: 0.6426 - val_loss: 0.6154 - val_acc: 0.6523\n",
            "Epoch 54/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6121 - acc: 0.6378Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6022 - acc: 0.6602\n",
            "Epoch 00054: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6116 - acc: 0.6370 - val_loss: 0.5893 - val_acc: 0.6561\n",
            "Epoch 55/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6107 - acc: 0.6371Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6038 - acc: 0.6582\n",
            "Epoch 00055: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6102 - acc: 0.6388 - val_loss: 0.5933 - val_acc: 0.6561\n",
            "Epoch 56/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6100 - acc: 0.6424Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6007 - acc: 0.6387\n",
            "Epoch 00056: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 395ms/step - loss: 0.6100 - acc: 0.6428 - val_loss: 0.5903 - val_acc: 0.6411\n",
            "Epoch 57/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6027 - acc: 0.6484Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6151 - acc: 0.6387\n",
            "Epoch 00057: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6057 - acc: 0.6460 - val_loss: 0.6144 - val_acc: 0.6355\n",
            "Epoch 58/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6167 - acc: 0.6255Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6006 - acc: 0.6660\n",
            "Epoch 00058: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.6151 - acc: 0.6259 - val_loss: 0.5915 - val_acc: 0.6617\n",
            "Epoch 59/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6090 - acc: 0.6584Epoch 1/500\n",
            " 5/16 [========>.....................] - ETA: 3s - loss: 0.5961 - acc: 0.6654\n",
            "Epoch 00059: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.6087 - acc: 0.6613 - val_loss: 0.5961 - val_acc: 0.6654\n",
            "Epoch 60/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6074 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6018 - acc: 0.6582\n",
            "Epoch 00060: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.6098 - acc: 0.6504 - val_loss: 0.6139 - val_acc: 0.6561\n",
            "Epoch 61/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6104 - acc: 0.6493Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5969 - acc: 0.6562\n",
            "Epoch 00061: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.6076 - acc: 0.6547 - val_loss: 0.6055 - val_acc: 0.6505\n",
            "Epoch 62/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6226 - acc: 0.6472Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6107 - acc: 0.6582\n",
            "Epoch 00062: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.6173 - acc: 0.6518 - val_loss: 0.6199 - val_acc: 0.6561\n",
            "Epoch 63/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6048 - acc: 0.6440Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6079 - acc: 0.6582\n",
            "Epoch 00063: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6091 - acc: 0.6416 - val_loss: 0.6181 - val_acc: 0.6561\n",
            "Epoch 64/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6028 - acc: 0.6536Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6045 - acc: 0.6484\n",
            "Epoch 00064: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.6017 - acc: 0.6509 - val_loss: 0.6109 - val_acc: 0.6449\n",
            "Epoch 65/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6120 - acc: 0.6446Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5995 - acc: 0.6387\n",
            "Epoch 00065: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6115 - acc: 0.6443 - val_loss: 0.6042 - val_acc: 0.6336\n",
            "Epoch 66/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6110 - acc: 0.6408Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6004 - acc: 0.6445\n",
            "Epoch 00066: val_loss did not improve from 0.58467\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6080 - acc: 0.6438 - val_loss: 0.6084 - val_acc: 0.6449\n",
            "Epoch 67/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6171 - acc: 0.6313Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5963 - acc: 0.6426\n",
            "Epoch 00067: val_loss improved from 0.58467 to 0.58451, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.6170 - acc: 0.6294 - val_loss: 0.5845 - val_acc: 0.6430\n",
            "Epoch 68/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6019 - acc: 0.6578Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6165 - acc: 0.6270\n",
            "Epoch 00068: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.6031 - acc: 0.6557 - val_loss: 0.5991 - val_acc: 0.6299\n",
            "Epoch 69/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6080 - acc: 0.6446Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6022 - acc: 0.6074\n",
            "Epoch 00069: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.6064 - acc: 0.6463 - val_loss: 0.5935 - val_acc: 0.6112\n",
            "Epoch 70/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6088 - acc: 0.6393Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6093 - acc: 0.6426\n",
            "Epoch 00070: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.6079 - acc: 0.6443 - val_loss: 0.5885 - val_acc: 0.6486\n",
            "Epoch 71/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6045 - acc: 0.6430Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6118 - acc: 0.6426\n",
            "Epoch 00071: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6023 - acc: 0.6463 - val_loss: 0.5877 - val_acc: 0.6486\n",
            "Epoch 72/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6080 - acc: 0.6516Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6007 - acc: 0.6484\n",
            "Epoch 00072: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.6101 - acc: 0.6519 - val_loss: 0.5846 - val_acc: 0.6542\n",
            "Epoch 73/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5996 - acc: 0.6600Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6141 - acc: 0.6445\n",
            "Epoch 00073: val_loss did not improve from 0.58451\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5986 - acc: 0.6638 - val_loss: 0.5931 - val_acc: 0.6505\n",
            "Epoch 74/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6131 - acc: 0.6547Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6132 - acc: 0.6465\n",
            "Epoch 00074: val_loss improved from 0.58451 to 0.58435, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.6102 - acc: 0.6572 - val_loss: 0.5844 - val_acc: 0.6523\n",
            "Epoch 75/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5991 - acc: 0.6454Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5971 - acc: 0.6523\n",
            "Epoch 00075: val_loss improved from 0.58435 to 0.57929, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5989 - acc: 0.6466 - val_loss: 0.5793 - val_acc: 0.6561\n",
            "Epoch 76/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5990 - acc: 0.6495Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6121 - acc: 0.6094\n",
            "Epoch 00076: val_loss did not improve from 0.57929\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6009 - acc: 0.6499 - val_loss: 0.6072 - val_acc: 0.6112\n",
            "Epoch 77/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5944 - acc: 0.6649Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6164 - acc: 0.6504\n",
            "Epoch 00077: val_loss did not improve from 0.57929\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5955 - acc: 0.6638 - val_loss: 0.5888 - val_acc: 0.6579\n",
            "Epoch 78/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6019 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5998 - acc: 0.6562\n",
            "Epoch 00078: val_loss did not improve from 0.57929\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.6010 - acc: 0.6552 - val_loss: 0.5825 - val_acc: 0.6579\n",
            "Epoch 79/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6207 - acc: 0.6453Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5935 - acc: 0.6641\n",
            "Epoch 00079: val_loss did not improve from 0.57929\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.6177 - acc: 0.6499 - val_loss: 0.5876 - val_acc: 0.6636\n",
            "Epoch 80/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6093 - acc: 0.6493Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5936 - acc: 0.6562\n",
            "Epoch 00080: val_loss improved from 0.57929 to 0.57853, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6070 - acc: 0.6533 - val_loss: 0.5785 - val_acc: 0.6617\n",
            "Epoch 81/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5881 - acc: 0.6573Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6106 - acc: 0.6445\n",
            "Epoch 00081: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5889 - acc: 0.6582 - val_loss: 0.5914 - val_acc: 0.6486\n",
            "Epoch 82/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6105 - acc: 0.6462Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5997 - acc: 0.6289\n",
            "Epoch 00082: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.6077 - acc: 0.6513 - val_loss: 0.5839 - val_acc: 0.6299\n",
            "Epoch 83/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6086 - acc: 0.6589Epoch 1/500\n",
            " 5/16 [========>.....................] - ETA: 3s - loss: 0.5850 - acc: 0.6654\n",
            "Epoch 00083: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.6087 - acc: 0.6598 - val_loss: 0.5850 - val_acc: 0.6654\n",
            "Epoch 84/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5963 - acc: 0.6469Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5973 - acc: 0.6562\n",
            "Epoch 00084: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5950 - acc: 0.6504 - val_loss: 0.5881 - val_acc: 0.6617\n",
            "Epoch 85/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6033 - acc: 0.6414Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5956 - acc: 0.6562\n",
            "Epoch 00085: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.6051 - acc: 0.6379 - val_loss: 0.5986 - val_acc: 0.6561\n",
            "Epoch 86/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5993 - acc: 0.6599Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5962 - acc: 0.6484\n",
            "Epoch 00086: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.6008 - acc: 0.6552 - val_loss: 0.5988 - val_acc: 0.6486\n",
            "Epoch 87/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5979 - acc: 0.6621Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5897 - acc: 0.6602\n",
            "Epoch 00087: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5973 - acc: 0.6622 - val_loss: 0.5987 - val_acc: 0.6561\n",
            "Epoch 88/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6047 - acc: 0.6403Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5961 - acc: 0.6406\n",
            "Epoch 00088: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6055 - acc: 0.6443 - val_loss: 0.5981 - val_acc: 0.6355\n",
            "Epoch 89/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5973 - acc: 0.6547Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5996 - acc: 0.6484\n",
            "Epoch 00089: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5979 - acc: 0.6538 - val_loss: 0.6004 - val_acc: 0.6467\n",
            "Epoch 90/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5965 - acc: 0.6627Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5949 - acc: 0.6641\n",
            "Epoch 00090: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5962 - acc: 0.6663 - val_loss: 0.5982 - val_acc: 0.6579\n",
            "Epoch 91/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6027 - acc: 0.6474Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5943 - acc: 0.6582\n",
            "Epoch 00091: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.6037 - acc: 0.6475 - val_loss: 0.5949 - val_acc: 0.6561\n",
            "Epoch 92/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5986 - acc: 0.6509Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.6016 - acc: 0.6582\n",
            "Epoch 00092: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5981 - acc: 0.6552 - val_loss: 0.5976 - val_acc: 0.6598\n",
            "Epoch 93/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5853 - acc: 0.6599Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5953 - acc: 0.6719\n",
            "Epoch 00093: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5859 - acc: 0.6602 - val_loss: 0.6061 - val_acc: 0.6673\n",
            "Epoch 94/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6012 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5922 - acc: 0.6504\n",
            "Epoch 00094: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5991 - acc: 0.6608 - val_loss: 0.5847 - val_acc: 0.6486\n",
            "Epoch 95/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5989 - acc: 0.6626Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5978 - acc: 0.6484\n",
            "Epoch 00095: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5988 - acc: 0.6637 - val_loss: 0.5903 - val_acc: 0.6561\n",
            "Epoch 96/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6000 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6018 - acc: 0.6523\n",
            "Epoch 00096: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.6005 - acc: 0.6548 - val_loss: 0.5945 - val_acc: 0.6598\n",
            "Epoch 97/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5919 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6004 - acc: 0.6387\n",
            "Epoch 00097: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5917 - acc: 0.6607 - val_loss: 0.5965 - val_acc: 0.6449\n",
            "Epoch 98/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5956 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5947 - acc: 0.6543\n",
            "Epoch 00098: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5947 - acc: 0.6647 - val_loss: 0.5910 - val_acc: 0.6636\n",
            "Epoch 99/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6024 - acc: 0.6405Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6075 - acc: 0.6445\n",
            "Epoch 00099: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.6005 - acc: 0.6421 - val_loss: 0.6017 - val_acc: 0.6486\n",
            "Epoch 100/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5950 - acc: 0.6771Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5885 - acc: 0.6582\n",
            "Epoch 00100: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5943 - acc: 0.6777 - val_loss: 0.5822 - val_acc: 0.6617\n",
            "Epoch 101/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5917 - acc: 0.6472Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5936 - acc: 0.6504\n",
            "Epoch 00101: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5913 - acc: 0.6483 - val_loss: 0.5942 - val_acc: 0.6523\n",
            "Epoch 102/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5904 - acc: 0.6647Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5894 - acc: 0.6680\n",
            "Epoch 00102: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5909 - acc: 0.6632 - val_loss: 0.5881 - val_acc: 0.6673\n",
            "Epoch 103/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6020 - acc: 0.6546Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5887 - acc: 0.6582\n",
            "Epoch 00103: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5969 - acc: 0.6542 - val_loss: 0.5917 - val_acc: 0.6561\n",
            "Epoch 104/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5955 - acc: 0.6552Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5983 - acc: 0.6445\n",
            "Epoch 00104: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5953 - acc: 0.6582 - val_loss: 0.5913 - val_acc: 0.6467\n",
            "Epoch 105/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5934 - acc: 0.6557Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6147 - acc: 0.6641\n",
            "Epoch 00105: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5937 - acc: 0.6577 - val_loss: 0.6026 - val_acc: 0.6692\n",
            "Epoch 106/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5939 - acc: 0.6630Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6075 - acc: 0.6484\n",
            "Epoch 00106: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 388ms/step - loss: 0.5912 - acc: 0.6675 - val_loss: 0.5938 - val_acc: 0.6523\n",
            "Epoch 107/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5956 - acc: 0.6551Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6085 - acc: 0.6523\n",
            "Epoch 00107: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5962 - acc: 0.6587 - val_loss: 0.5947 - val_acc: 0.6579\n",
            "Epoch 108/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5881 - acc: 0.6594Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5924 - acc: 0.6582\n",
            "Epoch 00108: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.5872 - acc: 0.6621 - val_loss: 0.5805 - val_acc: 0.6617\n",
            "Epoch 109/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6076 - acc: 0.6419Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5979 - acc: 0.6602\n",
            "Epoch 00109: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.6089 - acc: 0.6428 - val_loss: 0.5884 - val_acc: 0.6636\n",
            "Epoch 110/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5862 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5926 - acc: 0.6504\n",
            "Epoch 00110: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5883 - acc: 0.6721 - val_loss: 0.5827 - val_acc: 0.6542\n",
            "Epoch 111/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5939 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5965 - acc: 0.6602\n",
            "Epoch 00111: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5923 - acc: 0.6657 - val_loss: 0.5869 - val_acc: 0.6636\n",
            "Epoch 112/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5882 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5861 - acc: 0.6523\n",
            "Epoch 00112: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5874 - acc: 0.6662 - val_loss: 0.5820 - val_acc: 0.6542\n",
            "Epoch 113/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5885 - acc: 0.6546Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5963 - acc: 0.6562\n",
            "Epoch 00113: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5862 - acc: 0.6562 - val_loss: 0.5816 - val_acc: 0.6598\n",
            "Epoch 114/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5983 - acc: 0.6474Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5970 - acc: 0.6484\n",
            "Epoch 00114: val_loss did not improve from 0.57853\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5996 - acc: 0.6465 - val_loss: 0.5805 - val_acc: 0.6579\n",
            "Epoch 115/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5881 - acc: 0.6684Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5832 - acc: 0.6348\n",
            "Epoch 00115: val_loss improved from 0.57853 to 0.56184, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5887 - acc: 0.6701 - val_loss: 0.5618 - val_acc: 0.6430\n",
            "Epoch 116/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5924 - acc: 0.6594Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5977 - acc: 0.6426\n",
            "Epoch 00116: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5935 - acc: 0.6597 - val_loss: 0.5756 - val_acc: 0.6523\n",
            "Epoch 117/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5917 - acc: 0.6727Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5992 - acc: 0.6465\n",
            "Epoch 00117: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5946 - acc: 0.6672 - val_loss: 0.5770 - val_acc: 0.6561\n",
            "Epoch 118/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5794 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5956 - acc: 0.6602\n",
            "Epoch 00118: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5785 - acc: 0.6751 - val_loss: 0.5711 - val_acc: 0.6692\n",
            "Epoch 119/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5903 - acc: 0.6759Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5921 - acc: 0.6445\n",
            "Epoch 00119: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5887 - acc: 0.6796 - val_loss: 0.5634 - val_acc: 0.6523\n",
            "Epoch 120/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5835 - acc: 0.6790Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5993 - acc: 0.6348\n",
            "Epoch 00120: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 5s 322ms/step - loss: 0.5885 - acc: 0.6736 - val_loss: 0.5736 - val_acc: 0.6449\n",
            "Epoch 121/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5859 - acc: 0.6615Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5907 - acc: 0.6426\n",
            "Epoch 00121: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5873 - acc: 0.6612 - val_loss: 0.5733 - val_acc: 0.6523\n",
            "Epoch 122/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5935 - acc: 0.6599Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5861 - acc: 0.6543\n",
            "Epoch 00122: val_loss did not improve from 0.56184\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5950 - acc: 0.6567 - val_loss: 0.5678 - val_acc: 0.6636\n",
            "Epoch 123/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5722 - acc: 0.6801Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5874 - acc: 0.6562\n",
            "Epoch 00123: val_loss improved from 0.56184 to 0.56130, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 397ms/step - loss: 0.5721 - acc: 0.6816 - val_loss: 0.5613 - val_acc: 0.6654\n",
            "Epoch 124/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5963 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5904 - acc: 0.6641\n",
            "Epoch 00124: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5967 - acc: 0.6662 - val_loss: 0.5868 - val_acc: 0.6673\n",
            "Epoch 125/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5897 - acc: 0.6727Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5935 - acc: 0.6719\n",
            "Epoch 00125: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5898 - acc: 0.6701 - val_loss: 0.5785 - val_acc: 0.6748\n",
            "Epoch 126/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5782 - acc: 0.6661Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5877 - acc: 0.6582\n",
            "Epoch 00126: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5790 - acc: 0.6646 - val_loss: 0.5829 - val_acc: 0.6579\n",
            "Epoch 127/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5862 - acc: 0.6703Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6009 - acc: 0.6719\n",
            "Epoch 00127: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5866 - acc: 0.6714 - val_loss: 0.5908 - val_acc: 0.6729\n",
            "Epoch 128/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.6022 - acc: 0.6605Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5952 - acc: 0.6523\n",
            "Epoch 00128: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5985 - acc: 0.6652 - val_loss: 0.5858 - val_acc: 0.6523\n",
            "Epoch 129/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5961 - acc: 0.6547Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5981 - acc: 0.6465\n",
            "Epoch 00129: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5919 - acc: 0.6587 - val_loss: 0.5818 - val_acc: 0.6523\n",
            "Epoch 130/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5910 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5907 - acc: 0.6699\n",
            "Epoch 00130: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5934 - acc: 0.6692 - val_loss: 0.5694 - val_acc: 0.6748\n",
            "Epoch 131/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5789 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5913 - acc: 0.6602\n",
            "Epoch 00131: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5761 - acc: 0.6711 - val_loss: 0.5707 - val_acc: 0.6654\n",
            "Epoch 132/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5795 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6048 - acc: 0.6387\n",
            "Epoch 00132: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5793 - acc: 0.6776 - val_loss: 0.5911 - val_acc: 0.6393\n",
            "Epoch 133/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5943 - acc: 0.6621Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5899 - acc: 0.6621\n",
            "Epoch 00133: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5901 - acc: 0.6667 - val_loss: 0.5662 - val_acc: 0.6673\n",
            "Epoch 134/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5883 - acc: 0.6589Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5868 - acc: 0.6641\n",
            "Epoch 00134: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5845 - acc: 0.6602 - val_loss: 0.5638 - val_acc: 0.6673\n",
            "Epoch 135/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5910 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5986 - acc: 0.6582\n",
            "Epoch 00135: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5930 - acc: 0.6542 - val_loss: 0.5704 - val_acc: 0.6636\n",
            "Epoch 136/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5775 - acc: 0.6706Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5953 - acc: 0.6582\n",
            "Epoch 00136: val_loss did not improve from 0.56130\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5869 - acc: 0.6672 - val_loss: 0.5743 - val_acc: 0.6617\n",
            "Epoch 137/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5850 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5849 - acc: 0.6777\n",
            "Epoch 00137: val_loss improved from 0.56130 to 0.55994, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 0.5844 - acc: 0.6731 - val_loss: 0.5599 - val_acc: 0.6804\n",
            "Epoch 138/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5817 - acc: 0.6700Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5815 - acc: 0.6777\n",
            "Epoch 00138: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5788 - acc: 0.6726 - val_loss: 0.5674 - val_acc: 0.6785\n",
            "Epoch 139/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5785 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6030 - acc: 0.6445\n",
            "Epoch 00139: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5785 - acc: 0.6771 - val_loss: 0.5729 - val_acc: 0.6486\n",
            "Epoch 140/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5760 - acc: 0.6743Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6057 - acc: 0.6523\n",
            "Epoch 00140: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5769 - acc: 0.6736 - val_loss: 0.5736 - val_acc: 0.6561\n",
            "Epoch 141/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5963 - acc: 0.6693Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5901 - acc: 0.6699\n",
            "Epoch 00141: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5977 - acc: 0.6680 - val_loss: 0.5725 - val_acc: 0.6729\n",
            "Epoch 142/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.6876Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5885 - acc: 0.6602\n",
            "Epoch 00142: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5764 - acc: 0.6845 - val_loss: 0.5699 - val_acc: 0.6636\n",
            "Epoch 143/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5938 - acc: 0.6674Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5953 - acc: 0.6348\n",
            "Epoch 00143: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5910 - acc: 0.6692 - val_loss: 0.5818 - val_acc: 0.6374\n",
            "Epoch 144/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5889 - acc: 0.6679Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5938 - acc: 0.6504\n",
            "Epoch 00144: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5876 - acc: 0.6672 - val_loss: 0.5732 - val_acc: 0.6542\n",
            "Epoch 145/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5759 - acc: 0.6769Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5873 - acc: 0.6660\n",
            "Epoch 00145: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5747 - acc: 0.6751 - val_loss: 0.5629 - val_acc: 0.6673\n",
            "Epoch 146/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5657 - acc: 0.6943Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5901 - acc: 0.6543\n",
            "Epoch 00146: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5712 - acc: 0.6890 - val_loss: 0.5777 - val_acc: 0.6561\n",
            "Epoch 147/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5783 - acc: 0.6769Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5769 - acc: 0.6602\n",
            "Epoch 00147: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5847 - acc: 0.6766 - val_loss: 0.5703 - val_acc: 0.6636\n",
            "Epoch 148/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5756 - acc: 0.6780Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5750 - acc: 0.6699\n",
            "Epoch 00148: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5759 - acc: 0.6776 - val_loss: 0.5670 - val_acc: 0.6710\n",
            "Epoch 149/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5887 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6070 - acc: 0.6484\n",
            "Epoch 00149: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5874 - acc: 0.6701 - val_loss: 0.5934 - val_acc: 0.6523\n",
            "Epoch 150/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5828 - acc: 0.6642Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5868 - acc: 0.6504\n",
            "Epoch 00150: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5848 - acc: 0.6642 - val_loss: 0.5797 - val_acc: 0.6505\n",
            "Epoch 151/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5737 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5864 - acc: 0.6699\n",
            "Epoch 00151: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5714 - acc: 0.6786 - val_loss: 0.5688 - val_acc: 0.6692\n",
            "Epoch 152/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5907 - acc: 0.6568Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5836 - acc: 0.6680\n",
            "Epoch 00152: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5901 - acc: 0.6582 - val_loss: 0.5795 - val_acc: 0.6654\n",
            "Epoch 153/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5777 - acc: 0.6695Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5752 - acc: 0.6758\n",
            "Epoch 00153: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5768 - acc: 0.6682 - val_loss: 0.5733 - val_acc: 0.6748\n",
            "Epoch 154/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5873 - acc: 0.6679Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6102 - acc: 0.6426\n",
            "Epoch 00154: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5838 - acc: 0.6701 - val_loss: 0.5994 - val_acc: 0.6449\n",
            "Epoch 155/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5793 - acc: 0.6753Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5785 - acc: 0.6660\n",
            "Epoch 00155: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5784 - acc: 0.6761 - val_loss: 0.5746 - val_acc: 0.6654\n",
            "Epoch 156/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5927 - acc: 0.6668Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6004 - acc: 0.6523\n",
            "Epoch 00156: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5882 - acc: 0.6696 - val_loss: 0.5856 - val_acc: 0.6561\n",
            "Epoch 157/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5730 - acc: 0.6792Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5802 - acc: 0.6777\n",
            "Epoch 00157: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 391ms/step - loss: 0.5714 - acc: 0.6802 - val_loss: 0.5762 - val_acc: 0.6785\n",
            "Epoch 158/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5888 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5813 - acc: 0.6699\n",
            "Epoch 00158: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 394ms/step - loss: 0.5888 - acc: 0.6815 - val_loss: 0.5808 - val_acc: 0.6710\n",
            "Epoch 159/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5685 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5898 - acc: 0.6699\n",
            "Epoch 00159: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5681 - acc: 0.6831 - val_loss: 0.5785 - val_acc: 0.6710\n",
            "Epoch 160/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5806 - acc: 0.6668Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5871 - acc: 0.6621\n",
            "Epoch 00160: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5835 - acc: 0.6642 - val_loss: 0.5801 - val_acc: 0.6654\n",
            "Epoch 161/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5774 - acc: 0.6776Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5915 - acc: 0.6621\n",
            "Epoch 00161: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5737 - acc: 0.6816 - val_loss: 0.5792 - val_acc: 0.6654\n",
            "Epoch 162/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5684 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6699\n",
            "Epoch 00162: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5666 - acc: 0.6791 - val_loss: 0.5740 - val_acc: 0.6729\n",
            "Epoch 163/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5841 - acc: 0.6515Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5752 - acc: 0.6777\n",
            "Epoch 00163: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5825 - acc: 0.6542 - val_loss: 0.5667 - val_acc: 0.6804\n",
            "Epoch 164/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5825 - acc: 0.6732Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5780 - acc: 0.6719\n",
            "Epoch 00164: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5820 - acc: 0.6751 - val_loss: 0.5811 - val_acc: 0.6710\n",
            "Epoch 165/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5723 - acc: 0.6838Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6641\n",
            "Epoch 00165: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5741 - acc: 0.6806 - val_loss: 0.5758 - val_acc: 0.6654\n",
            "Epoch 166/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5801 - acc: 0.6679Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6047 - acc: 0.6465\n",
            "Epoch 00166: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5787 - acc: 0.6667 - val_loss: 0.5892 - val_acc: 0.6505\n",
            "Epoch 167/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5687 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5850 - acc: 0.6699\n",
            "Epoch 00167: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5663 - acc: 0.6841 - val_loss: 0.5690 - val_acc: 0.6673\n",
            "Epoch 168/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5737 - acc: 0.6753Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5822 - acc: 0.6758\n",
            "Epoch 00168: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5730 - acc: 0.6781 - val_loss: 0.5758 - val_acc: 0.6785\n",
            "Epoch 169/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5740 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5707 - acc: 0.6836\n",
            "Epoch 00169: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5752 - acc: 0.6801 - val_loss: 0.5708 - val_acc: 0.6822\n",
            "Epoch 170/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5711 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5866 - acc: 0.6523\n",
            "Epoch 00170: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5681 - acc: 0.6836 - val_loss: 0.6197 - val_acc: 0.6467\n",
            "Epoch 171/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5737 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5995 - acc: 0.6504\n",
            "Epoch 00171: val_loss did not improve from 0.55994\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5721 - acc: 0.6875 - val_loss: 0.5733 - val_acc: 0.6579\n",
            "Epoch 172/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5792 - acc: 0.6637Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5821 - acc: 0.6641\n",
            "Epoch 00172: val_loss improved from 0.55994 to 0.55429, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5756 - acc: 0.6672 - val_loss: 0.5543 - val_acc: 0.6692\n",
            "Epoch 173/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5748 - acc: 0.6880Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5796 - acc: 0.6738\n",
            "Epoch 00173: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5737 - acc: 0.6860 - val_loss: 0.6081 - val_acc: 0.6692\n",
            "Epoch 174/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5596 - acc: 0.6784Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5829 - acc: 0.6797\n",
            "Epoch 00174: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5605 - acc: 0.6800 - val_loss: 0.6081 - val_acc: 0.6748\n",
            "Epoch 175/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5623 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5739 - acc: 0.6641\n",
            "Epoch 00175: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5658 - acc: 0.6786 - val_loss: 0.5949 - val_acc: 0.6598\n",
            "Epoch 176/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5873 - acc: 0.6786Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5954 - acc: 0.6426\n",
            "Epoch 00176: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5857 - acc: 0.6782 - val_loss: 0.5936 - val_acc: 0.6449\n",
            "Epoch 177/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5788 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6602\n",
            "Epoch 00177: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5773 - acc: 0.6796 - val_loss: 0.5824 - val_acc: 0.6598\n",
            "Epoch 178/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5856 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5851 - acc: 0.6680\n",
            "Epoch 00178: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5844 - acc: 0.6726 - val_loss: 0.5705 - val_acc: 0.6692\n",
            "Epoch 179/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5719 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5905 - acc: 0.6484\n",
            "Epoch 00179: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5694 - acc: 0.6836 - val_loss: 0.5794 - val_acc: 0.6505\n",
            "Epoch 180/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5782 - acc: 0.6711Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5827 - acc: 0.6699\n",
            "Epoch 00180: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5802 - acc: 0.6701 - val_loss: 0.5635 - val_acc: 0.6729\n",
            "Epoch 181/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5717 - acc: 0.6800Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5861 - acc: 0.6738\n",
            "Epoch 00181: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5740 - acc: 0.6785 - val_loss: 0.5554 - val_acc: 0.6804\n",
            "Epoch 182/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.6901Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5878 - acc: 0.6660\n",
            "Epoch 00182: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5481 - acc: 0.6920 - val_loss: 0.5805 - val_acc: 0.6692\n",
            "Epoch 183/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5754 - acc: 0.6682Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5833 - acc: 0.6680\n",
            "Epoch 00183: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5705 - acc: 0.6724 - val_loss: 0.5748 - val_acc: 0.6729\n",
            "Epoch 184/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5655 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6222 - acc: 0.6504\n",
            "Epoch 00184: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5639 - acc: 0.6870 - val_loss: 0.6124 - val_acc: 0.6523\n",
            "Epoch 185/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5753 - acc: 0.6663Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5711 - acc: 0.6738\n",
            "Epoch 00185: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5709 - acc: 0.6731 - val_loss: 0.5608 - val_acc: 0.6748\n",
            "Epoch 186/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5709 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5691 - acc: 0.6777\n",
            "Epoch 00186: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5715 - acc: 0.6895 - val_loss: 0.5981 - val_acc: 0.6692\n",
            "Epoch 187/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5672 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5756 - acc: 0.6797\n",
            "Epoch 00187: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 350ms/step - loss: 0.5678 - acc: 0.6816 - val_loss: 0.6057 - val_acc: 0.6710\n",
            "Epoch 188/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5646 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5888 - acc: 0.6660\n",
            "Epoch 00188: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 5s 312ms/step - loss: 0.5654 - acc: 0.6766 - val_loss: 0.6269 - val_acc: 0.6579\n",
            "Epoch 189/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5621 - acc: 0.6769Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5696 - acc: 0.6738\n",
            "Epoch 00189: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5663 - acc: 0.6746 - val_loss: 0.5953 - val_acc: 0.6654\n",
            "Epoch 190/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5625 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5732 - acc: 0.6738\n",
            "Epoch 00190: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5657 - acc: 0.6875 - val_loss: 0.5888 - val_acc: 0.6673\n",
            "Epoch 191/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5810 - acc: 0.6775Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5878 - acc: 0.6738\n",
            "Epoch 00191: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5762 - acc: 0.6786 - val_loss: 0.6182 - val_acc: 0.6692\n",
            "Epoch 192/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5579 - acc: 0.6944Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5754 - acc: 0.6738\n",
            "Epoch 00192: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5610 - acc: 0.6920 - val_loss: 0.5942 - val_acc: 0.6692\n",
            "Epoch 193/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5738 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5785 - acc: 0.6621\n",
            "Epoch 00193: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5738 - acc: 0.6731 - val_loss: 0.6117 - val_acc: 0.6579\n",
            "Epoch 194/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5724 - acc: 0.6766Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5674 - acc: 0.6699\n",
            "Epoch 00194: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5712 - acc: 0.6787 - val_loss: 0.6214 - val_acc: 0.6636\n",
            "Epoch 195/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5723 - acc: 0.6773Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5619 - acc: 0.6855\n",
            "Epoch 00195: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5692 - acc: 0.6785 - val_loss: 0.6033 - val_acc: 0.6804\n",
            "Epoch 196/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5657 - acc: 0.6969Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5882 - acc: 0.6660\n",
            "Epoch 00196: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5683 - acc: 0.6943 - val_loss: 0.6155 - val_acc: 0.6617\n",
            "Epoch 197/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5745 - acc: 0.6816Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5864 - acc: 0.6641\n",
            "Epoch 00197: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5736 - acc: 0.6795 - val_loss: 0.5942 - val_acc: 0.6598\n",
            "Epoch 198/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5661 - acc: 0.6839Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5825 - acc: 0.6582\n",
            "Epoch 00198: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5689 - acc: 0.6826 - val_loss: 0.5866 - val_acc: 0.6579\n",
            "Epoch 199/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5592 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5812 - acc: 0.6738\n",
            "Epoch 00199: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5558 - acc: 0.6990 - val_loss: 0.5843 - val_acc: 0.6729\n",
            "Epoch 200/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5629 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5972 - acc: 0.6680\n",
            "Epoch 00200: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5596 - acc: 0.6850 - val_loss: 0.6010 - val_acc: 0.6654\n",
            "Epoch 201/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5756 - acc: 0.6703Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5840 - acc: 0.6621\n",
            "Epoch 00201: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5736 - acc: 0.6719 - val_loss: 0.5572 - val_acc: 0.6654\n",
            "Epoch 202/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5711 - acc: 0.6786Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5976 - acc: 0.6543\n",
            "Epoch 00202: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5715 - acc: 0.6787 - val_loss: 0.5622 - val_acc: 0.6598\n",
            "Epoch 203/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5606 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5904 - acc: 0.6621\n",
            "Epoch 00203: val_loss did not improve from 0.55429\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5604 - acc: 0.6860 - val_loss: 0.5611 - val_acc: 0.6673\n",
            "Epoch 204/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5691 - acc: 0.6737Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5766 - acc: 0.6641\n",
            "Epoch 00204: val_loss improved from 0.55429 to 0.54964, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5683 - acc: 0.6766 - val_loss: 0.5496 - val_acc: 0.6673\n",
            "Epoch 205/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5650 - acc: 0.6771Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5685 - acc: 0.6680\n",
            "Epoch 00205: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.5672 - acc: 0.6751 - val_loss: 0.5776 - val_acc: 0.6692\n",
            "Epoch 206/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5729 - acc: 0.6759Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5833 - acc: 0.6738\n",
            "Epoch 00206: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5706 - acc: 0.6781 - val_loss: 0.5833 - val_acc: 0.6729\n",
            "Epoch 207/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5607 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5844 - acc: 0.6523\n",
            "Epoch 00207: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5619 - acc: 0.6870 - val_loss: 0.5883 - val_acc: 0.6523\n",
            "Epoch 208/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5683 - acc: 0.6762Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5706 - acc: 0.6914\n",
            "Epoch 00208: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5691 - acc: 0.6800 - val_loss: 0.5746 - val_acc: 0.6935\n",
            "Epoch 209/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5613 - acc: 0.6786Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5782 - acc: 0.6797\n",
            "Epoch 00209: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5577 - acc: 0.6826 - val_loss: 0.5761 - val_acc: 0.6804\n",
            "Epoch 210/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5785 - acc: 0.6751Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5784 - acc: 0.6660\n",
            "Epoch 00210: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5765 - acc: 0.6759 - val_loss: 0.5836 - val_acc: 0.6636\n",
            "Epoch 211/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5761 - acc: 0.6776Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5887 - acc: 0.6797\n",
            "Epoch 00211: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5746 - acc: 0.6796 - val_loss: 0.5933 - val_acc: 0.6804\n",
            "Epoch 212/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5656 - acc: 0.6695Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6120 - acc: 0.6250\n",
            "Epoch 00212: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5662 - acc: 0.6706 - val_loss: 0.6057 - val_acc: 0.6243\n",
            "Epoch 213/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5840 - acc: 0.6836\n",
            "Epoch 00213: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5473 - acc: 0.7021 - val_loss: 0.5922 - val_acc: 0.6822\n",
            "Epoch 214/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5733 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5997 - acc: 0.6523\n",
            "Epoch 00214: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5724 - acc: 0.6806 - val_loss: 0.5989 - val_acc: 0.6523\n",
            "Epoch 215/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5595 - acc: 0.6817Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5766 - acc: 0.6875\n",
            "Epoch 00215: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5613 - acc: 0.6806 - val_loss: 0.5908 - val_acc: 0.6822\n",
            "Epoch 216/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5735 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5824 - acc: 0.6777\n",
            "Epoch 00216: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5720 - acc: 0.6886 - val_loss: 0.6038 - val_acc: 0.6729\n",
            "Epoch 217/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5676 - acc: 0.6943Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5950 - acc: 0.6465\n",
            "Epoch 00217: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5661 - acc: 0.6929 - val_loss: 0.6302 - val_acc: 0.6430\n",
            "Epoch 218/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5605 - acc: 0.6822Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5611 - acc: 0.6875\n",
            "Epoch 00218: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5605 - acc: 0.6816 - val_loss: 0.5938 - val_acc: 0.6804\n",
            "Epoch 219/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5610 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5692 - acc: 0.6816\n",
            "Epoch 00219: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5632 - acc: 0.6885 - val_loss: 0.6048 - val_acc: 0.6748\n",
            "Epoch 220/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5739 - acc: 0.6738\n",
            "Epoch 00220: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5547 - acc: 0.7079 - val_loss: 0.6077 - val_acc: 0.6654\n",
            "Epoch 221/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5591 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5903 - acc: 0.6582\n",
            "Epoch 00221: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5592 - acc: 0.6865 - val_loss: 0.5957 - val_acc: 0.6579\n",
            "Epoch 222/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5600 - acc: 0.6833Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5612 - acc: 0.6758\n",
            "Epoch 00222: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 5s 313ms/step - loss: 0.5636 - acc: 0.6776 - val_loss: 0.5752 - val_acc: 0.6729\n",
            "Epoch 223/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5522 - acc: 0.6918Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5784 - acc: 0.6523\n",
            "Epoch 00223: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5528 - acc: 0.6925 - val_loss: 0.6117 - val_acc: 0.6505\n",
            "Epoch 224/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5734 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5673 - acc: 0.6875\n",
            "Epoch 00224: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5763 - acc: 0.6766 - val_loss: 0.5922 - val_acc: 0.6841\n",
            "Epoch 225/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5620 - acc: 0.6886Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5737 - acc: 0.6738\n",
            "Epoch 00225: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5645 - acc: 0.6850 - val_loss: 0.5876 - val_acc: 0.6673\n",
            "Epoch 226/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5723 - acc: 0.6719\n",
            "Epoch 00226: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5512 - acc: 0.6915 - val_loss: 0.5808 - val_acc: 0.6673\n",
            "Epoch 227/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5667 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5841 - acc: 0.6621\n",
            "Epoch 00227: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5633 - acc: 0.6860 - val_loss: 0.6061 - val_acc: 0.6617\n",
            "Epoch 228/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5549 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5699 - acc: 0.6777\n",
            "Epoch 00228: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5584 - acc: 0.6836 - val_loss: 0.5863 - val_acc: 0.6729\n",
            "Epoch 229/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5567 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5904 - acc: 0.6719\n",
            "Epoch 00229: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5541 - acc: 0.7094 - val_loss: 0.6086 - val_acc: 0.6673\n",
            "Epoch 230/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5685 - acc: 0.6875\n",
            "Epoch 00230: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5571 - acc: 0.6909 - val_loss: 0.5999 - val_acc: 0.6785\n",
            "Epoch 231/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5612 - acc: 0.6914Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5747 - acc: 0.6797\n",
            "Epoch 00231: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5642 - acc: 0.6881 - val_loss: 0.5902 - val_acc: 0.6710\n",
            "Epoch 232/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5602 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5669 - acc: 0.6875\n",
            "Epoch 00232: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5586 - acc: 0.7024 - val_loss: 0.5863 - val_acc: 0.6748\n",
            "Epoch 233/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5679 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5593 - acc: 0.6797\n",
            "Epoch 00233: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5651 - acc: 0.6855 - val_loss: 0.5831 - val_acc: 0.6729\n",
            "Epoch 234/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5508 - acc: 0.6865Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5624 - acc: 0.6855\n",
            "Epoch 00234: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5499 - acc: 0.6880 - val_loss: 0.5904 - val_acc: 0.6766\n",
            "Epoch 235/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5662 - acc: 0.6843Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5814 - acc: 0.6816\n",
            "Epoch 00235: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5646 - acc: 0.6850 - val_loss: 0.5976 - val_acc: 0.6748\n",
            "Epoch 236/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5570 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5670 - acc: 0.6738\n",
            "Epoch 00236: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5566 - acc: 0.6870 - val_loss: 0.5693 - val_acc: 0.6748\n",
            "Epoch 237/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5583 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5763 - acc: 0.6875\n",
            "Epoch 00237: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5577 - acc: 0.6965 - val_loss: 0.5910 - val_acc: 0.6822\n",
            "Epoch 238/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5618 - acc: 0.6748Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5649 - acc: 0.6660\n",
            "Epoch 00238: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5581 - acc: 0.6771 - val_loss: 0.5672 - val_acc: 0.6673\n",
            "Epoch 239/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5555 - acc: 0.6938Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5626 - acc: 0.6973\n",
            "Epoch 00239: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 5s 308ms/step - loss: 0.5581 - acc: 0.6948 - val_loss: 0.5621 - val_acc: 0.6972\n",
            "Epoch 240/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5593 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5696 - acc: 0.6934\n",
            "Epoch 00240: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5616 - acc: 0.6870 - val_loss: 0.5761 - val_acc: 0.6935\n",
            "Epoch 241/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5588 - acc: 0.6827Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5696 - acc: 0.7031\n",
            "Epoch 00241: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5599 - acc: 0.6815 - val_loss: 0.5780 - val_acc: 0.7009\n",
            "Epoch 242/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5700 - acc: 0.6690Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5810 - acc: 0.6699\n",
            "Epoch 00242: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5710 - acc: 0.6706 - val_loss: 0.5863 - val_acc: 0.6692\n",
            "Epoch 243/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5616 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5672 - acc: 0.6855\n",
            "Epoch 00243: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5600 - acc: 0.6841 - val_loss: 0.5672 - val_acc: 0.6897\n",
            "Epoch 244/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5710 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5585 - acc: 0.6836\n",
            "Epoch 00244: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5695 - acc: 0.6841 - val_loss: 0.5726 - val_acc: 0.6860\n",
            "Epoch 245/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5394 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5902 - acc: 0.6875\n",
            "Epoch 00245: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5416 - acc: 0.7073 - val_loss: 0.5860 - val_acc: 0.6879\n",
            "Epoch 246/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5586 - acc: 0.6880Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5627 - acc: 0.6816\n",
            "Epoch 00246: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5600 - acc: 0.6851 - val_loss: 0.5779 - val_acc: 0.6841\n",
            "Epoch 247/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.6978Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5873 - acc: 0.6875\n",
            "Epoch 00247: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5535 - acc: 0.6921 - val_loss: 0.5816 - val_acc: 0.6879\n",
            "Epoch 248/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5624 - acc: 0.6995Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6143 - acc: 0.6484\n",
            "Epoch 00248: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5593 - acc: 0.7007 - val_loss: 0.6022 - val_acc: 0.6523\n",
            "Epoch 249/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5507 - acc: 0.6912Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5750 - acc: 0.6836\n",
            "Epoch 00249: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5516 - acc: 0.6910 - val_loss: 0.5548 - val_acc: 0.6879\n",
            "Epoch 250/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5630 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5808 - acc: 0.6621\n",
            "Epoch 00250: val_loss did not improve from 0.54964\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5630 - acc: 0.6880 - val_loss: 0.5650 - val_acc: 0.6654\n",
            "Epoch 251/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5578 - acc: 0.6859Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5601 - acc: 0.6777\n",
            "Epoch 00251: val_loss improved from 0.54964 to 0.54117, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5564 - acc: 0.6870 - val_loss: 0.5412 - val_acc: 0.6822\n",
            "Epoch 252/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5489 - acc: 0.6950Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5879 - acc: 0.6523\n",
            "Epoch 00252: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5537 - acc: 0.6930 - val_loss: 0.5729 - val_acc: 0.6579\n",
            "Epoch 253/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5586 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5884 - acc: 0.6602\n",
            "Epoch 00253: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5551 - acc: 0.6880 - val_loss: 0.5703 - val_acc: 0.6636\n",
            "Epoch 254/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5550 - acc: 0.6812Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5795 - acc: 0.6816\n",
            "Epoch 00254: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5599 - acc: 0.6796 - val_loss: 0.5666 - val_acc: 0.6822\n",
            "Epoch 255/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5519 - acc: 0.6995Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5773 - acc: 0.6699\n",
            "Epoch 00255: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5489 - acc: 0.7034 - val_loss: 0.5497 - val_acc: 0.6766\n",
            "Epoch 256/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5524 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5701 - acc: 0.6875\n",
            "Epoch 00256: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.5505 - acc: 0.7069 - val_loss: 0.5509 - val_acc: 0.6916\n",
            "Epoch 257/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5561 - acc: 0.6966Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5748 - acc: 0.6816\n",
            "Epoch 00257: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5578 - acc: 0.6925 - val_loss: 0.5550 - val_acc: 0.6860\n",
            "Epoch 258/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5620 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5748 - acc: 0.6836\n",
            "Epoch 00258: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5629 - acc: 0.6885 - val_loss: 0.5549 - val_acc: 0.6897\n",
            "Epoch 259/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5527 - acc: 0.6833Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5964 - acc: 0.6719\n",
            "Epoch 00259: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5545 - acc: 0.6826 - val_loss: 0.5740 - val_acc: 0.6748\n",
            "Epoch 260/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5432 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5845 - acc: 0.6699\n",
            "Epoch 00260: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5462 - acc: 0.6975 - val_loss: 0.5605 - val_acc: 0.6748\n",
            "Epoch 261/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5601 - acc: 0.6818Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5707 - acc: 0.6758\n",
            "Epoch 00261: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 393ms/step - loss: 0.5580 - acc: 0.6846 - val_loss: 0.5597 - val_acc: 0.6822\n",
            "Epoch 262/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5631 - acc: 0.6930Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5827 - acc: 0.6836\n",
            "Epoch 00262: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5615 - acc: 0.6931 - val_loss: 0.5664 - val_acc: 0.6879\n",
            "Epoch 263/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5823 - acc: 0.6719\n",
            "Epoch 00263: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5465 - acc: 0.7064 - val_loss: 0.5631 - val_acc: 0.6766\n",
            "Epoch 264/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6979Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5828 - acc: 0.6797\n",
            "Epoch 00264: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5505 - acc: 0.6992 - val_loss: 0.5603 - val_acc: 0.6841\n",
            "Epoch 265/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.6914Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5798 - acc: 0.6699\n",
            "Epoch 00265: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5516 - acc: 0.6926 - val_loss: 0.5627 - val_acc: 0.6748\n",
            "Epoch 266/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5591 - acc: 0.6844Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5914 - acc: 0.6680\n",
            "Epoch 00266: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5565 - acc: 0.6890 - val_loss: 0.5668 - val_acc: 0.6729\n",
            "Epoch 267/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5376 - acc: 0.7172Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5618 - acc: 0.6895\n",
            "Epoch 00267: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5416 - acc: 0.7154 - val_loss: 0.5465 - val_acc: 0.6953\n",
            "Epoch 268/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5504 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5779 - acc: 0.6816\n",
            "Epoch 00268: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5527 - acc: 0.6860 - val_loss: 0.5528 - val_acc: 0.6879\n",
            "Epoch 269/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5566 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5714 - acc: 0.6621\n",
            "Epoch 00269: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5567 - acc: 0.6915 - val_loss: 0.5481 - val_acc: 0.6692\n",
            "Epoch 270/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5431 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5589 - acc: 0.6914\n",
            "Epoch 00270: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5436 - acc: 0.6970 - val_loss: 0.5733 - val_acc: 0.6916\n",
            "Epoch 271/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5468 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5596 - acc: 0.6816\n",
            "Epoch 00271: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5462 - acc: 0.7014 - val_loss: 0.5673 - val_acc: 0.6804\n",
            "Epoch 272/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5625 - acc: 0.6790Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5860 - acc: 0.6602\n",
            "Epoch 00272: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5613 - acc: 0.6781 - val_loss: 0.6015 - val_acc: 0.6598\n",
            "Epoch 273/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5520 - acc: 0.7029Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5710 - acc: 0.6953\n",
            "Epoch 00273: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.5491 - acc: 0.7044 - val_loss: 0.5845 - val_acc: 0.6935\n",
            "Epoch 274/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5570 - acc: 0.6785Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5612 - acc: 0.6875\n",
            "Epoch 00274: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5549 - acc: 0.6791 - val_loss: 0.5651 - val_acc: 0.6879\n",
            "Epoch 275/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5558 - acc: 0.6912Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5643 - acc: 0.6914\n",
            "Epoch 00275: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5572 - acc: 0.6890 - val_loss: 0.6004 - val_acc: 0.6897\n",
            "Epoch 276/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5399 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6055 - acc: 0.6660\n",
            "Epoch 00276: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 389ms/step - loss: 0.5402 - acc: 0.7019 - val_loss: 0.5672 - val_acc: 0.6673\n",
            "Epoch 277/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5471 - acc: 0.7052Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5769 - acc: 0.6797\n",
            "Epoch 00277: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5454 - acc: 0.7061 - val_loss: 0.5685 - val_acc: 0.6804\n",
            "Epoch 278/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5594 - acc: 0.6816Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5744 - acc: 0.6641\n",
            "Epoch 00278: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5582 - acc: 0.6855 - val_loss: 0.5871 - val_acc: 0.6673\n",
            "Epoch 279/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5502 - acc: 0.6995Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5730 - acc: 0.6777\n",
            "Epoch 00279: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5488 - acc: 0.7002 - val_loss: 0.5698 - val_acc: 0.6785\n",
            "Epoch 280/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.6968Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5665 - acc: 0.6934\n",
            "Epoch 00280: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5481 - acc: 0.6992 - val_loss: 0.5689 - val_acc: 0.6953\n",
            "Epoch 281/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5626 - acc: 0.6895\n",
            "Epoch 00281: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5498 - acc: 0.6925 - val_loss: 0.5768 - val_acc: 0.6879\n",
            "Epoch 282/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5651 - acc: 0.6816\n",
            "Epoch 00282: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5394 - acc: 0.7054 - val_loss: 0.5650 - val_acc: 0.6822\n",
            "Epoch 283/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5543 - acc: 0.6854Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5633 - acc: 0.6797\n",
            "Epoch 00283: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5540 - acc: 0.6860 - val_loss: 0.5658 - val_acc: 0.6804\n",
            "Epoch 284/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5498 - acc: 0.6943Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5639 - acc: 0.6816\n",
            "Epoch 00284: val_loss did not improve from 0.54117\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5485 - acc: 0.6982 - val_loss: 0.5692 - val_acc: 0.6785\n",
            "Epoch 285/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5552 - acc: 0.6828Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5766 - acc: 0.6758\n",
            "Epoch 00285: val_loss improved from 0.54117 to 0.53390, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5525 - acc: 0.6846 - val_loss: 0.5339 - val_acc: 0.6822\n",
            "Epoch 286/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5413 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5851 - acc: 0.6699\n",
            "Epoch 00286: val_loss improved from 0.53390 to 0.53351, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5412 - acc: 0.7009 - val_loss: 0.5335 - val_acc: 0.6804\n",
            "Epoch 287/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.6833Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5618 - acc: 0.6973\n",
            "Epoch 00287: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5433 - acc: 0.6811 - val_loss: 0.5683 - val_acc: 0.6916\n",
            "Epoch 288/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5540 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5763 - acc: 0.6777\n",
            "Epoch 00288: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5548 - acc: 0.7029 - val_loss: 0.5847 - val_acc: 0.6766\n",
            "Epoch 289/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5468 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5889 - acc: 0.6699\n",
            "Epoch 00289: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5493 - acc: 0.6900 - val_loss: 0.5728 - val_acc: 0.6748\n",
            "Epoch 290/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5563 - acc: 0.6796Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5794 - acc: 0.6719\n",
            "Epoch 00290: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.5540 - acc: 0.6841 - val_loss: 0.5663 - val_acc: 0.6729\n",
            "Epoch 291/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5442 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5735 - acc: 0.6738\n",
            "Epoch 00291: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5462 - acc: 0.7000 - val_loss: 0.5551 - val_acc: 0.6785\n",
            "Epoch 292/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5410 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5853 - acc: 0.6719\n",
            "Epoch 00292: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5428 - acc: 0.6980 - val_loss: 0.5720 - val_acc: 0.6766\n",
            "Epoch 293/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5495 - acc: 0.7034Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5638 - acc: 0.6855\n",
            "Epoch 00293: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5524 - acc: 0.7024 - val_loss: 0.5388 - val_acc: 0.6916\n",
            "Epoch 294/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5496 - acc: 0.6932Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5677 - acc: 0.6855\n",
            "Epoch 00294: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5489 - acc: 0.6973 - val_loss: 0.5437 - val_acc: 0.6916\n",
            "Epoch 295/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5456 - acc: 0.6966Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5597 - acc: 0.7012\n",
            "Epoch 00295: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5455 - acc: 0.6985 - val_loss: 0.5346 - val_acc: 0.7047\n",
            "Epoch 296/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5397 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5600 - acc: 0.6816\n",
            "Epoch 00296: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5400 - acc: 0.6960 - val_loss: 0.5382 - val_acc: 0.6860\n",
            "Epoch 297/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5415 - acc: 0.6984Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5777 - acc: 0.6758\n",
            "Epoch 00297: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5447 - acc: 0.6967 - val_loss: 0.5605 - val_acc: 0.6785\n",
            "Epoch 298/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5583 - acc: 0.6922Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5728 - acc: 0.6836\n",
            "Epoch 00298: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5571 - acc: 0.6938 - val_loss: 0.5526 - val_acc: 0.6879\n",
            "Epoch 299/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5405 - acc: 0.7034Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5977 - acc: 0.6777\n",
            "Epoch 00299: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5404 - acc: 0.7059 - val_loss: 0.5784 - val_acc: 0.6822\n",
            "Epoch 300/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5341 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5651 - acc: 0.6895\n",
            "Epoch 00300: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5340 - acc: 0.7009 - val_loss: 0.5437 - val_acc: 0.6953\n",
            "Epoch 301/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5627 - acc: 0.6876Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5954 - acc: 0.6660\n",
            "Epoch 00301: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5600 - acc: 0.6906 - val_loss: 0.5751 - val_acc: 0.6692\n",
            "Epoch 302/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5384 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5772 - acc: 0.6895\n",
            "Epoch 00302: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5403 - acc: 0.7002 - val_loss: 0.5559 - val_acc: 0.6935\n",
            "Epoch 303/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5451 - acc: 0.6924Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5732 - acc: 0.6738\n",
            "Epoch 00303: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5450 - acc: 0.6957 - val_loss: 0.5584 - val_acc: 0.6766\n",
            "Epoch 304/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5455 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5680 - acc: 0.6758\n",
            "Epoch 00304: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5426 - acc: 0.6885 - val_loss: 0.5496 - val_acc: 0.6841\n",
            "Epoch 305/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5516 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5706 - acc: 0.6699\n",
            "Epoch 00305: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5547 - acc: 0.6885 - val_loss: 0.5475 - val_acc: 0.6766\n",
            "Epoch 306/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5931 - acc: 0.6602\n",
            "Epoch 00306: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5409 - acc: 0.7049 - val_loss: 0.5734 - val_acc: 0.6636\n",
            "Epoch 307/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5467 - acc: 0.6875Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5966 - acc: 0.6699\n",
            "Epoch 00307: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5485 - acc: 0.6870 - val_loss: 0.5838 - val_acc: 0.6729\n",
            "Epoch 308/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5443 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.6025 - acc: 0.6777\n",
            "Epoch 00308: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5447 - acc: 0.7000 - val_loss: 0.5773 - val_acc: 0.6822\n",
            "Epoch 309/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5504 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5796 - acc: 0.6621\n",
            "Epoch 00309: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5486 - acc: 0.6915 - val_loss: 0.5588 - val_acc: 0.6673\n",
            "Epoch 310/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5382 - acc: 0.7031Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5561 - acc: 0.6875\n",
            "Epoch 00310: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5373 - acc: 0.7012 - val_loss: 0.5582 - val_acc: 0.6841\n",
            "Epoch 311/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5551 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5764 - acc: 0.6836\n",
            "Epoch 00311: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5590 - acc: 0.6860 - val_loss: 0.5672 - val_acc: 0.6879\n",
            "Epoch 312/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5398 - acc: 0.7167Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5760 - acc: 0.6914\n",
            "Epoch 00312: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 385ms/step - loss: 0.5401 - acc: 0.7168 - val_loss: 0.5681 - val_acc: 0.6897\n",
            "Epoch 313/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5401 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5864 - acc: 0.6777\n",
            "Epoch 00313: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5393 - acc: 0.7034 - val_loss: 0.5726 - val_acc: 0.6766\n",
            "Epoch 314/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5513 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5710 - acc: 0.7031\n",
            "Epoch 00314: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5491 - acc: 0.7009 - val_loss: 0.5714 - val_acc: 0.6972\n",
            "Epoch 315/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5525 - acc: 0.6953Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5766 - acc: 0.6914\n",
            "Epoch 00315: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5519 - acc: 0.6955 - val_loss: 0.5618 - val_acc: 0.6935\n",
            "Epoch 316/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5398 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5789 - acc: 0.6777\n",
            "Epoch 00316: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5430 - acc: 0.7099 - val_loss: 0.5705 - val_acc: 0.6766\n",
            "Epoch 317/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5487 - acc: 0.6990Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5684 - acc: 0.6895\n",
            "Epoch 00317: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5475 - acc: 0.6963 - val_loss: 0.5648 - val_acc: 0.6897\n",
            "Epoch 318/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5360 - acc: 0.7097Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5554 - acc: 0.7031\n",
            "Epoch 00318: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5413 - acc: 0.7032 - val_loss: 0.5584 - val_acc: 0.6991\n",
            "Epoch 319/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5674 - acc: 0.7090\n",
            "Epoch 00319: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5467 - acc: 0.6992 - val_loss: 0.5571 - val_acc: 0.7047\n",
            "Epoch 320/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5345 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5735 - acc: 0.6836\n",
            "Epoch 00320: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5357 - acc: 0.7064 - val_loss: 0.5630 - val_acc: 0.6804\n",
            "Epoch 321/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5379 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5745 - acc: 0.6934\n",
            "Epoch 00321: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5373 - acc: 0.7014 - val_loss: 0.5621 - val_acc: 0.6916\n",
            "Epoch 322/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5482 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6012 - acc: 0.6738\n",
            "Epoch 00322: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5478 - acc: 0.7109 - val_loss: 0.5964 - val_acc: 0.6748\n",
            "Epoch 323/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5498 - acc: 0.6923Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5684 - acc: 0.6934\n",
            "Epoch 00323: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5466 - acc: 0.6925 - val_loss: 0.5556 - val_acc: 0.6935\n",
            "Epoch 324/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5469 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5880 - acc: 0.6816\n",
            "Epoch 00324: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5466 - acc: 0.7009 - val_loss: 0.5739 - val_acc: 0.6860\n",
            "Epoch 325/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.6902Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5589 - acc: 0.6934\n",
            "Epoch 00325: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5482 - acc: 0.6880 - val_loss: 0.5482 - val_acc: 0.6935\n",
            "Epoch 326/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5521 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5769 - acc: 0.6758\n",
            "Epoch 00326: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5521 - acc: 0.6935 - val_loss: 0.5730 - val_acc: 0.6766\n",
            "Epoch 327/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5547 - acc: 0.6764Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6914\n",
            "Epoch 00327: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5541 - acc: 0.6806 - val_loss: 0.5707 - val_acc: 0.6916\n",
            "Epoch 328/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5454 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5792 - acc: 0.6875\n",
            "Epoch 00328: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5456 - acc: 0.6980 - val_loss: 0.5732 - val_acc: 0.6879\n",
            "Epoch 329/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5400 - acc: 0.7182Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5699 - acc: 0.6934\n",
            "Epoch 00329: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 390ms/step - loss: 0.5413 - acc: 0.7197 - val_loss: 0.5697 - val_acc: 0.6935\n",
            "Epoch 330/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5351 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5577 - acc: 0.7090\n",
            "Epoch 00330: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5384 - acc: 0.7034 - val_loss: 0.5655 - val_acc: 0.7084\n",
            "Epoch 331/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5326 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.6044 - acc: 0.6758\n",
            "Epoch 00331: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5281 - acc: 0.7124 - val_loss: 0.5951 - val_acc: 0.6785\n",
            "Epoch 332/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5464 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5699 - acc: 0.6875\n",
            "Epoch 00332: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5443 - acc: 0.6876 - val_loss: 0.5634 - val_acc: 0.6879\n",
            "Epoch 333/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5386 - acc: 0.7047Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5638 - acc: 0.6816\n",
            "Epoch 00333: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5360 - acc: 0.7080 - val_loss: 0.5589 - val_acc: 0.6841\n",
            "Epoch 334/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5570 - acc: 0.7019Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5604 - acc: 0.6836\n",
            "Epoch 00334: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5565 - acc: 0.7000 - val_loss: 0.5589 - val_acc: 0.6860\n",
            "Epoch 335/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5545 - acc: 0.6907Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5654 - acc: 0.6992\n",
            "Epoch 00335: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5525 - acc: 0.6950 - val_loss: 0.5638 - val_acc: 0.6953\n",
            "Epoch 336/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5436 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5624 - acc: 0.6914\n",
            "Epoch 00336: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5452 - acc: 0.7014 - val_loss: 0.5485 - val_acc: 0.6935\n",
            "Epoch 337/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5873 - acc: 0.6719\n",
            "Epoch 00337: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5406 - acc: 0.6985 - val_loss: 0.5809 - val_acc: 0.6729\n",
            "Epoch 338/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5433 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5607 - acc: 0.6855\n",
            "Epoch 00338: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5401 - acc: 0.6960 - val_loss: 0.5512 - val_acc: 0.6860\n",
            "Epoch 339/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5403 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5683 - acc: 0.6816\n",
            "Epoch 00339: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5408 - acc: 0.7004 - val_loss: 0.5556 - val_acc: 0.6841\n",
            "Epoch 340/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5471 - acc: 0.6966Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5939 - acc: 0.6777\n",
            "Epoch 00340: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5451 - acc: 0.6975 - val_loss: 0.5881 - val_acc: 0.6766\n",
            "Epoch 341/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5421 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5683 - acc: 0.6836\n",
            "Epoch 00341: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5424 - acc: 0.6920 - val_loss: 0.5643 - val_acc: 0.6860\n",
            "Epoch 342/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5572 - acc: 0.6987Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5586 - acc: 0.6973\n",
            "Epoch 00342: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5539 - acc: 0.6985 - val_loss: 0.5897 - val_acc: 0.6953\n",
            "Epoch 343/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5488 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5577 - acc: 0.7051\n",
            "Epoch 00343: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5477 - acc: 0.6960 - val_loss: 0.5847 - val_acc: 0.7028\n",
            "Epoch 344/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5401 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5555 - acc: 0.6895\n",
            "Epoch 00344: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5357 - acc: 0.7059 - val_loss: 0.5727 - val_acc: 0.6860\n",
            "Epoch 345/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5384 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5593 - acc: 0.6934\n",
            "Epoch 00345: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5408 - acc: 0.6960 - val_loss: 0.5530 - val_acc: 0.6972\n",
            "Epoch 346/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5523 - acc: 0.6849Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5568 - acc: 0.7051\n",
            "Epoch 00346: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5534 - acc: 0.6860 - val_loss: 0.5472 - val_acc: 0.7047\n",
            "Epoch 347/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5501 - acc: 0.6870Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5857 - acc: 0.6621\n",
            "Epoch 00347: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5447 - acc: 0.6925 - val_loss: 0.5587 - val_acc: 0.6673\n",
            "Epoch 348/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5473 - acc: 0.7068Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5911 - acc: 0.6816\n",
            "Epoch 00348: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5477 - acc: 0.7024 - val_loss: 0.5694 - val_acc: 0.6841\n",
            "Epoch 349/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5251 - acc: 0.7115Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5816 - acc: 0.6836\n",
            "Epoch 00349: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5218 - acc: 0.7134 - val_loss: 0.5462 - val_acc: 0.6897\n",
            "Epoch 350/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5355 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5856 - acc: 0.6895\n",
            "Epoch 00350: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5344 - acc: 0.7119 - val_loss: 0.5512 - val_acc: 0.6935\n",
            "Epoch 351/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5463 - acc: 0.6939Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5659 - acc: 0.6855\n",
            "Epoch 00351: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5463 - acc: 0.6955 - val_loss: 0.5395 - val_acc: 0.6897\n",
            "Epoch 352/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5403 - acc: 0.7082Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5711 - acc: 0.6992\n",
            "Epoch 00352: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5445 - acc: 0.7014 - val_loss: 0.5373 - val_acc: 0.7065\n",
            "Epoch 353/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5374 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5768 - acc: 0.6934\n",
            "Epoch 00353: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5361 - acc: 0.7129 - val_loss: 0.5626 - val_acc: 0.6916\n",
            "Epoch 354/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5400 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5784 - acc: 0.6777\n",
            "Epoch 00354: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5374 - acc: 0.7059 - val_loss: 0.5710 - val_acc: 0.6785\n",
            "Epoch 355/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5436 - acc: 0.6806Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5691 - acc: 0.6816\n",
            "Epoch 00355: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5417 - acc: 0.6836 - val_loss: 0.5567 - val_acc: 0.6822\n",
            "Epoch 356/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5344 - acc: 0.7024Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5860 - acc: 0.6934\n",
            "Epoch 00356: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5352 - acc: 0.7014 - val_loss: 0.5655 - val_acc: 0.6991\n",
            "Epoch 357/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5462 - acc: 0.6923Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5734 - acc: 0.6816\n",
            "Epoch 00357: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5445 - acc: 0.6935 - val_loss: 0.5664 - val_acc: 0.6822\n",
            "Epoch 358/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5629 - acc: 0.7031\n",
            "Epoch 00358: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.5363 - acc: 0.7059 - val_loss: 0.5497 - val_acc: 0.7009\n",
            "Epoch 359/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5407 - acc: 0.6995Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5558 - acc: 0.6973\n",
            "Epoch 00359: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5426 - acc: 0.6948 - val_loss: 0.5482 - val_acc: 0.6935\n",
            "Epoch 360/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5384 - acc: 0.7043Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5515 - acc: 0.6973\n",
            "Epoch 00360: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5371 - acc: 0.7037 - val_loss: 0.5367 - val_acc: 0.6991\n",
            "Epoch 361/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5536 - acc: 0.6839Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5694 - acc: 0.6875\n",
            "Epoch 00361: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5525 - acc: 0.6875 - val_loss: 0.5626 - val_acc: 0.6841\n",
            "Epoch 362/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5391 - acc: 0.7072Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5835 - acc: 0.6719\n",
            "Epoch 00362: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5388 - acc: 0.7094 - val_loss: 0.5666 - val_acc: 0.6729\n",
            "Epoch 363/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5414 - acc: 0.7049Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5696 - acc: 0.6875\n",
            "Epoch 00363: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5425 - acc: 0.7078 - val_loss: 0.5735 - val_acc: 0.6860\n",
            "Epoch 364/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5427 - acc: 0.6934Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5637 - acc: 0.6680\n",
            "Epoch 00364: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5455 - acc: 0.6920 - val_loss: 0.5462 - val_acc: 0.6748\n",
            "Epoch 365/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5444 - acc: 0.7000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5653 - acc: 0.6875\n",
            "Epoch 00365: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5450 - acc: 0.7017 - val_loss: 0.5584 - val_acc: 0.6860\n",
            "Epoch 366/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5327 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5617 - acc: 0.7012\n",
            "Epoch 00366: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5325 - acc: 0.7029 - val_loss: 0.5514 - val_acc: 0.7028\n",
            "Epoch 367/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5568 - acc: 0.7148\n",
            "Epoch 00367: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5317 - acc: 0.7104 - val_loss: 0.5537 - val_acc: 0.7140\n",
            "Epoch 368/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5652 - acc: 0.6934\n",
            "Epoch 00368: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5358 - acc: 0.6997 - val_loss: 0.5555 - val_acc: 0.6953\n",
            "Epoch 369/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5490 - acc: 0.6958Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5604 - acc: 0.6895\n",
            "Epoch 00369: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5486 - acc: 0.6958 - val_loss: 0.5461 - val_acc: 0.6935\n",
            "Epoch 370/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5657 - acc: 0.6934\n",
            "Epoch 00370: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5301 - acc: 0.7000 - val_loss: 0.5580 - val_acc: 0.6935\n",
            "Epoch 371/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5341 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6992\n",
            "Epoch 00371: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5329 - acc: 0.7093 - val_loss: 0.5604 - val_acc: 0.6972\n",
            "Epoch 372/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5446 - acc: 0.7052Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5730 - acc: 0.6914\n",
            "Epoch 00372: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5434 - acc: 0.7049 - val_loss: 0.5669 - val_acc: 0.6953\n",
            "Epoch 373/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.6984Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5772 - acc: 0.6875\n",
            "Epoch 00373: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5342 - acc: 0.6978 - val_loss: 0.5640 - val_acc: 0.6860\n",
            "Epoch 374/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5368 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5608 - acc: 0.7148\n",
            "Epoch 00374: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5356 - acc: 0.7069 - val_loss: 0.5478 - val_acc: 0.7103\n",
            "Epoch 375/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5807 - acc: 0.7012\n",
            "Epoch 00375: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 307ms/step - loss: 0.5398 - acc: 0.7019 - val_loss: 0.5663 - val_acc: 0.6972\n",
            "Epoch 376/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5371 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5782 - acc: 0.7031\n",
            "Epoch 00376: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5400 - acc: 0.7034 - val_loss: 0.5688 - val_acc: 0.7009\n",
            "Epoch 377/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5364 - acc: 0.7005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5648 - acc: 0.6895\n",
            "Epoch 00377: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5368 - acc: 0.6992 - val_loss: 0.5491 - val_acc: 0.6897\n",
            "Epoch 378/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5378 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5676 - acc: 0.6875\n",
            "Epoch 00378: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 379ms/step - loss: 0.5363 - acc: 0.7109 - val_loss: 0.5587 - val_acc: 0.6916\n",
            "Epoch 379/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5393 - acc: 0.6928Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5801 - acc: 0.6758\n",
            "Epoch 00379: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5386 - acc: 0.6957 - val_loss: 0.5793 - val_acc: 0.6766\n",
            "Epoch 380/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5403 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5969 - acc: 0.6699\n",
            "Epoch 00380: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5424 - acc: 0.7064 - val_loss: 0.5914 - val_acc: 0.6710\n",
            "Epoch 381/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5329 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5527 - acc: 0.6973\n",
            "Epoch 00381: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5322 - acc: 0.7158 - val_loss: 0.5530 - val_acc: 0.6972\n",
            "Epoch 382/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5320 - acc: 0.7031Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5586 - acc: 0.6992\n",
            "Epoch 00382: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5329 - acc: 0.7014 - val_loss: 0.5600 - val_acc: 0.7009\n",
            "Epoch 383/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5413 - acc: 0.7000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5626 - acc: 0.6738\n",
            "Epoch 00383: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5405 - acc: 0.7021 - val_loss: 0.5596 - val_acc: 0.6766\n",
            "Epoch 384/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5182 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5610 - acc: 0.7031\n",
            "Epoch 00384: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5191 - acc: 0.7183 - val_loss: 0.5586 - val_acc: 0.7047\n",
            "Epoch 385/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5512 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5709 - acc: 0.6855\n",
            "Epoch 00385: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5509 - acc: 0.6940 - val_loss: 0.5651 - val_acc: 0.6860\n",
            "Epoch 386/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5404 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5426 - acc: 0.7168\n",
            "Epoch 00386: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5399 - acc: 0.7044 - val_loss: 0.5457 - val_acc: 0.7178\n",
            "Epoch 387/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5334 - acc: 0.7065Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5881 - acc: 0.6738\n",
            "Epoch 00387: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5348 - acc: 0.7037 - val_loss: 0.5686 - val_acc: 0.6766\n",
            "Epoch 388/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5435 - acc: 0.7052Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5666 - acc: 0.6973\n",
            "Epoch 00388: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5442 - acc: 0.7036 - val_loss: 0.5492 - val_acc: 0.6972\n",
            "Epoch 389/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.7061Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5566 - acc: 0.6895\n",
            "Epoch 00389: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5382 - acc: 0.7027 - val_loss: 0.5427 - val_acc: 0.6935\n",
            "Epoch 390/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7130Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5662 - acc: 0.6934\n",
            "Epoch 00390: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5242 - acc: 0.7163 - val_loss: 0.5837 - val_acc: 0.6916\n",
            "Epoch 391/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.6981Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5692 - acc: 0.6855\n",
            "Epoch 00391: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 355ms/step - loss: 0.5354 - acc: 0.6995 - val_loss: 0.5827 - val_acc: 0.6841\n",
            "Epoch 392/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.7050Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5582 - acc: 0.7188\n",
            "Epoch 00392: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5363 - acc: 0.7039 - val_loss: 0.5645 - val_acc: 0.7178\n",
            "Epoch 393/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5452 - acc: 0.6918Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5675 - acc: 0.6914\n",
            "Epoch 00393: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5412 - acc: 0.6960 - val_loss: 0.5827 - val_acc: 0.6879\n",
            "Epoch 394/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5450 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5626 - acc: 0.6934\n",
            "Epoch 00394: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5429 - acc: 0.6975 - val_loss: 0.5681 - val_acc: 0.6935\n",
            "Epoch 395/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5342 - acc: 0.7005Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5652 - acc: 0.6836\n",
            "Epoch 00395: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 384ms/step - loss: 0.5309 - acc: 0.7051 - val_loss: 0.5624 - val_acc: 0.6841\n",
            "Epoch 396/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5370 - acc: 0.7119Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5540 - acc: 0.7031\n",
            "Epoch 00396: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5359 - acc: 0.7123 - val_loss: 0.5525 - val_acc: 0.7028\n",
            "Epoch 397/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5421 - acc: 0.7010Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5926 - acc: 0.6836\n",
            "Epoch 00397: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5401 - acc: 0.7029 - val_loss: 0.6062 - val_acc: 0.6766\n",
            "Epoch 398/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5318 - acc: 0.6992Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5595 - acc: 0.6738\n",
            "Epoch 00398: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5297 - acc: 0.7019 - val_loss: 0.5878 - val_acc: 0.6673\n",
            "Epoch 399/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5317 - acc: 0.7203Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5661 - acc: 0.7051\n",
            "Epoch 00399: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5283 - acc: 0.7231 - val_loss: 0.5413 - val_acc: 0.7047\n",
            "Epoch 400/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5358 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5484 - acc: 0.7129\n",
            "Epoch 00400: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5358 - acc: 0.7054 - val_loss: 0.5345 - val_acc: 0.7121\n",
            "Epoch 401/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5324 - acc: 0.7065Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5554 - acc: 0.6875\n",
            "Epoch 00401: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5338 - acc: 0.7048 - val_loss: 0.5418 - val_acc: 0.6953\n",
            "Epoch 402/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5383 - acc: 0.7057Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5664 - acc: 0.6895\n",
            "Epoch 00402: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5381 - acc: 0.7056 - val_loss: 0.5499 - val_acc: 0.6972\n",
            "Epoch 403/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7003Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5738 - acc: 0.7012\n",
            "Epoch 00403: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5317 - acc: 0.6970 - val_loss: 0.5699 - val_acc: 0.7047\n",
            "Epoch 404/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5289 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5645 - acc: 0.6895\n",
            "Epoch 00404: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5311 - acc: 0.7144 - val_loss: 0.5556 - val_acc: 0.6953\n",
            "Epoch 405/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5359 - acc: 0.6995Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5841 - acc: 0.6719\n",
            "Epoch 00405: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5333 - acc: 0.7012 - val_loss: 0.5609 - val_acc: 0.6766\n",
            "Epoch 406/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7066Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5499 - acc: 0.7109\n",
            "Epoch 00406: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5224 - acc: 0.7074 - val_loss: 0.5398 - val_acc: 0.7140\n",
            "Epoch 407/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5396 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5540 - acc: 0.7012\n",
            "Epoch 00407: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5380 - acc: 0.7044 - val_loss: 0.5540 - val_acc: 0.7047\n",
            "Epoch 408/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5201 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5793 - acc: 0.6758\n",
            "Epoch 00408: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5212 - acc: 0.7094 - val_loss: 0.5595 - val_acc: 0.6785\n",
            "Epoch 409/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5413 - acc: 0.7008Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5677 - acc: 0.6836\n",
            "Epoch 00409: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.5390 - acc: 0.7044 - val_loss: 0.5538 - val_acc: 0.6897\n",
            "Epoch 410/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5762 - acc: 0.6934\n",
            "Epoch 00410: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5249 - acc: 0.7178 - val_loss: 0.5606 - val_acc: 0.7009\n",
            "Epoch 411/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5302 - acc: 0.7146Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5720 - acc: 0.6934\n",
            "Epoch 00411: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5312 - acc: 0.7139 - val_loss: 0.5536 - val_acc: 0.6972\n",
            "Epoch 412/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5370 - acc: 0.7104Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5929 - acc: 0.6895\n",
            "Epoch 00412: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5375 - acc: 0.7119 - val_loss: 0.5396 - val_acc: 0.7009\n",
            "Epoch 413/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5367 - acc: 0.7043Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5802 - acc: 0.6895\n",
            "Epoch 00413: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5367 - acc: 0.7037 - val_loss: 0.5419 - val_acc: 0.6991\n",
            "Epoch 414/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5185 - acc: 0.7182Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5834 - acc: 0.6758\n",
            "Epoch 00414: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 387ms/step - loss: 0.5225 - acc: 0.7173 - val_loss: 0.5399 - val_acc: 0.6860\n",
            "Epoch 415/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5519 - acc: 0.6881Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5884 - acc: 0.6816\n",
            "Epoch 00415: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5490 - acc: 0.6905 - val_loss: 0.5409 - val_acc: 0.6916\n",
            "Epoch 416/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5296 - acc: 0.7151Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5792 - acc: 0.6895\n",
            "Epoch 00416: val_loss did not improve from 0.53351\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5285 - acc: 0.7154 - val_loss: 0.5468 - val_acc: 0.6972\n",
            "Epoch 417/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5399 - acc: 0.7093Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5750 - acc: 0.6953\n",
            "Epoch 00417: val_loss improved from 0.53351 to 0.52858, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5417 - acc: 0.7054 - val_loss: 0.5286 - val_acc: 0.7047\n",
            "Epoch 418/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5740 - acc: 0.6895\n",
            "Epoch 00418: val_loss improved from 0.52858 to 0.52819, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5366 - acc: 0.7119 - val_loss: 0.5282 - val_acc: 0.6991\n",
            "Epoch 419/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5441 - acc: 0.7000Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5725 - acc: 0.6797\n",
            "Epoch 00419: val_loss improved from 0.52819 to 0.52481, saving model to model_3_benmal_best.h5\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5396 - acc: 0.7068 - val_loss: 0.5248 - val_acc: 0.6897\n",
            "Epoch 420/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5315 - acc: 0.6971Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5665 - acc: 0.6973\n",
            "Epoch 00420: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5265 - acc: 0.7014 - val_loss: 0.5601 - val_acc: 0.6972\n",
            "Epoch 421/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5309 - acc: 0.7099Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5663 - acc: 0.6797\n",
            "Epoch 00421: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5287 - acc: 0.7104 - val_loss: 0.5813 - val_acc: 0.6785\n",
            "Epoch 422/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5351 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5573 - acc: 0.7012\n",
            "Epoch 00422: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5371 - acc: 0.6970 - val_loss: 0.5465 - val_acc: 0.7047\n",
            "Epoch 423/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5279 - acc: 0.7114Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5672 - acc: 0.6914\n",
            "Epoch 00423: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5274 - acc: 0.7109 - val_loss: 0.5534 - val_acc: 0.6916\n",
            "Epoch 424/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5299 - acc: 0.7040Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5514 - acc: 0.6836\n",
            "Epoch 00424: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 353ms/step - loss: 0.5289 - acc: 0.7034 - val_loss: 0.5431 - val_acc: 0.6841\n",
            "Epoch 425/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5319 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5532 - acc: 0.7109\n",
            "Epoch 00425: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5331 - acc: 0.7173 - val_loss: 0.5487 - val_acc: 0.7084\n",
            "Epoch 426/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5237 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5458 - acc: 0.7090\n",
            "Epoch 00426: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.5274 - acc: 0.7074 - val_loss: 0.5382 - val_acc: 0.7103\n",
            "Epoch 427/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5338 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5593 - acc: 0.6875\n",
            "Epoch 00427: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5352 - acc: 0.6975 - val_loss: 0.5584 - val_acc: 0.6935\n",
            "Epoch 428/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7088Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5690 - acc: 0.6738\n",
            "Epoch 00428: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5339 - acc: 0.7034 - val_loss: 0.5695 - val_acc: 0.6766\n",
            "Epoch 429/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5252 - acc: 0.7066Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5642 - acc: 0.6855\n",
            "Epoch 00429: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5247 - acc: 0.7064 - val_loss: 0.5758 - val_acc: 0.6860\n",
            "Epoch 430/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5385 - acc: 0.7057Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5867 - acc: 0.6836\n",
            "Epoch 00430: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 383ms/step - loss: 0.5334 - acc: 0.7095 - val_loss: 0.5881 - val_acc: 0.6841\n",
            "Epoch 431/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5256 - acc: 0.7045Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5633 - acc: 0.6914\n",
            "Epoch 00431: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5298 - acc: 0.7032 - val_loss: 0.5523 - val_acc: 0.6935\n",
            "Epoch 432/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5305 - acc: 0.7013Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5760 - acc: 0.6973\n",
            "Epoch 00432: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5295 - acc: 0.7019 - val_loss: 0.5735 - val_acc: 0.6991\n",
            "Epoch 433/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5381 - acc: 0.7052Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5659 - acc: 0.6934\n",
            "Epoch 00433: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5354 - acc: 0.7056 - val_loss: 0.5554 - val_acc: 0.6972\n",
            "Epoch 434/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5382 - acc: 0.6978Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5545 - acc: 0.6953\n",
            "Epoch 00434: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5382 - acc: 0.6987 - val_loss: 0.5578 - val_acc: 0.6972\n",
            "Epoch 435/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5254 - acc: 0.7036Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5630 - acc: 0.7070\n",
            "Epoch 00435: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 373ms/step - loss: 0.5296 - acc: 0.7046 - val_loss: 0.5636 - val_acc: 0.7084\n",
            "Epoch 436/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5430 - acc: 0.6976Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5612 - acc: 0.7051\n",
            "Epoch 00436: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5415 - acc: 0.6965 - val_loss: 0.5629 - val_acc: 0.7065\n",
            "Epoch 437/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5405 - acc: 0.6955Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5706 - acc: 0.6738\n",
            "Epoch 00437: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5386 - acc: 0.6990 - val_loss: 0.5902 - val_acc: 0.6766\n",
            "Epoch 438/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5208 - acc: 0.7210Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5539 - acc: 0.6953\n",
            "Epoch 00438: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5217 - acc: 0.7233 - val_loss: 0.5422 - val_acc: 0.7009\n",
            "Epoch 439/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5388 - acc: 0.6897Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5557 - acc: 0.6914\n",
            "Epoch 00439: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5370 - acc: 0.6900 - val_loss: 0.5409 - val_acc: 0.6935\n",
            "Epoch 440/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5400 - acc: 0.6891Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5579 - acc: 0.6914\n",
            "Epoch 00440: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5368 - acc: 0.6925 - val_loss: 0.5515 - val_acc: 0.6953\n",
            "Epoch 441/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5361 - acc: 0.7098Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5757 - acc: 0.6953\n",
            "Epoch 00441: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5353 - acc: 0.7109 - val_loss: 0.5621 - val_acc: 0.6972\n",
            "Epoch 442/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5226 - acc: 0.7172Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5565 - acc: 0.7129\n",
            "Epoch 00442: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5204 - acc: 0.7178 - val_loss: 0.5509 - val_acc: 0.7121\n",
            "Epoch 443/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5356 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5490 - acc: 0.7031\n",
            "Epoch 00443: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 5s 310ms/step - loss: 0.5341 - acc: 0.7104 - val_loss: 0.5439 - val_acc: 0.7028\n",
            "Epoch 444/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5326 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5557 - acc: 0.6953\n",
            "Epoch 00444: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5312 - acc: 0.7183 - val_loss: 0.5496 - val_acc: 0.6972\n",
            "Epoch 445/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5264 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5736 - acc: 0.6875\n",
            "Epoch 00445: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5248 - acc: 0.7109 - val_loss: 0.5637 - val_acc: 0.6897\n",
            "Epoch 446/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5287 - acc: 0.7056Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5543 - acc: 0.7012\n",
            "Epoch 00446: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5285 - acc: 0.7094 - val_loss: 0.5479 - val_acc: 0.6991\n",
            "Epoch 447/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5370 - acc: 0.6964Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5796 - acc: 0.6797\n",
            "Epoch 00447: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 382ms/step - loss: 0.5387 - acc: 0.6968 - val_loss: 0.5876 - val_acc: 0.6804\n",
            "Epoch 448/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5463 - acc: 0.7103Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5799 - acc: 0.6777\n",
            "Epoch 00448: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5468 - acc: 0.7113 - val_loss: 0.5897 - val_acc: 0.6804\n",
            "Epoch 449/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5257 - acc: 0.7321Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5564 - acc: 0.6758\n",
            "Epoch 00449: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5263 - acc: 0.7312 - val_loss: 0.5445 - val_acc: 0.6785\n",
            "Epoch 450/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5346 - acc: 0.7219Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5788 - acc: 0.6777\n",
            "Epoch 00450: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 381ms/step - loss: 0.5369 - acc: 0.7192 - val_loss: 0.5861 - val_acc: 0.6766\n",
            "Epoch 451/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5263 - acc: 0.7268Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5587 - acc: 0.7012\n",
            "Epoch 00451: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5288 - acc: 0.7218 - val_loss: 0.5674 - val_acc: 0.6935\n",
            "Epoch 452/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5301 - acc: 0.7270Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5604 - acc: 0.7012\n",
            "Epoch 00452: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 363ms/step - loss: 0.5311 - acc: 0.7224 - val_loss: 0.5668 - val_acc: 0.6916\n",
            "Epoch 453/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5117 - acc: 0.7354Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5677 - acc: 0.7090\n",
            "Epoch 00453: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 359ms/step - loss: 0.5114 - acc: 0.7407 - val_loss: 0.5697 - val_acc: 0.7047\n",
            "Epoch 454/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5491 - acc: 0.7157Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5550 - acc: 0.6953\n",
            "Epoch 00454: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5492 - acc: 0.7149 - val_loss: 0.5633 - val_acc: 0.6897\n",
            "Epoch 455/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5290 - acc: 0.7359Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5878 - acc: 0.7129\n",
            "Epoch 00455: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5284 - acc: 0.7334 - val_loss: 0.5860 - val_acc: 0.7065\n",
            "Epoch 456/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5312 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5545 - acc: 0.7129\n",
            "Epoch 00456: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5336 - acc: 0.7253 - val_loss: 0.5679 - val_acc: 0.7047\n",
            "Epoch 457/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7252Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5579 - acc: 0.7109\n",
            "Epoch 00457: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5270 - acc: 0.7263 - val_loss: 0.5665 - val_acc: 0.7047\n",
            "Epoch 458/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5265 - acc: 0.7337Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5523 - acc: 0.6992\n",
            "Epoch 00458: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 351ms/step - loss: 0.5273 - acc: 0.7293 - val_loss: 0.5589 - val_acc: 0.6935\n",
            "Epoch 459/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5361 - acc: 0.7326Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5674 - acc: 0.6680\n",
            "Epoch 00459: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5366 - acc: 0.7322 - val_loss: 0.5840 - val_acc: 0.6673\n",
            "Epoch 460/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5331 - acc: 0.7225Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.6009 - acc: 0.6875\n",
            "Epoch 00460: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.5314 - acc: 0.7213 - val_loss: 0.6018 - val_acc: 0.6879\n",
            "Epoch 461/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5279 - acc: 0.7300Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5555 - acc: 0.6934\n",
            "Epoch 00461: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5284 - acc: 0.7288 - val_loss: 0.5621 - val_acc: 0.6897\n",
            "Epoch 462/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5235 - acc: 0.7284Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5659 - acc: 0.6914\n",
            "Epoch 00462: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 370ms/step - loss: 0.5234 - acc: 0.7273 - val_loss: 0.5393 - val_acc: 0.6953\n",
            "Epoch 463/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5422 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5486 - acc: 0.7031\n",
            "Epoch 00463: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5419 - acc: 0.7193 - val_loss: 0.5313 - val_acc: 0.7084\n",
            "Epoch 464/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5215 - acc: 0.7401Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5621 - acc: 0.7070\n",
            "Epoch 00464: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5254 - acc: 0.7357 - val_loss: 0.5484 - val_acc: 0.7103\n",
            "Epoch 465/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5348 - acc: 0.7240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5463 - acc: 0.7070\n",
            "Epoch 00465: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 378ms/step - loss: 0.5329 - acc: 0.7192 - val_loss: 0.5273 - val_acc: 0.7140\n",
            "Epoch 466/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5314 - acc: 0.7210Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5498 - acc: 0.6836\n",
            "Epoch 00466: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 386ms/step - loss: 0.5327 - acc: 0.7168 - val_loss: 0.5307 - val_acc: 0.6916\n",
            "Epoch 467/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5176 - acc: 0.7427Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5687 - acc: 0.6797\n",
            "Epoch 00467: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5188 - acc: 0.7427 - val_loss: 0.5551 - val_acc: 0.6841\n",
            "Epoch 468/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5295 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5453 - acc: 0.7129\n",
            "Epoch 00468: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5291 - acc: 0.7238 - val_loss: 0.5303 - val_acc: 0.7178\n",
            "Epoch 469/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5343 - acc: 0.7198Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5595 - acc: 0.6914\n",
            "Epoch 00469: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5324 - acc: 0.7213 - val_loss: 0.5486 - val_acc: 0.6972\n",
            "Epoch 470/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5283 - acc: 0.7302Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5449 - acc: 0.7031\n",
            "Epoch 00470: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 364ms/step - loss: 0.5293 - acc: 0.7314 - val_loss: 0.5384 - val_acc: 0.7065\n",
            "Epoch 471/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5371 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5485 - acc: 0.7012\n",
            "Epoch 00471: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5360 - acc: 0.7218 - val_loss: 0.5365 - val_acc: 0.7047\n",
            "Epoch 472/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5067 - acc: 0.7337Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5492 - acc: 0.6934\n",
            "Epoch 00472: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5088 - acc: 0.7317 - val_loss: 0.5406 - val_acc: 0.6953\n",
            "Epoch 473/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5425 - acc: 0.7215Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5666 - acc: 0.6719\n",
            "Epoch 00473: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 0.5416 - acc: 0.7228 - val_loss: 0.5481 - val_acc: 0.6766\n",
            "Epoch 474/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5186 - acc: 0.7316Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5737 - acc: 0.6797\n",
            "Epoch 00474: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 356ms/step - loss: 0.5198 - acc: 0.7322 - val_loss: 0.5709 - val_acc: 0.6804\n",
            "Epoch 475/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5270 - acc: 0.7347Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5469 - acc: 0.7012\n",
            "Epoch 00475: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 347ms/step - loss: 0.5284 - acc: 0.7331 - val_loss: 0.5473 - val_acc: 0.7009\n",
            "Epoch 476/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5322 - acc: 0.7349Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5466 - acc: 0.6992\n",
            "Epoch 00476: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5309 - acc: 0.7358 - val_loss: 0.5599 - val_acc: 0.6953\n",
            "Epoch 477/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5244 - acc: 0.7273Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5793 - acc: 0.6875\n",
            "Epoch 00477: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 5s 309ms/step - loss: 0.5245 - acc: 0.7268 - val_loss: 0.5869 - val_acc: 0.6841\n",
            "Epoch 478/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5278 - acc: 0.7167Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5512 - acc: 0.6992\n",
            "Epoch 00478: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 352ms/step - loss: 0.5277 - acc: 0.7193 - val_loss: 0.5554 - val_acc: 0.6991\n",
            "Epoch 479/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5213 - acc: 0.7271Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5582 - acc: 0.6914\n",
            "Epoch 00479: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 358ms/step - loss: 0.5194 - acc: 0.7251 - val_loss: 0.5891 - val_acc: 0.6879\n",
            "Epoch 480/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5269 - acc: 0.7316Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5495 - acc: 0.7012\n",
            "Epoch 00480: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 362ms/step - loss: 0.5287 - acc: 0.7308 - val_loss: 0.5536 - val_acc: 0.7009\n",
            "Epoch 481/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5215 - acc: 0.7374Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5519 - acc: 0.6855\n",
            "Epoch 00481: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5250 - acc: 0.7308 - val_loss: 0.5651 - val_acc: 0.6804\n",
            "Epoch 482/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5322 - acc: 0.7157Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5491 - acc: 0.7129\n",
            "Epoch 00482: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 380ms/step - loss: 0.5329 - acc: 0.7184 - val_loss: 0.5794 - val_acc: 0.7065\n",
            "Epoch 483/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5240 - acc: 0.7263Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5449 - acc: 0.7305\n",
            "Epoch 00483: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 375ms/step - loss: 0.5255 - acc: 0.7228 - val_loss: 0.5735 - val_acc: 0.7234\n",
            "Epoch 484/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5167 - acc: 0.7391Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5492 - acc: 0.7305\n",
            "Epoch 00484: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 369ms/step - loss: 0.5159 - acc: 0.7393 - val_loss: 0.5666 - val_acc: 0.7234\n",
            "Epoch 485/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5275 - acc: 0.7141Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 5s - loss: 0.5568 - acc: 0.7051\n",
            "Epoch 00485: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 377ms/step - loss: 0.5262 - acc: 0.7144 - val_loss: 0.5645 - val_acc: 0.7047\n",
            "Epoch 486/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5308 - acc: 0.7162Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5559 - acc: 0.6914\n",
            "Epoch 00486: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 368ms/step - loss: 0.5292 - acc: 0.7163 - val_loss: 0.5813 - val_acc: 0.6860\n",
            "Epoch 487/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5227 - acc: 0.7270Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5389 - acc: 0.7070\n",
            "Epoch 00487: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 366ms/step - loss: 0.5216 - acc: 0.7275 - val_loss: 0.5544 - val_acc: 0.7028\n",
            "Epoch 488/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5275 - acc: 0.7323Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5609 - acc: 0.7090\n",
            "Epoch 00488: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 371ms/step - loss: 0.5249 - acc: 0.7319 - val_loss: 0.5838 - val_acc: 0.7047\n",
            "Epoch 489/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5286 - acc: 0.7188Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5557 - acc: 0.7051\n",
            "Epoch 00489: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 365ms/step - loss: 0.5271 - acc: 0.7183 - val_loss: 0.5777 - val_acc: 0.6935\n",
            "Epoch 490/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5179 - acc: 0.7353Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5570 - acc: 0.7012\n",
            "Epoch 00490: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 360ms/step - loss: 0.5156 - acc: 0.7357 - val_loss: 0.5752 - val_acc: 0.6991\n",
            "Epoch 491/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5457 - acc: 0.7109Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5414 - acc: 0.7109\n",
            "Epoch 00491: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5429 - acc: 0.7149 - val_loss: 0.5718 - val_acc: 0.7009\n",
            "Epoch 492/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5284 - acc: 0.7247Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5559 - acc: 0.7227\n",
            "Epoch 00492: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 354ms/step - loss: 0.5280 - acc: 0.7273 - val_loss: 0.5709 - val_acc: 0.7140\n",
            "Epoch 493/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5232 - acc: 0.7240Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5483 - acc: 0.7148\n",
            "Epoch 00493: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 349ms/step - loss: 0.5256 - acc: 0.7218 - val_loss: 0.5710 - val_acc: 0.7121\n",
            "Epoch 494/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5274 - acc: 0.7406Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 2s - loss: 0.5483 - acc: 0.7051\n",
            "Epoch 00494: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 5s 311ms/step - loss: 0.5272 - acc: 0.7402 - val_loss: 0.5718 - val_acc: 0.7028\n",
            "Epoch 495/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5231 - acc: 0.7332Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5530 - acc: 0.7051\n",
            "Epoch 00495: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 348ms/step - loss: 0.5303 - acc: 0.7293 - val_loss: 0.5718 - val_acc: 0.7028\n",
            "Epoch 496/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5373 - acc: 0.7353Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 3s - loss: 0.5701 - acc: 0.6816\n",
            "Epoch 00496: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.5377 - acc: 0.7342 - val_loss: 0.6008 - val_acc: 0.6748\n",
            "Epoch 497/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5239 - acc: 0.7401Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5553 - acc: 0.7109\n",
            "Epoch 00497: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 372ms/step - loss: 0.5232 - acc: 0.7392 - val_loss: 0.5689 - val_acc: 0.7084\n",
            "Epoch 498/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5294 - acc: 0.7342Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5525 - acc: 0.7148\n",
            "Epoch 00498: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 376ms/step - loss: 0.5330 - acc: 0.7317 - val_loss: 0.5664 - val_acc: 0.7084\n",
            "Epoch 499/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5150 - acc: 0.7333Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5519 - acc: 0.7051\n",
            "Epoch 00499: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 374ms/step - loss: 0.5139 - acc: 0.7337 - val_loss: 0.5772 - val_acc: 0.6991\n",
            "Epoch 500/500\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.5183 - acc: 0.7369Epoch 1/500\n",
            " 4/16 [======>.......................] - ETA: 4s - loss: 0.5539 - acc: 0.7012\n",
            "Epoch 00500: val_loss did not improve from 0.52481\n",
            "16/16 [==============================] - 6s 367ms/step - loss: 0.5209 - acc: 0.7352 - val_loss: 0.5761 - val_acc: 0.6953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKkaSUW8ZJmk",
        "colab_type": "code",
        "outputId": "629cfdab-c719-4b3c-a68c-a7202ddf7e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# History of accuracy and loss\n",
        "tra_loss_3 = history_3.history['loss']\n",
        "tra_acc_3 = history_3.history['acc']\n",
        "val_loss_3 = history_3.history['val_loss']\n",
        "val_acc_3 = history_3.history['val_acc']\n",
        "\n",
        "# Total number of epochs training\n",
        "epochs_3 = range(1, len(tra_acc_3)+1)\n",
        "end_epoch_3 = len(tra_acc_3)\n",
        "\n",
        "# Epoch when reached the validation loss minimum\n",
        "opt_epoch_3 = val_loss_3.index(min(val_loss_3)) + 1\n",
        "\n",
        "# Loss and accuracy on the validation set\n",
        "end_val_loss_3 = val_loss_3[-1]\n",
        "end_val_acc_3 = val_acc_3[-1]\n",
        "opt_val_loss_3 = val_loss_3[opt_epoch_3-1]\n",
        "opt_val_acc_3 = val_acc_3[opt_epoch_3-1]\n",
        "\n",
        "# Loss and accuracy on the test set\n",
        "opt_model_3 = models.load_model('model_3_benmal_best.h5')\n",
        "test_loss_3, test_acc_3 = model_3.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_test_loss_3, opt_test_acc_3 = opt_model_3.evaluate(test_images, test_labels, verbose=False)\n",
        "opt_pred_3 = opt_model_3.predict([test_images, test_labels])\n",
        "pred_classes_3 = np.rint(opt_pred_3)\n",
        "\n",
        "print(\"Model 3\\n\")\n",
        "\n",
        "print(\"Epoch [end]: %d\" % end_epoch_3)\n",
        "print(\"Epoch [opt]: %d\" % opt_epoch_3)\n",
        "print(\"Valid accuracy [end]: %.4f\" % end_val_acc_3)\n",
        "print(\"Valid accuracy [opt]: %.4f\" % opt_val_acc_3)\n",
        "print(\"Test accuracy [end]:  %.4f\" % test_acc_3)\n",
        "print(\"Test accuracy [opt]:  %.4f\" % opt_test_acc_3)\n",
        "print(\"Valid loss [end]: %.4f\" % end_val_loss_3)\n",
        "print(\"Valid loss [opt]: %.4f\" % opt_val_loss_3)\n",
        "print(\"Test loss [end]:  %.4f\" % test_loss_3)\n",
        "print(\"Test loss [opt]:  %.4f\" % opt_test_loss_3)\n",
        "\n",
        "print(classification_report(test_labels, pred_classes_3, digits=4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 3\n",
            "\n",
            "Epoch [end]: 500\n",
            "Epoch [opt]: 419\n",
            "Valid accuracy [end]: 0.6953\n",
            "Valid accuracy [opt]: 0.6897\n",
            "Test accuracy [end]:  0.6577\n",
            "Test accuracy [opt]:  0.6905\n",
            "Valid loss [end]: 0.5761\n",
            "Valid loss [opt]: 0.5248\n",
            "Test loss [end]:  0.6358\n",
            "Test loss [opt]:  0.5918\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7602    0.7671    0.7636       219\n",
            "           1     0.5565    0.5470    0.5517       117\n",
            "\n",
            "    accuracy                         0.6905       336\n",
            "   macro avg     0.6584    0.6571    0.6577       336\n",
            "weighted avg     0.6893    0.6905    0.6898       336\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9LtiOTTZSAI",
        "colab_type": "code",
        "outputId": "12d5fa2c-e9c0-4d47-e4ba-98414d645cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "# Model accuracy\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 3 accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(epochs_3, tra_acc_3, 'r', label='Training set')\n",
        "plt.plot(epochs_3, val_acc_3, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_3, val_acc_3[opt_epoch_3-1], 'go')\n",
        "plt.vlines(opt_epoch_3, min(val_acc_3), opt_val_acc_3, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_acc_3, 1, opt_epoch_3, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Model loss\n",
        "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.title('Model 3 loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(epochs_3, tra_loss_3, 'r', label='Training set')\n",
        "plt.plot(epochs_3, val_loss_3, 'g', label='Validation set')\n",
        "plt.plot(opt_epoch_3, val_loss_3[opt_epoch_3-1], 'go')\n",
        "plt.vlines(opt_epoch_3, min(val_loss_3), opt_val_loss_3, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.hlines(opt_val_loss_3, 1, opt_epoch_3, linestyle=\"dashed\", color='g', linewidth=1)\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAHnCAYAAABHfw/FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHnCAYAAACcxKXnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd1gU1/4G8HdhF0SaChrRBRvB2BE1\nscYScy1RTDTqTSwxsRCT302MacZ4U71JjImJN8WO3sSosUWxpNg1ttiwi6AioAgivS/s/P4YZpht\nFGFhIO/neXjcMjt7ANl3vuecOaMRBEEAERERqY5DdTeAiIiIrGNIExERqRRDmoiISKUY0kRERCrF\nkCYiIlIphjQREZFKMaSJiIhUiiFNVIvt3r0bGo2mzNvv378fGo0GBQUFldaGDz74AL179660/RH9\nnTCkiapRv379oNFosGTJEpPHMzIy4O7uDo1Gg6ioqGpqnaXr16+jV69e8Pb2hoeHB1q1aoWPP/4Y\nRqOxuptGVCsxpImqWdu2bS1C+scff0SzZs2qqUW2NWzYEKGhoUhISEB6ejp27dqFNWvW4Lvvvqvu\nphHVSgxpomo2fPhwJCQk4Pjx4/JjixYtQkhIiMW2O3bsQJcuXeDp6YmAgAB88cUXJlXsqVOn8Mgj\nj8DNzQ1du3bFuXPnLPbxww8/oFOnTvD09ES7du2wbt26MrfV3d0drVu3hqOjIwBAo9HAwcEBERER\nZd5Hamoqpk2bBr1eD29vbwwZMsTk9fv27UPXrl3h6ekJLy8v9OrVCykpKQCA9evXo127dvDw8IC3\ntzcGDhxY5vclqpEEIqo2ffv2Fd59913hvffeEyZNmiQIgiAcOnRI8PPzE65duyYAECIjIwVBEIS/\n/vpL0Ol0ws8//ywYDAbh5MmTgo+Pj/DVV18JgiAIaWlpgre3tzBnzhwhNzdXuHjxotCqVStB+We+\ncuVKwdfXVzhx4oRQWFgoHDp0SHB3dxcOHTokCIIg7Nu3TwAgGAyGEtvdu3dvoU6dOgIAQa/XC5cu\nXbK57fvvvy/06tVLvj9s2DChX79+Qnx8vJCVlSW8+uqrgl6vFzIyMgRBEIQmTZoIoaGhgtFoFPLy\n8oQjR44ImZmZQlZWlqDT6YQ9e/YIgiAIOTk58m2i2oqVNJEKTJ06FZs2bUJqaioWLVqEqVOnwsHB\n9M9z+fLleOKJJzBmzBhotVp06dIFb775JhYvXgwA2LZtGxwcHPDBBx/A2dkZbdu2xauvvmqyjwUL\nFuDdd99F165d4eDggN69e2Ps2LFYtWpVudp76NAhZGZm4vDhw5gwYQIaNWpUptfFx8dj+/bt+Prr\nr9G4cWPUrVsX8+fPR05ODrZv3w4AcHJywrVr13D79m04OTmhR48ecHV1BQDodDpcvnwZSUlJqFOn\nDgYMGFCudhPVNAxpIhXQ6/Xo378/vvjiC2zduhWTJ0+22CY2NhatWrUyeczf3x8xMTEAgLi4OPj6\n+spd0QDQokULk+0jIyPx+uuvo169evLX2rVrcfv27XK32dHRET179kS9evUwbdq0Mr0mNjYWAEy+\nD51Oh2bNmsnfR1hYGK5fv44uXbrA398f77//PgoKClC3bl389ttv2L17N1q3bo0OHTpg4cKF5W43\nUU2ire4GEJFo+vTpGDp0KEaNGgUfHx9ER0ebPO/r64tr166ZPHbt2jX4+fkBEIM+NjYWhYWFclCb\n76Nx48b48MMPMXHixEprt8FgKPOYtK+vr9zuTp06AQAKCgoQExMjfx8dOnTAmjVrAADh4eEYNGgQ\n9Ho9pk6dij59+qBPnz4QBAEHDhzA4MGD0bZtWzz++OOV9v0QqQkraSKVGDRoEHbt2oWvvvrK6vMv\nvPACduzYgU2bNqGwsBBnzpzB/Pnz5Sp22LBhKCwsxEcffYS8vDxcuXLFotKcMWMGPv74Y5w4cQJG\noxF5eXk4ceIETp06VaY27tq1C0eOHEFeXh4KCgqwb98+LFy4EEOHDi3T6318fDB06FC8/vrrSEhI\nQE5ODt5++204OTnhiSeeQH5+PlauXIm7d+8CADw9PeHo6AitVos7d+5gw4YNSE1NhUajQb169aDR\naKDVstagWqy6B8WJ/s6kiWPW3Lhxw2TimCAIwtatW4XOnTsL7u7uQqtWrYTPPvtMKCgokJ8/fvy4\n0LVrV8HV1VXo0qWL8OWXXwrmf+arV68WgoKCBE9PT8HLy0vo27evcODAAUEQSp84tmnTJqFDhw6C\nq6ur4OHhIbRp00b4+OOPS5xoZj5x7N69e8LkyZOFJk2aCA0aNBAGDRokTzzLy8sThg4dKjRs2FCo\nW7eu4OvrK7zzzjtCYWGhcPv2bWHAgAFC/fr1BVdXV6FVq1bCl19+WcpPmKhm0wiCIFTzcQIRERFZ\nwe5uIiIilWJIExERqRRDmoiISKUY0kRERCrFkCYiIlKpGn+CobOzMxo2bFjdzSAiIrovd+/eRV5e\nntXnanxIN2zYEHFxcdXdDCIiovui1+ttPsfubiIiIpViSBMREakUQ5qIiEilavyYNBFRbWc0GsEV\nnGsujUZjcX34smJIExGpVH5+PmJiYmAwGKq7KVRBOp0Ofn5+cHJyKtfrGNJERCoVExMDd3d3eHl5\nQaPRVHdz6D4JgoB79+4hJiYG/v7+5XotQ5qISIWMRiMMBgO8vLx4zexawMvLC8nJyTAajeXq+ubE\nMSIiFZLGoFlB1w7S77G8cwsY0kRERCrFkCYiolIFBgYiMDAQbdu2haOjo3x/7Nix5d7X888/j0OH\nDpW63XfffYevvvrqfppbaYxGIz744APk5+dXy/trhBo+r1+v13NZUCKqdQoLC3H16lUEBATA0dGx\nupsji46ORmBgIFJTU21uU1BQUGvG0QsKCqDT6ZCRkQE3N7f73k9Jv8+Scqx2/BSJiGq74GDg2jX7\n7LtVKyAs7L5fvnv3bsycORNBQUEIDw/He++9h6ysLHzzzTcwGAwQBAGffPIJhg4dCgDo3bs3Zs2a\nhWHDhmH8+PFwd3dHREQE4uLi0KlTJ6xZswY6nQ5z5sxBbm4uvvjiCyxfvhwbNmxA/fr1cfHiRbi4\nuGD9+vVo3rw5AGDOnDlYt24d6tevj3/84x/4+eefERUVZdHWJUuWYOHChXBycoLRaERoaCi6du2K\niIgIzJgxA0lJScjLy8P06dMxffp0vPjiiwCAnj17wsHBAXv27IGXl9d9/6zKiyFNREQVduHCBXz/\n/ffo3bs3ACApKQnjx4+HRqPB9evX0bNnT8TGxkKn01m89uzZs9izZw+cnJzQq1cvbNmyBaNHj7bY\n7vjx4zh79iyaNWuGN954A/Pnz8d3332HrVu3Ytu2bQgPD4erqysmTpxos50zZ87EjRs30KhRIxgM\nBuTl5cFgMODZZ5/F2rVrERAQgKysLDz88MN45JFHsHjxYqxYsQJHjhypUCV9vxjSREQ1QQUq3aoQ\nEBAgBzQAXL9+HePGjcOtW7eg1WqRnJyMmzdvWj1PeOTIkXBxcQEAdOvWDdds9Bj07t0bzZo1AwD0\n6NEDy5YtAwDs2bMHY8aMkUN08uTJOHr0qNV9PPbYYxg/fjyGDRuGoUOHwt/fH+fOncPly5cxZswY\nebvs7GxcunQJHTt2vI+fRuVhSBMRUYWZV5ljxozB119/jSeffBIA4OHhgdzcXKuvrVOnjnzb0dER\nBQUFFdqupNPWtm7dipMnT2L//v34xz/+gXnz5iEgIADe3t4IDw+32N7We1QVzu4mIqJKl5qaihYt\nWgAAVq1ahYyMDLu914ABA7Bx40ZkZWVBEASEhoZa3c5gMOD69evo1q0b3nzzTYwcORInTpxA27Zt\n4eLigh9//FHeNjIyEqmpqdBqtahbty7S0tLs1v6SMKQlCQnA+PHAunXV3RIiohpv4cKFGDFiBIKC\ngnDp0iU0bdrUbu/15JNPYsiQIQgMDES3bt3g5eWFevXqWWxnMBgwadIkdOjQAYGBgTh37hxmzJgB\nnU6H7du34+eff0bHjh3Rrl07TJ06FTk5OQCA119/Hf3790dgYCDu3btnt+/DGp6CJbl2DfD3B956\nC5g3r+L7IyKqALWegqVWGRkZcHd3hyAIePXVVyEIAr755pvqbpaMp2BVlDSGUbOPWYiI/pbGjRuH\n2NhY5ObmokOHDli8eHF1N6lSMKQlXB+XiKjGClP57Pf7xTFpc6ykiYhIJRjSEnZ3ExGRyjCkJQxp\nIiJSGYa0hGPSRESkMgxpc6ykiYgsDB06FN9++63F4506dcLmzZtLfO0HH3yAGTNmABAneL322mtW\nt7tw4YJ8wYySREdHW8zeHjp0KCIiIkp9rT2tWrUKV65cqdR9MqQl7O4mIrJp8uTJWLlypcljJ0+e\nRHx8PIYPH17m/QQHB1f4GtHWQnrnzp1o3bp1hfZbUQxpe2JIExHZFBwcjNjYWJw7d05+LDQ0FBMn\nToROp8P58+fRu3dvBAUFoW3btpg7d67V/axatUpezxsQq+wHH3wQXbp0wTrFio8FBQUYNGgQunbt\ninbt2uHZZ59FVlYWAODFF19EREQEAgMDERwcDABo3ry5vPZ2VFQUBg4ciI4dOyIwMBBbtmyR96vR\naPDJJ5/g4YcfRosWLSwOPCTHjh1Dly5dEBgYiPbt22PRokUAxEVTpk6diocffhgdO3bEtGnTkJ+f\nj+XLl+PkyZN47bXXEBgYiJ07d97Pj9kCz5OWcEyaiFQseG0wrqXY53rSreq3QtgzJZ9nrNPpMGHC\nBISGhuLrr79Gbm4u1q5diyNHjgAQQ3LPnj1wdnZGTk4OevbsiYEDB6J79+4297ljxw5s2LABp06d\ngru7OyZMmCA/5+joiDVr1sDLywuCIOCll17CN998g1mzZmHx4sWYMWOG1QtiAOLCJi+88AJCQkIQ\nGRmJ7t27o3PnzvIVtJydnfHXX3/hypUr6NatGyZMmACt1jQOP/30U7zxxht45plnAAApKSkAxCVC\n+/Tpg2XLlkEQBEydOhULFy7Em2++idWrV2PGjBkmByEVxZA2x0qaiMiqyZMno2/fvvj888+xefNm\ntGnTBm3atAEA5OTk4KWXXkJ4eDgcHBwQGxuL8PDwEkNausSkh4cHACAkJAR//vknAEAQBHz11VfY\nsWMHCgoKkJaWhp49e5baxoyMDJw+fRqHDx8GADz44IPo3bs3Dh06JIf0uHHjAAAPPfQQtFot7ty5\nA71eb7Kf/v374+OPP0ZkZCQGDBggX4Zzy5YtOHr0KBYsWCB/3/ZctpUhLWF3NxGpWGmVblVo27Yt\n/P39sW3bNoSGhmLy5Mnyc7Nnz4a3tzfOnDkDrVaLkSNH2rw0pS3KS0yuWbMGe/fuxYEDB+Dh4YH/\n/ve/2Lt373212/zSlWW55OWMGTMwYsQI7N69G7Nnz0b79u3x/fffQxAEbNq0CQEBAffVlvLimLSE\nIU1EVKrJkyfjk08+wV9//YWxY8fKj6ekpECv10Or1SIiIgK7du0qdV8DBw7Ehg0bkJGRAUEQsHTp\nUpP9eXt7w8PDAxkZGVi1apX8nIeHh81LR7q7uyMoKEgea46KisKff/6JRx99tFzfZ0REBFq0aIGp\nU6di9uzZOHbsGADxilvz5s2Tgz0lJQVRUVGltut+MaQlDGkiolKNHTsWERERGD16NNzc3OTH58yZ\ng5UrV6Jjx46YNWsWBgwYUOq+hg4diqeffhpBQUHo2rUr/Pz85OcmTpyI7OxstG7dGkOGDEGfPn3k\n56TLSbZv316eOKb0008/4eeff0anTp3w9NNPY/ny5Sb7Lotvv/0W7dq1Q+fOnTFnzhx8+eWXAICv\nvvoKLi4uCAwMRMeOHfHYY48hOjoaADBt2jR88sknlTpxjJeqlCQkAI0bAy+/DFg5F5CIqCrxUpW1\ny/1eqpKVtLmafcxCRES1CENawu5uIiJSGYa0hCFNREQqw5CWcDETIlIR6bShGj5tiIpIv0fz08FK\nw/OkzfEPgohUwMHBATqdDvfu3YOXl1e5P9xJPQRBwL1796DT6eDgUL7a2O4h/corryAsLAw3b97E\nmTNnEBgYaLFNdHQ0Jk2ahDNnzqBFixY2l3qzK3Z3E5HK+Pn5ISYmBsnJydXdFKognU5X7tPAgCoI\n6aeffhpvvfWWvKSaNR4eHpg7dy7S0tLw7rvv2rtJ1jGkiUhlnJyc4O/vD6PRyG7vGkyj0ZS7gpbY\nPaTLsspLgwYN0Lt3b+zfv9/ezbGNXUlEpFL3+wFPNR9/8+Z4tEpERCpR40J6wYIF0Ov18ldmZmbl\n7Jjd3UREpDI1LqRnzpyJuLg4+Uu5dmyFMKSJiEhlalxI2w3HpImISGXsHtIhISHy4uGDBg2Cv78/\nAGDKlCkICxOvj5qdnQ29Xo/Ro0fj0qVL0Ov1eOedd+zdNOtYSRMRkUrYfXb3kiVLrD6+fPly+Xbd\nunUr50pWFcHubiIiUhl2d0sY0kREpDIMaQlDmoiIVIYhLeHEMSIiUhmGtDlW0kREpBIMaQm7u4mI\nSGUY0hKGNBERqQxDWsIxaSIiUhmGtDlW0kREpBIMaQm7u4mISGUY0hKGNBERqQxDWsIxaSIiUhmG\ntDlW0kREpBIMaQm7u4mISGUY0hKGNBERqQxDWsKQJiIilWFISzhxjIiIVIYhbY6VNBERqQRD2hxD\nmoiIVIIhraTRMKSJiEg1GNJKHJcmIiIVYUibYyVNREQqwZBWYnc3ERGpCENaiSFNREQqwpBW4pg0\nERGpCEPaHCtpIiJSCYa0Eru7iYhIRRjSSgxpIiJSEYa0EkOaiIhUhCGtxIljRESkIgxpc6ykiYhI\nJRjSSuzuJiIiFWFIKzGkiYhIRRjSShyTJiIiFWFIm2MlTUREKsGQVmJ3NxERqQhDWokhTUREKsKQ\nVuKYNBERqQhD2hwraSIiUgmGtBK7u4mISEUY0koMaSIiUhGGtBJDmoiIVIQhTUREpFIMaSVW0kRE\npCIMaSWGNBERqQhDWokhTUREKsKQVuJiJkREpCIMaXOspImISCUY0krs7iYiIhVhSCsxpImISEUY\n0kockyYiIhVhSJtjJU1ERCrBkFZidzcREamI3UP6lVdeQfPmzaHRaBAeHm5zuxUrVuDBBx9Eq1at\nMHXqVBgMBns3zRJDmoiIVMTuIf3000/jzz//RLNmzWxuc+PGDfz73//GoUOHEBUVhYSEBCxdutTe\nTbPEMWkiIlIRu4f0o48+Cr1eX+I2GzduRHBwMBo3bgyNRoMXX3wRa9eutXfTrGMlTUREKqGKMemY\nmBiTSrt58+aIiYmp+oawu5uIiFREFSFdHgsWLIBer5e/MjMzK2/nDGkiIlIRVYS0n58fbt68Kd+P\njo6Gn5+f1W1nzpyJuLg4+cvNza3yGsKQJiIiFVFFSI8aNQphYWG4c+cOBEHA4sWL8c9//rPqG8KJ\nY0REpCJ2D+mQkBDo9XrExcVh0KBB8Pf3BwBMmTIFYWFhAICWLVviww8/RK9eveDv74+GDRsiJCTE\n3k2zjpU0ERGphEYQanYqSQcAlaJlS6BePeD06crZHxERUSlKyjFVdHerBsekiYhIRRjSShyTJiIi\nFWFIm2MlTUREKsGQVmJ3NxERqQhDWokhTUREKsKQVuKYNBERqQhD2hwraSIiUgmGtBK7u4mISEUY\n0koMaSIiUhGGtBJDmoiIVIQhrcSJY0REpCIMaXOspImISCUY0krs7iYiIhVhSCsxpImISEUY0koc\nkyYiIhVhSJtjJU1ERCrBkFZidzcREakIQ1qJIU1ERCrCkFbimDQREakIQ9ocK2kiIlIJhrQSu7uJ\niEhFGNJKDGkiIlIRhrQSQ5qIiFSEIa3EiWNERKQiDGlzrKSJiEglGNJK7O4mIiIVYUgrMaSJiEhF\nGNJKHJMmIiIVYUibYyVNREQqwZBWYnc3ERGpCENaiSFNREQqwpBW4pg0ERGpCEPaHCtpIiJSCYa0\nEru7iYhIRRjSSgxpIiJSEYa0EsekiYhIRRjS5lhJExGRSjCkldjdTUREKsKQVmJIExGRijCklRjS\nRESkIgxpJU4cIyIiFWFIm2MlTUREKsGQVmJ3NxERqQhDWokhTUREKsKQVuKYNBERqQhD2hwraSIi\nUgmGtBK7u4mISEUY0koMaSIiUhGGtBLHpImISEUY0uZYSRMRkUowpJXY3U1ERCrCkFZiSBMRkYrY\nPaQjIyPRs2dPBAQEoFu3brh48aLFNkajEW+88Qbat2+Phx56CJMnT0Z+fr69m2aJIU1ERCpi95AO\nCQnBtGnTcPXqVbz99tuYNGmSxTYrVqzA6dOncfr0aVy+fBkODg5YuHChvZtmiRPHiIhIRewa0omJ\niTh58iTGjx8PABg1ahRiY2MRFRVlst3Zs2cxcOBAODk5QaPRYMiQIfjxxx/t2TTbWEkTEZFK2DWk\nY2Nj4ePjA61WCwDQaDTw8/NDTEyMyXZdunRBWFgY0tPTYTAYsH79ekRHR1vd54IFC6DX6+WvzMzM\nymswu7uJiEhFVDFxbNKkSRg8eDD69u2Lvn37IiAgQA52czNnzkRcXJz85ebmVnkNYUgTEZGK2DWk\nfX19ER8fj4KCAgCAIAiIiYmBn5+fyXYajQYffPABzpw5gyNHjqBt27Zo166dPZtmHcekiYhIRewa\n0o0aNUJQUBBWr14NANi0aRP0ej38/f1NtsvNzUVKSgoAICkpCZ999hneeustezbNNlbSRESkEtb7\nlCvRkiVLMGnSJHzyySfw8PDAypUrAQBTpkxBcHAwgoODkZaWhn79+sHBwQFGoxGvvvoqhg8fbu+m\nWWJ3NxERqYhGEGp2Kun1esTFxVXOzp56CggLAwoLK2d/REREpSgpx1QxcUw1OCZNREQqwpA2V7M7\nFoiIqBZhSCtxTJqIiFSEIa3E7m4iIlIRhrSSFNKspomISAUY0kqspImISEUY0tawkiYiIhVgSCux\nu5uIiFSEIa3EkCYiIhVhSCtxTJqIiFSEIW0NK2kiIlIBhrQSu7uJiEhFGNJKDGkiIlIRhrQSx6SJ\niEhFGNLWsJImIiIVYEgrsbubiIhUhCGtxJAmIiIVYUgrMaSJiEhFGNJEREQqxZBWYiVNREQqwpBW\nYkgTEZGKMKSVGNJERKQiDGklLmZCREQqwpC2hpU0ERGpAENaid3dRESkIgxpJYY0ERGpCENaiWPS\nRESkIgxpa1hJExGRCjCkldjdTUREKsKQVmJIExGRijCklTgmTUREKsKQtoaVNBERqUCZQ/q9995D\namoqBEHAE088AW9vb2zatMmebat67O4mIiIVKXNIb926FfXq1cPu3buh1Wpx+PBhzJ07155tq3oM\naSIiUpEyh7SDg7jpgQMHMHr0aLRu3Rqa2jaGy5AmIiIV0ZZ1Q1dXV8ybNw/r1q3D4cOHIQgC8vPz\n7dm2qlfbDjqIiKhGK3MlvWrVKsTHx+Pzzz/HAw88gGvXrmH8+PH2bFv1YSVNREQqUOZK2t/fH19/\n/TUAIC0tDbm5uZg1a5bdGlYt2N1NREQqUuZKevDgwUhNTUVmZiY6deqEYcOG4b333rNn26oeQ5qI\niFSkzCGdkJCAevXqYefOnRgxYgQiIyPxyy+/2LNtVY9j0kREpCJlDmmDwQAAOHjwIB5//HHodDpo\ntWXuLa9ZWEkTEZEKlDmk27dvjyFDhmD79u0YMGAAsrOz7dmu6sHubiIiUpEyl8KrVq3Cb7/9hk6d\nOqFu3bq4desWPv30U3u2reoxpImISEXKXEnXqVMHXbp0wdGjR7FmzRoIgoDBgwfbs21Vj2PSRESk\nIuVaFrRz587YsGEDNmzYgKCgIGzbts2ebas+rKSJiEgFytzd/eGHH+LYsWPw9/cHAERFRWHMmDEY\nPny43RpX5djdTUREKlLmSrqwsFAOaEBc3MRoNNqlUdWGIU1ERCpS5pBu1KgRli9fDqPRCKPRiBUr\nVqBhw4b2bFvVY0gTEZGKlDmkFy9ejOXLl8PFxQUuLi5Yvnw5PvzwQ3u2repx4hgREalImcekW7Vq\nhWPHjiEzMxMA4ObmBj8/P8TExNitcdWGlTQREalAuZcMc3Nzk28LtS3M2N1NREQqUububms0ta17\nmCFNREQqUmolfe7cOZvPSet5lyQyMhLPPfcckpKS4OnpiVWrVqFdu3Ym2xiNRrzxxhv47bffoNVq\n4eXlhWXLlpnMJq8Ste2gg4iIarRSQ3rEiBE2n3NxcSn1DUJCQjBt2jRMmjQJGzduxKRJk3DixAmT\nbcLCwnD48GGcPXsWOp0Oc+fOxezZs7F+/foyfAt2wEqaiIhUoNSQvnHjxn3vPDExESdPnsQff/wB\nABg1ahT+7//+D1FRUSZVskajQV5eHnJzc6HVapGeng69Xn/f73vf2N1NREQqYtdrTcbGxsLHx0e+\npKVGo5FnhCtDevjw4di3bx8aN24Md3d3NG3aFAcOHLC6zwULFmDBggXyfWm2eaVgSBMRkYpUaOJY\nZTl58iQuXLiAW7du4fbt23jsscfw4osvWt125syZiIuLk7+Us80rjGPSRESkInYNaV9fX8THx6Og\noACAeMpWTEwM/Pz8TLb74YcfMGDAANSrVw8ODg547rnnsG/fPns2rWSspImISAXsGtKNGjVCUFAQ\nVq9eDQDYtGkT9Hq9xaztli1bYu/evcjPzwcAbN++He3bt7dn06xjdzcREamIXcekAWDJkiWYNGkS\nPvnkE3h4eGDlypUAgClTpiA4OBjBwcF4+eWXcfnyZXTq1Ak6nQ6NGzfG4sWL7d00SwxpIiJSEbuH\ndOvWrXH06FGLx5cvXy7fdnZ2xrJly+zdlNIxpImISEVUMXGMiIiILDGklVhJExGRijCklRjSRESk\nIgxpJYY0ERGpCENaiYuZEBGRijCkrWElTUREKsCQVmJ3NxERqQhDWokhTUREKsKQVuKYNBERqQhD\n2hpW0kREpAIMaSV2dxMRkYowpJUY0kREpCIMaSWGNBERqQhDmoiISKUY0kqspImISEUY0koMaSIi\nUhGGtBJDmoiIVIQhrcTFTIiISEUY0tawkiYiIhVgSCuxu5uIiFSEIa3EkCYiIhVhSCtxTJqIiFSE\nIW0NK2kiIlIBhrQSu7uJiJ15hSgAACAASURBVEhFGNJKDGkiIlIRhrQSx6SJiEhFGNLWsJImIiIV\nYEgrsbubiIhUhCGtxJAmIiIVYUgrMaSJiEhFGNJKnDhGREQqwpC2hpU0ERGpAENaid3dRESkIgxp\nJYY0ERGpCENaiWPSRESkIgxpa1hJExGRCjCkldjdTUREKsKQVmJIExGRijCklTgmTUREKsKQtoaV\nNBERqQBDWond3UREpCIMaSWGNBERqQhDWokhTUREKsKQVuLEMSIiUhGGtDWspImISAUY0krs7iYi\nIhXRVncD1EIQBOQI+YAOqMuQJiIiFWAlXeTqvatwvTEF/+lT3S0hIiISMaSLeDh7AADS6oDd3URE\npAoM6SKedTwBAOnOqFBIP7vpWTz4zYOV1CoiIvo745h0ERetC7RwRJpzIWA03vd+1l5YW4mtIiKi\nvzNW0kU0Gg08ta5id3dycnU3h4iIyP4hHRkZiZ49eyIgIADdunXDxYsXLbZZuXIlAgMD5S9vb2+M\nHDnS3k2z4OnsgTRnALdvV/l7ExERmbN7SIeEhGDatGm4evUq3n77bUyaNMlim+effx7h4eHyV+PG\njTFu3Dh7N82Cp0t9sZKOj6/y9yYiIjJn15BOTEzEyZMnMX78eADAqFGjEBsbi6ioKJuvOX78OBIT\nExEcHGzPplnl6epVaZW0wBniRERUQXYN6djYWPj4+ECrFeenaTQa+Pn5ISYmxuZrVqxYgQkTJkCn\n01l9fsGCBdDr9fJXZmZmpbXXs44n0uoAwu1bFs8JgoAfz/6I9Lz0Mu3LKNz/5DMiIiJAZRPHsrKy\nsG7dOkyePNnmNjNnzkRcXJz85ebmVmnv71nHE4UOQE6iZSW99sJaTNwyEZO2TCrTvgqFwkprFxER\n/T3ZNaR9fX0RHx+PgoICAGI1GhMTAz8/P6vbb9iwAe3atUPbtm3t2SybPJ3Fc6XT0hKAojZLolOj\nAQDnEs6VaV8FxoLSNyIiIiqBXUO6UaNGCAoKwurVqwEAmzZtgl6vh7+/v9XtV6xYUWIVbW9ySDsD\nuHPH5DkpdB0dHMu0L4Y0ERFVlN27u5csWYIlS5YgICAAn332GVauXAkAmDJlCsLCwuTtIiIiEB4e\njrFjx9q7STbJS4M6AzAbNy80it3XWoeyrf/CkCYiooqy+4pjrVu3xtGjRy0eX758ucV2GRkZ9m5O\niaSlQec+CoRFR0PTs6f8nFxJa2xX0soZ3QxpIiKqKFVNHKtuLeu3BABsbw0ciN5v8pzBaABQciWt\nDGaGNBERVRRDWuGxFo9h25AfAACr0g+YPCedelXSmHR+Yb58myFNREQVxZBW0Gg0GNZtPLrGa/CL\n7hoK8nPl59Ly0gCU3N3NkCYiosrEkDan0WBwnAvStYU4/dYE+eG0XDGk8wrzbL5UGdLSRDMiIqL7\nxZC2on9HcUnSfbEH5cekSjorP8vm61hJExFRZWJIW9Hjw1C4FjhgXsBdnL1zFkBxJZ2Zb3sZUoY0\nERFVJoa0FS46F/wU0w0pdQSsPBOKguQkJGYmAACyDKykiYioajCkbRju3gWu+cD5i/swbE5L3M1J\nAiBW0oIgYOuVrbiZetPkNQxpIiKqTHZfzKSmcmiqR4drwCnjeaQ9UPy4UTDiZtpNPPnzk3B3ckf6\nO8VXxWJIExFRZWIlbUvTpuiQCKTVEe/qCoH2dzUAgGsblwIAMvJNV0hjSBMRUWViSNtSvz4CFdfY\n2JwTjEGR4rKfUYs/tfoS5elZDGkiIqoohrQt3btj/E1P6IpOd3445CO4NmgMAIhqULyZodAg3zY5\nT5rXkyYiogpiSNvSsCE8ElJx5/mLONphIRr5d4Lbs88BAK56FW92K+OWfJvd3UREVJkY0qVo0KIt\nuo98BQDQslU3AMD+5sXPR6dGy7cZ0kREVJkY0uXQtUlXAEB6neLHGNJERGQvDOly8PP0g7eju8lj\nSdlJ8m2GNBERVSaGdDloNBp0dW8NAHCDEwAgOS5Sfp4hTURElYkhXU7/afcK3tsPbP2fGMjJu7YC\nJ04AV64wpImIqFJxxbFyCmrWHUH7gYKiw5t4JwPS+jwMzzwg/8iX8nYMaSIiqihW0uXl6QkA0BoB\nz1wgrFEy6r0DGBx4PWkiIqpcDOnyKgppAGiQU/zwiaYckyYiosrFkC4vZ2fAxQWAaUjvacGQJiKi\nysWQvh9F1bSDUPzQwWZAVNJV+T5DmoiIKoohfT9atQIApDkXP3TbHdgfvV++z5AmIqKKYkjfj7Fj\nAQDpipC+1AhIyLmLPn59ADCkiYio4hjS96MopN3yLZ/q7dsLgFlIp6QAublV0TIiIqpFGNL3o1Ej\nICsLYa6T8fJfwDPni59q/pc4Li2HtCAADRoAvXrJ2wiCgC1XtiC3gMFNRES2MaTvV926aPPUNHyb\n9DB8corXhPHdvBuA4nrSOUVTwE+flrdZd2Ednvr5KUzfMb3KmktERDUPQ7oiHn4YOH4cDZ4eLz/U\nNC4dgKKSzsy0eNmN1BsAgEM3D9ncdVJ2EvZc31OJjSUiopqGIV0JvNwfkG+754n/FiTEA+3aAefO\nWWzv7CjOOCupu7t3aG8M/HEgIu9F2tyGiIhqN67dXQm8XBvKt7VG8d+CA/uASzeB11+32N5ZK4Z0\nXmGezX1G3IsAAKTmplZiS4mIqCZhJV0JvPw7yLflkNYW/WgTEy221zqIx0Z5BbZD2nxbIiL6+2FI\nVwJXl+L1vOWQvimOO+POHYvtpW7ukippiUajqXgDiYioRmJIVwJfT18AwOiHRskhHdUAGPYskOiq\n2NAoPpltyAZguta3LYZCQ6W2lYiIag72pVaCJu5NEPdaHBq6NkTuc+J48y5x5VB80gf4+reiDXNy\nAFdX5BSFtDXLTi2D3kMv3y9LkBMRUe3EkK4kTT2aAgAKjKaPFyp7qzMzxZC+eNZ0G2MhHB0cIQgC\npm2fZvIcQ5qsiUuPMzmYI6Laid3dlUy7PNTkvqPiSlnSOdPZibfkh47HHYf2Yy0+PfQp7mbftdgf\nQ5rMhZ4Jhe9XvthwcUN1N4WI7IwhXcm0E54zua+8UpYU0jlC8Thz9xXdAQCz985GXHqcxf4Y0jVX\nam4qBEEofcNy2nBJDOedUTsrfd9EpC4M6UrmoHGABsV93JvbAH8UjU9j9Wpg8mTk5KRbvM6/gT9i\n02ItHmdI10x7ru9B/Xn1sTJ8ZaXvW1rNzlHjWOn7JiJ1YUjbgYOm+MeaXgcYNAEwagB8+SWE0FDk\n3LEMY40AxKXFWDzOkK6ZNl/eDABYFb6q0vddaBTXhec59ES1H0PaDuSLayhENQA+6S3A9V0g0UXs\nAl2zCYj7wRud7gDxyTcRu+Iri9cxpGsmg1Ec0tA56ip936ykif4+GNJ20LVJV4vHTjQB3n0MyNEB\nx3yBBzLFS1w2vZ6EjglApsaAyxk3LF5nvuCJPcY4qfLJIe1QHNJ7b+zFqPWjKnzgJR0EspImqv0Y\n0nZwfMpxGP5tQBefLvJjJ5oCjbOLKx8XXV35tk+G+G/YQ5b7ys9Mk28PXzscD37zoM33HbtxLJ7f\n+jyDvBIdjT2K0RtGl2kJVyUpiJWV9GM/PIbNlzfj5O2TFWqTVEkzpIlqP4a0HThoHKB10GLbM9uw\nczXQMAvY3xzIq+skb+PSuPgcV5/6vvLtt/403Vf+taviSmXnzmH71e24lnIN8RnxFu8pCALWX1yP\nVeGr8NP5n8QHQ0OBfftKbuytW8DcuUChZRc9AT1De2LjpY34NerXcr1OWilO56DD/+38P8z7c578\nnHwZ0/skjUlbG1YhotqFIW1HPu4+GPLKf/G4awecbQykIEd+TqPVAmPHAo0awb21eIGOFinA4CjT\nfeTHRgNffgl06iQ/tvv6bov3ysjPkG+HRYQB+fnA5MnAgAElN3LMGODf/xZnnlOlUY5Jf3fiO8za\nM0t+7mLiRaTnWc7wLyspnHMMOaVsSUQ1HUPa3v71Lwx68g2Lh+9m3QV++gmIicHoJ97CK8eAP4Xn\n4WX2uZsfdxP45ReTxw7c3A8A+CPqd+z74DkYDh/C0ZjD8vNnE84CNyzHt62KKzo3+9atkrejcpG6\nx42C0eK5l3a+hA6LOlg8XlZSJZ5dYHt5WSKqHRjSVWDog0Pl2x0aiR/Od7PvAo6OgLMzPLr3xcLN\n2Wjy5kfwSzN9bX58HOAkdpM7FfWS3o2+BBQUYNBPgzFA8wNmffQoBq8pfo/Ie5HIvGy69KhNbm7i\nv1lZ9/fN/U2UZZw/vzAfx+OOA4BcKdu6HniMldPtykrqSmclTVT7MaSrgHddbzg5ikEb2DjQ+kYu\nLkDjxqiXZ3ppyvz8HODQIRRqgPyieUIZp48BffvK2/zUsXj79gmAAAHn/zW2+MHcXNuNcy26TFe8\n5Tg3lc//7fw/dF/RHb9H/S6Hc2KW5fXEKyrLIB5Q5RQUh/TSU0sRtCSIV00jqmUY0lXk1LRTCG4d\njJAuIbY30mqBXr3grShq8x0BGI3IKp5zhgwnQDhyRL7fULG9NKZ9trFiv4mJgCAAQUFAjx6ARgNc\nuSI+Zyj6UL95U5ygtm2bOJ5NJgSUXklvv7odAHDp7iWk5YldIvYI6Yw8cf6BspIO2R6CM3fO4Gba\nzUp/PyKqPgzpKtK+UXts/edWBPkEAQAmd55sfcP9+zH6oZHy3fyis7ayJz4jP5apb4Tc92fL95Xj\n2IOuif9OHwb88+miB7/5BujXDzhzBjh2DACQu3qV+CF/7564TXQ08O67QHAwsHTp/X6blp5+Gmje\nvPL2V01yDDkYvWE0nljzBG5n3La6jTT+7OjgaLdKWhAEeZJgTkEOwiLCcCHxgkk7JaM3jMakLZMq\n9f2JqGoxpKuYi84Fhn8bsGz4MusbODpi4XPr8M2QbwAA+Q0bAI88gqzX/yVvkuGmQ/pTxWPQ9zq0\nkm8HpteFR1Hv9s/tix784gvg4EGTt2mg/RL159UH7hZdeSsmBvjsM/H2nTtWm5ZtyMbSU0vlU4DK\nZNMmsUqv4S4nXcbGSxuxM3Kn1dn1QPGsa6NgRGZ+pnzblvKcinU+4TwEQUBuQa68z7N3zmLEuhEY\n8L/iGfxpeWlIzknGohOLcPDmwXKfOka129JTS3Em/kx1N4PKgSFdDbQOWmg0GpvP6xx1cqWdH/wE\ncOwYsuq5ys9n5GcgQ99Ivn8jtzhUG7Rqj/Q6xfsy2PgN52gKxNXMpPHqAkVg/Oc/wPDhQGws8NRT\nwG2xcpwcNhkh20MQeibUyh5LkVOzJzlJXcwArJ6nDhQHckpOSpn2mZabVvpGANacX4OOizvim7++\nMTnVThqTVl7iNCUnBeM2j8NLO19CYlYiErMSK3S6F9Ue2YZshGwPQdDSoOpuCpWD3UM6MjISPXv2\nREBAALp164aLFy9a3e78+fPo168f2rRpgzZt2mDz5s32bpqqSRPNpJWrsg3Fp9tk5mciTfHBm2XI\nQhP3Jtj33D44dOyEF04X7ydDeanM8ti+HRg5EtiyBZg9G2jSBHvPbgGAUj/0z8SfQeS9SNMHU63P\ncq4p0vOLv+f4zJJD+mry1TLt09bMb3N/XPsDALAjcofJwYKkfp368u2U3BRcvnvZ5Pmo5Cjzl5Qq\nryCP68bXMvx91kx2D+mQkBBMmzYNV69exdtvv41JkyZZbJOdnY0RI0Zg7ty5uHz5Mi5cuIA+ffrY\nu2mq5ujgCAeNg/yHlZVfPDvMKBiRkJVgsv0jTR9Bv+b9gI4d8e1OYLBTWwBAxn/eE7uybcjRAvDw\nsP7kyZMQAOSdPQXExyPRUay66+rqAnl5gCBg3YV1mLx1sskpSkFLgxDwbYDpvmp4SJtU0qWEtBSS\n7k7uFtt8P/R7+XZaXhouJl7E6fjTFttZ26+DxsHi9w6IwSxJzkmGo4PphTfuJ6Trz6uPxl80Ln3D\nMlhxegXWnF9TKfui+1dbQjopOwmdFnfCYcXaEPaWnJNscy6Kvdk1pBMTE3Hy5EmMHz8eADBq1CjE\nxsYiKsr0Q2PNmjXo3r07evfuDQBwdHREw4YN7dm0GsHJ0ak4pItOu3FzEs9rvpVuuviIl4uXeGPU\nKLgMexI92g8BAJweHAhDk8biOPNbbwEw7QJPdgHQwXJhjUINkK0D3u8PNBp8AdeLizWkHd4DeHoC\ns2bhmU3PIDQ8FBlFXbfKil9QdnFLIX3kCLDb+phuVdh0aRPuZd8r9+uU3cy2urulsfpzCecAAI/o\nH7HYZnq36fhu6HcAxEq6/aL26LK0i8V2StLYtaHQUGrgpuSkWKzpfT8hnVOQYxL+FTFl2xSM2zwO\ni04sguZDjcX/XaoatSWkV5xegXMJ5/Ds5mer7D0bzm+IpguaVtn7Kdk1pGNjY+Hj4wOtVvzQ0Gg0\n8PPzQ4xZZXfp0iU4Oztj2LBhCAwMxMSJE3H37l1ru8SCBQug1+vlr8zMTHt+C9XKJKSLKunGbmJ1\nY35U51W3KKR9fIBffoFHQ3Ft8JHrR2L+kfnAAw8A7cWZZMrTuZJdAAwZYrKvWQMB7fuA67vAx33F\na2L/S7FJ2vZNYiX9VfGlNVMuiheNiE0rvlb2vfffLH5RStEHfq9ewOOPl+0HcPgwsKYMFVhubpmC\n/8+YP/H0hqcxVLHwS1kpK2lbR9TSxDEBAurXqQ//+v5Wt6tXpx6Aso9JJ2UnAQD23NiD57Y8Z/Kc\nsqsbEKvqkkL6eNzxMr9vZfvXr+LkxwM3D1TL+//d1ZaQlgqBuoqLFNlbSRNA7U0VE8cKCgqwe/du\nLFmyBGfOnEHTpk0xffp0q9vOnDkTcXFx8pebtGJWLaQMaek/5gOuDwAAbmWYViMNXBqY3PdwLu7C\nlmf4PvMMMGcOMv/cKz93ry6A7t1NXvt7K8CjwBEjrgD1cgC3PGCnovc6tQ6QUgeY/kRxF3fKyT+B\nP/5A7MQR8mNX132neJFZd7etg6ubN+WJaujdGxg3rvTV0F58UQz+bdtK3CwhU+wq/uvWXyXvzwrl\nOHx8ZrzVFciUf8gt6reAq5OrxTYA4OnsCaDsY9Lmv2ulbk27mdwvKaSjU6PRM7Qnvjz6ZYnvZ+8P\nczVdpS0rPwvv73vfZDiptlL+XmvyojfSZ6Grzvrflz1Vx/9du4a0r68v4uPjUVA0c1gQBMTExMDP\nz89kOz8/P/Tv3x9NmzaFRqPB+PHjcazofN6/M2vd3Q+4iSFtXs1JFbZEGdJ/3fpLPH9WqwU+/hiZ\nzXzk55JdAHh7m7z2Zj2gs9uD2NJ+LlKePYv1UaarpKV5u+KZV5pgcWDxjPBTl/cg6/uvEXMnQn5s\nSnDRmDcghrRB8cGwfTvw5ptAttn6082bA03NupUOHhQXY7Flzx7x34gI29tUkBTSTo5OyDZkWz3/\nWXlqWl1dXZtH+lIlbWts21xJ3cOBD5j+bkrq7j4df1qcz5BpOa6tVJmVtvJDTY1X7Zq1exY+OvgR\n3tz1psVzmfmZGLZmGMLvhFdDyyqf8nKrZT1AVKPqqKQluQUlrN5oJ3YN6UaNGiEoKAiri66wtGnT\nJuj1evj7m3YDjhkzBidOnEB6uvhBuHPnTnRSXPXp78pqd7erGMY7IneYbNu/eX+T+8pJS/mF+TgW\ndwwFxgK8t+89k9m/yR+9AzQorsLTtqxDigvQvFWQuLhJx474x9oTGNhiIN475gwXA3C+oYDf65oe\nJExp8CcG+exBrGIO2uWGwMa2RXe2bweuXy9+8plnxPO3Dx0S769bB7grJlopx7OHDgW+/Va8XVAA\nzJljui8bAV5gLMDEXybiQLTYvVqWVcNskcakH28pdtWbdzsDppW0i9YFLeq1MHn+kabiGHV9F7GL\n+krSFfk5W0foWflZ8uplStIchE6NTf9OUnJTTM6/9nT2RHxmPDLzM3E+4TwAINNQ8hBRZY1FAxBP\n8zNTkd9DZbuTJZ6+qDyNTbLyzErsiNyBJ9Y8UdXNsgtlJZ2ck1yNLakY6cIy1RHSyjk3VcXu3d1L\nlizBkiVLEBAQgM8++wwrV64EAEyZMgVhYWEAxEp69uzZ6NmzJzp27Ii9e/di8eLF9m6a6lnr7jav\nmCW+nr4m95WVNCCOA649vxYfH/wYI9cXr2iW7FMPaNAAv7cC/vQDbnZvAwBo5lV8IOXoqMWuibvw\n4Y3m8MwFLrhZ/496uHE+Pig6VhheVNTGS7n7669i2JpLEsdb8cwzJl3gBbt+xx3FSEbyzk04FH0Q\n2LFDPI9bOa4tBVx0tMlM9vA74fjx3I/o979+AFChLk1pTDqkSwj6N++P/dH7LYJVGT4uOhd0aVI8\nIezXcb/i9/G/AwBa1m8JB40D/owpvni4rS5mWxfi+POFP7HgHwswqNUgk8fvZt01GT+X2nAt+RrO\nJYoT2hKzEnEx0fqpkEDlVlnWLgJSneN75qS2aGC5boH0nJraK0nITMDXx74uV/drbQlp6e/Y1nCS\nXd/bUPXDInYP6datW+Po0aO4evUqTp48iQ5FM4mXL1+O4OBgebsJEybgwoULOHfuHH799Vf4+vra\n2uXfhrXubvOx52aezTC3/1yL1ypDWuugxYGbB6x20SbnJAN162LwBKDPC8DNVHF1sOb1mls2SKtF\nvaLeHicHJ4y5Yf1ItpmLD/5bNAyeoPw7Ula/kqQkq2uFv7//ffi8AVxuJH54Dgg8i0f/1xfXnn/S\ncl/SB9V33wHNmolrkEM8ZUlJOUMbAPDXX1Yv0ZmZn2kxg1vqqnXWOiPAKwB5hXlWqy+Ji9YFbRu2\nle/3b94fnnXEseg62jp4sMGDuJFafDlRaxUnAFxPEb/Pd/u8i88Hfi4//pD3Q3itx2vyPiWRyZEm\n++3iI4b01XtX5Up69/XdaL+ovc1Z38qQLs+qaNZYqzyqo8vQFjmkrSwuJB10WQtwSVx6HJadWlbl\nY5WP//g4Xvv9NeyM3Fnm11R3SEckRWDw6sElvvfp+NPYcmVLifuRPgvN/77LKik7qVyrJip/brWy\nkqb7J41/zj88H0dixQtqSOFZV1cXb/Z8E9EzovHuo+9avFYZ0n2b9cXR2KP45covFtslZCXgXOJ5\n+b7UBdusXjPLBjk6wrVoWLmXXy+sXpGKFcErTDbROegQ9UYMHrgkhv2CnsAH/ax8c9K485dfFl+J\nS2G+m1j1/frvZwB3d5ytIwbHPeVxgfl4tqRoprdJ5bxtGzIPKmaADxwIPPII0KIFzHVb1g1NFjSx\n+ofs5OgEXw/xAFI5k92ci87FZGxYWpxG0r5Re5P7yuBKz0vH1LCpuJl6E9dSxMXYH2vxGN7s9SZO\nTD2BqH8Vh6vyPcxnegPAwJYDAQBbIrZYhLKt9itDWjmOeT+sfahVxwedLVK4WvvALy14l55ail6h\nvTBt+zRsvly1iy+dL/qbLU+VX5Uhvf3qdpy8fdLksTEbx+D3a79jwdEFVl+TkpOCLku74Kmfnypx\n39KKfvdzsHc+4Twazm+IOXvnlPk1ys+R6phgyJBWMSdHJ8Smx+Kt3W/h+K3jeKTpIxj64FAceeEI\n0mal4fPHP7f5WmVIv9ztZRgFIw7HWp78vyp8FTotLh7X/P7k99BAg4e8H7LcqYMDbhYVbr18e0Hn\nqMOwgGEmm2wcsxFaBy1cfPzgUfT5/mE/oO8kYH/zoo0OHwaOi9ddRmysyZKkX3cHQjsXX9nriMMt\nkzFzo7KoOXpU/Nf8w7RotboM5cpowcHI+HVr8X1pspnBcpardKBi7YPMydEJvu7iAUZcepzF8xIX\nrQsAYP9z+7Fs+DKLSq2kkN58eTOWn1mODZc2yJV0qwbi+uxdm3SVb5sb8dAIi8e6NemGjg90xJrz\na2yOBcelx2HrFfFnczHxonzKF2C7wi8ra4F8N+su2nzXBqvPra7QviuDcqEYW6xV2RFJEQjZHiIP\nR+y5scc+DSyFs7bsSwoqQ7oy5x2YEwQBw9cOR7dlpmceSO9vPrFRolxu+FjcMZMzKiKSIuTXS3+X\n5Q3pBUcXyENf35/8vuSNFZRd3KykyURXn64m9+cNnAeNRoMevj1s/keXSIueAMBTbZ7CZwM/K3Eb\nSXRqNAb7D0YT9yaWO506FUlFRa+0UIeyekt6MwnBrYuHMLy9iocsDjYHnnqm6L9bu3bieduSfv3k\n08BeGwxMHgEkFr3P3rRwFHgXh3SGsiAtmpBoEdJxcUBODjIvFc/KFWC6RKr7O0CmtK/z52HNnUzL\nC404OTpB/+48AEBs1Cm5q9686pJCum/zvpgSNMViP718e5ncV1asB2+KF0NJyEzA9ZTr0Dno0NTd\n9kIK0v8FXw9fq6fijW031trL5A+cDos64Mmfn8Qvl39B+0Xt8fLOl622634or3ktORhzEFeSrmDC\nLxOsvkYQBKwKX1UlM5BL6tIuqavf/ODFvGqsKuUJKmWb7TnkYKtKl0JW56Cz+rzy4LDHih4YtFqc\nb3Eu4Rwe+u4hTN02FQBwL0dcjMjafIeSvP7H6/fVgyBdLAeopWPSdP/e6/se6tepjzHtxiBvTh76\nNu9b5tdKS0N61xVPr3o+8HmLbXroe1h97es9Xre+0+nT8WKbiQCAnr49AYgXA5GYB0SmYPpBlups\nhDH6hrhamVZxkNGtG/DTTybbFhStbJmSn4YDzYs/QNOdIXaPP/IIsH69eA61+XnUt28DAwYg46Pi\nLq1kF9OAz3QGLkmL2nXsKJ5j3akThNnvyNtYW4LTydEJvsfFSjt24cfArFkALD+0XXQuFq9Vkrqh\nJcoPTSmk72TdwbWUa2her7nFUp9Kzo7i0UddXV20qi9W2VM6T0HW7Cw4OjhiYqeJ8rZ+nsWnP0qV\nihSG0pCKkj0qaeWHnjVbI7bi+a3PY8yGMeV+P6nL1NaVysyVVElLH8jWAtx8KORC4oVqOYfWVlD9\ncvkX9A7tjVO3TyHyXiQMhQaTSrqiB18lsXVev3RutvnQj8T8wOFYnHgartSz9cPZHyAIQoW6uyXW\nfqeGQgN6rOiB70+YfU2YjQAAIABJREFUVtnKLm5W0mSioWtDXH/1On548geb/7FLcnvmbXn8sr5L\nfdyaeQsHJxVfsvLhpg+bbP/UQ09h/3P78VjLx6zvUKPBt0+HIn1WukUgi0+b/se3dtQ6OfxDyw+W\nli0BT08Umv3dvHterNI3Ni0+BSlj2OPApUvibPDsbOD33+VZ4Yu6AisDAcTHA8eOmVTOd9wUlXOR\nCC8Uv+fPPwPnzuHwT8U9DtbOJ3ZydIK+qBcu1hPAhg0ALD8spUraFo1Gg4Q3EjCghXiZSSkMb2fc\nlsehY9NiEZUchQe9HixxX1KXp4vWBS3rtwQgflBKp6joPfTo26wvPJ090a5hO/l16XnpJsFi7bxt\ne4xJW+uhUJImOB6NO1ru9/s16lecjj+Nx38s26p2JY1JSx/Oyv/XRsGI1NxUix6CylxGtTTKgxxr\nPRWAOCZ8OPYwui7rioBvAzDjtxmmIV3Bgy8AOBp71OrfiK3z+g1GMaSVB/ZKtr4XZa9hliFLnsRZ\nmb0BMWkx2HBpA47FHTPpSQLMKmmOSZO5enXqlWvcScnH3cdk9m8T9yZyBQwAL3V7yWTi138G/KfU\nat3RwRHuzqYXjlg1YhXWjlprsa20stb8x+fj4KSDaFW/FVaFr8K4zeNMN2zRAvDwsAjRV2J80Nit\nMU67Fc/KTu/0EODnBwwrGgtfXTyu+dIw4IUnASSIHxzK/Z1tDJxT9LADwMSRQLuXgfEjgXPJl7Gv\nuTjDXWKtknZ2dIZLAeCdBcR4AuEFcTDqm6L7MtN1uuto61i81lwj10YIDhCHB6QPnEM3D8nPH4o5\nhPzCfHRoZLm2unmbADFApN+v+UHUHxP+wK2Zt0x+d+l56SYz/i/dvWSxb3tU0qWFtFTl3M+HcGlV\nujmpkra20Iq0L2XV9dGBj1B/Xn0cjztusX1JcxQqkzIEy/ozWnFmRaVW0sk5yegZ2tPqZS9Lq6Rt\nDdXZ+l6U7VaeXmgr1MvqXvY93M0Sz9Bo9nUzk88lZU8Jx6SpSim7TT2dPfFC5+JUsjoOXQbPBT6H\nf7b/p8XjuybswmvdX8OM7jPQp1kfXP3XVXTx6SJ358qaNwd0OrErW6FRi/bw9fDFTV3xH0mGW1Hy\ntmoljm1vEU/XyFP0BmfoBNx2N+3eHjcKuGq6sBoAIMIb+Kkj8GjnMzjtY/rcHSsX0nAyiH+8vunA\nYT+g84vAGq/buJpiemnO0rq7JdIBmPQBJf1sfD185QAxn2Rmax95hXl4udvL+HbIt/jvkP+attvR\nCa5OrnDTFc9DSM9Lx+Wk4oVtLt61PHe6wmPS5Rw3BIp7YEoaE45KjsLcg3MtupijU6Pl22X5QJV+\nxtYCwtr441fHxPXqd13fZfFcVYW0crVBWz/f3ELT78dgNFRqJS11OVtbx760StrWkqS2Qlo5gUx5\nuzwHceaz4DUaDZouaIpGXzSyur3y78JkdjfHpKkqxL4Wi90TdsuLAXw/9HuMajPKYgGUiurs0xkL\nBi2Qj5wdNA7w8/RDSm4K9lzfg3+8oxcniDUTT/dSdk//L2cw8P33aOrRFAkaRUjXVaTxDz8A9cQl\nNm8rivtnngaavi4uzlIapwIg6DaQ5iRgn9nZWAnJlguJOKWLH/x6xcTxg1bOVjPp7s7IMF1BTUGq\nuKUwPBp3FD5uPujhWzxfoNSQLqqk8xJuwzHiKl5++GWrwxGA6XhgWl4a/nf2f/J95Yd432Zij4o9\nKmmljZc2YuGxhSaPlXT+uaTHih74975/Y++NvSaPK0NauhpZSaQP+pJC2tppTtY+rKsqpJXd6raq\nSfPvxygYTX6/Fe0qLqnHwlYlLb2/rf9Ttr4X5TK1yrUOynMAaG1bqR3Wroq3/ep2+bbye917Yy9a\nLGyBDRc3lPm9K4oh/Tek99CbjDtP7zYdG8dstHqqSWVr4NIARsGIgT8OxC7nOOw4uByoIwaVVPl+\n3vktTPzsV8DLy2JWc3rRH2l6XjoQFCSeE92jB+IUxxc7ii4GcqQMIZ07F1i6zfR1kh8i1lts75Qu\n/sH6Zhd32ZmPpQNmlbSHhzg5zQoppKUPzXs596D30JtUvA99v14ch7dBrqQXfQO0bWtzO8C0W/ds\nwlmsCl8FvYdefqyJexNcfOmi3DNS0Q9zKaRtLQgyesNozPh9hklFrJzla+v9pW2+PfEtfov6TX5c\nGdK2LilqrX1WQ7qoglKGhzR2rTz4kFYBrKpLcEpVLGD752M1lBS9IhU9+Cpp5n1sevH598rfq1RB\n2+qdsfW9KJfFVXZ3l+f/pvlCRsoDL2mCmtL8I/Pl91IekO2I3IHo1GiM2TimTAeBlYEhTVXKfMGN\no7eOY8ZvMxCyLQRRRcWfu5uX/Lx5SGfkZ2DRiUXw/MwTZ+LPYH+DdIx8tTFC51u/tqytcJj/B3Dz\nK0ADIPAO4KH4e59w1nb7nX4Vuzl9ew6WH4vxtNxOrqSlCjoqCli+vHjp04MHgawsOBdVtrlrxIo2\nMz8Tbk5u8prcs856oM7HnwLz59ts07Lhy9CiXgu8dKLogULbqykpx9qkD5nnOhWvQ6730KNtw7bF\n1XlBHu5l30NEkunFS1JzU1F/Xn2sOG26mI3S3ay7+OywOBFPWq/cFum0GsA0pG1dFlSy5coWDPmp\n+DqqypBWTly0VXVJYbv7+m48v/V5/Bb1Gy4kXsCGixvkD2dld6f0/0n5mH+D/2/vzONruNc//pmc\nk5yckxzZiCURQRZbFlvF1lDVWm7Rlrpq30tpS3tvby+tpS2q6A/Vonpzr6X2rdRSWkopUmKnlohI\nhARZZU/m98f3zMx3zpmThJJEPe/Xy8vJbGdmMpnP93m+z8JK6D6sJb3sxDL8cOmH0je0wAukXXe3\nhoA9yjlprXryABPlmKQY+edxO8bJlqg0QLRrSWtcS05BjsrFzV97bmFumSPqrS1//pjWAYod/Dvg\nXs49rD23VnNfAPhX238htLr2wPtRQyJNlCvWbtgN5zdg/tH5WHpiKeZaYtrMVarJ632qWIl0XibG\n7hgLANh1ZRe6ruqKzRc3478XtPtOWwe5DQwdiFNvnMJ7hwE/y3tGJwItLVrgJDhisfCS3fN3WrwU\nAFCLGzzEWvRnSuQUeZkxzfKHHRen7DxyJDBmDCvkEhkJDBoE5zw2os/bsQ24ckUW6VHNR+HY8KOY\nsdPyQrt8GRg0CKhhW7v9GZ9nEPt2rOKCv2lf2ApFZZ5XeuE1dPGXl0lxCfw8d98NfdFgUQOVCEUn\nRiMtNw0jttnmgEuM3zleFlytamg8/LF5d/e11Gt4Y/sbZSp/WSwW41bWLdkzIIn0d2e+g3mm2Sa1\nBlBbyf89+V90XdUVIV+H4LUNr8m11fOK8uTBjeRt4i0zN4MbTI4mJGTaF+nUnFTN+uwFRQUYuW0k\n/rb6bxp7aaMS6TK6uwG19f+4LOmEjARVwOVXv3+FD3/+ULXNg1jSqbmpqgGBZKW7OLpAhFimtqqi\nKKoscGusp0xGNhsJo96IRdGL8MOlH/DBTx+o1jvpnPBpp09L/d5HBYk0Ua7wIi3NT0vEWAK3zK7K\nNtaW9PX06/Ln/df3I7cwVzNoTYLvBrb+2S+x/OXlmiPgEMt7JV8sgPGjj+0eT5fABNDg6i4vu2bR\nn8CFSq638V+TWZEV63rlK1cCw4ezz5s2wZDBLLJcPZC/ZhXyi/Lhuv8wnL+JQkuPxhByLS+0q1eB\nFStY5Hr//oB1K1feorh+HfbQCsaq87ESZNbMi0WS85a0VE1r9iGlwl1ZpkZ4a8XeHLlEQkaC7ILk\nLeltl7ZhyfElZepEdT//PkSIculcSaT/E/MfFIlFeHPHm+j4v45Yc3aNvE9Zo3Wtt+PPUYSIqqaq\n8vfdz79vE+DkOdsTvdf1tjmuVkR9aVhbk1poLefniiWhLCwuxNfRXz9w1LI9kY6+ydw5fKGkOzl3\nVNvYGyBonfPJWyfx2w3F0pV6C3i7eNvdh+dm5k04THfAP/f+0+421pZ0TdeaGN18NE7eOikPnqzf\nWw9bN/xhIJEmyhX+Ybfu4CRh5gLY+EYfLo4uqt6+P179EQDwXuv30DWgKzyNnjYCzL+Y3Py4Sec7\nd4CpU+UfQ7lsK8FXmaN9/1egv8bUU896XTEwmY0qRIteVTmr1MY2Xo5j88jWIm02A+eUKGrn48y3\nnqcD7v/E5lZdk+4CY8cCLbiKc7e4tKXvvgNaWxWiyeVeViWIdKCnbc51jd+UimvPzmf13XlLup1f\nOwCsmIT0UixLz2m+VWdp7u45h+dAN12HCykXcCf7DlrUYte+P25/qd8jIbkl67ixSL57OfeQXZCt\nKoe7P24/lp1YJv9sT5ysBxXSdtLLmRcHURTh7uwuzxUP2DwAz/3vOXm9VDN926VtNt/zIL2qo2Ki\n8Nr610oMHFv8+2LsuLxDU7z4uWJJKBceXYixO8ba5AaXhj2Rllzd0jMDsCkWfg7YniWt5RXo/l13\nXL6nZE7EpccBYDUk7O3Dc/zmcQCwW9ymmqmazTJXJ1fMfXEulvdajg+f/RCn3ziNlH+kQCewoFXp\n+SovSKSJcoV/+UmFPACgS4Ayx8u7qOt71sem1zbh5sSbmp25XBxdEFYjDNv6bUPChAT5D4i3wPcO\n3IuewT3Rvk57ZUcvLyXXetgwNOEbhHl64tutwPp1wCzHrqivUUnQuVpNLH//CBrlKxPSZs7zZiwA\n0KQJ8NZb6h137mS1yC1R6QaLSOfqgfsnmHXsIh3noqXftJOdQjZ8g5FMzp3Hu9itmNR+Er7t8a0s\nggBQg5tye2bXWXZenCUtCVR6Xrrsdubne2NTY+UOW6duncKiY4vYNXFCYe3uHho+FEv+tgTP1nkW\nAGulCjARzcjLQD2PeqjvUR+nbpcQIMCRW5gru6B9q/hCgIA7OXfQdVVX5BbmIqy6Up8++mY0isVi\niKKoOQ+65tU1eKP5G6plJVUfEyHCw9lDFs/jN4/j0t1L8np+Pj8hIwHRidHyzzG3lPlbLdacXYPF\nvy/G3ey7GPb9MKw/vx7X0q7BSecEAYLN+Y/5YQy6f9cduYW5cp0CCemcHB0cZaGUBrEPGgRlb5Am\niSif218kFpXJ1V6WQDCpkUlZLenSeDHA1lBwcXKBg+CAgWEDMb3jdIRUD4GD4CD3M/izneEeFBJp\nolzhRbppjabyZ76+NO+iBljt8Zrmmqp9pZdl05pNoXfQQ+egg9HRKLvZwmqE4aWglzC5/WR0qtcJ\nW/6+xbbASPPmrM730qVoZJkGHd18NCAIGJZeD71NLYCVK+E5YKR6P0EAatUC/PxQLTBcOW/u3WPs\n3hNoq67PDYAtu32blSEF4HyRWQm5eqX4iqv1NJulrrkNP3NzaRlcTpiWJX38OLBhAxx1jhjWdBga\nVm0or3LNB5ZsA2buBYyW949kSc87Mg9JmUlwcWTpej/FMtc3b801XNQQoYtDkV2QjfAl4Ri3cxzu\nZt9FRj47p/dav6cS6aHhQ7Gw60KMaj4K2/spqS6A4kY3OZrQrKZtoQx7pOakyvOObgY3uDu748zt\nMzhw/QB6BPfAnBfmKLcqLwN/3PkDBcUFmkVMzAYzejXopVqmVX2Mx8PogbTcNBQUFSAxMxGZ+Zmy\n9SiVtQSA2l/URsS3EXKks5QXLw2KrOm3sR/G/DAGc3+bKy+7lnoNHs4eMDoaZUsyPj1e1V0qtzDX\npu6BVETGbDDL4sYPxh4Ee5Z0fHo8PI2eKgu1qLhIFXxV1sAxL6OX6ucIX+XvwNtUNpHW+v3y9Aru\nZbNMetat+ex5VrP/hfovlHjMRw2JNFGu8G5PvpsTX8vaOthLYkj4EPmzVJTFuhqXVAozpyAH3/f7\nHh8/Z39+GQBrmanTwWXKJ8hznI6vuluCi65cYfO+np7waNFOvY+3N+DKBgOS2w2wsqQXfA38+isQ\nEwP84x/q/fV6wNIv3TmWuSDzXAyKSD9j9X1N7ORJf/QRsHo1q10+j2v/d/48EBXF+mpnZwOTJjHX\neZ8+wMaNwO3b6ODfQXWoUceBf/2q/Cy9vC/dvYSkrCQEebGpAml+8V6C4oKUgncW/75YXpaay0TT\nxdEFn7/wuVw0Z+vft+I/Pf8j5+ibDWZVxzUpZ9WkV+qQl4XU3FTZkjYbzPA0espu0ufrPo9gr2DV\n9scSj9mdD65iqILmtZqjf0h/+fnKys/C+B3jNXuy13WvCw9nDxSLxbh456IszpIw/XFXHRlfLBbj\nXs49xKbGypY0H5wmwX/XzF9nyp+TspLg7uwOZ72zLFItlrbAuz8qNfdzC3NR3VUpscfXQDA7mWWh\nlP6OyhKAxZOWp4g0H2Ednx4PPzc/VQpiXlGeKp7BekAgiiIm/zwZd3Puyi5lADg64iguvqkMcAaF\nKjXoZXd3KbnSfGyAFt0Cu6Gmq7qKkVbjIQDoHtQdl8Zdwntt3ivxmI8aEmmiXOGtYb2DHnXd6yLY\nK1jlnrZXVGVw2GD0a9IPi7svll1O1iIt1SPn3bllYtIkOP37QyUgRBAAnc7mnAEANZU/6lquirVS\nhbekpZdUeDjQQ+kMJmOJ0jZY3suftczD0ubss81LIsgqgRtgQh8TA7z+OtC3L/D118q6Q4eAYcOA\n7duBr74CZsxQ1vXuDUREqKYaVBjZeVuXonVzZtbpnew7QEEBUlcus9mVF4nUHCaaZoMZSE1FS9cg\niFNEVZc0iZjRMdg3eB8AJRXL6GhUNQPh0Uq74S1ps5NZNRgM8AywyRI4knAEzZZoW+pmJzMcBAes\nfGWl3Gxm7bm1+DL6S9V2egc95neZj7kvzJU9BbzbWHIJW4s0wALPpIIZktVpneojzadq4e7sDqPe\nKIuUdQGY3MJcmBxN+HXor4h9K1ZlhZoNZlkopXv2oNHevLtbEvii4iIkZCQwkeaK+Vy6e0mu1CZ9\nV3ZBNkZtG4W4tDhcvncZnx5k0dJ8GWNnvbNct769X3s09lbqzkuNg0qzpEsSaSedE4yORpx/8zw+\n7qgM5qUBpBaBXoEP1Ufhz0AiTZQrZiczdIJOzs29NP4Szo49q3IjStawNToHHb579TuMbjFaXmZd\njWtks5HY+vet+OS5Tx7ZOdukD9VShJm3Anl3t8p9WdVSj7QB16PbYACqVoUzN731jSTSra3qp2uJ\nNNeDGz/YybHdsUPb9R0XB393fwzy6IAF1plNOh0QFQWDoG6CYHI0oaqjG+5kJQNHj+KeVdVTT6v4\nq7TcNGTmZbKpC09P5R5o4Kx3lgcmkkibHE2aIl0sFmPUtlE2y+/l3LOxpCUCvQLhIDigvkd9tKjV\nAtVdqmPVmVV2+2vznhyp6M/8o/NttnMzuOGtVm/B6GiUBwX8HLokEFoVre5k35GLn0htX61FWoqU\n1sLd2R1GRyOOJh7VrH6VU5gDZ70z2vq1RV2Pugj1VgIqnfXOsihL0xbx6fFo8hX7W8rMy8SXx77E\nez/atxh5d7c033wr6xYKiwvhV0VtSUtz4Z3qdoLeQY/8onws/n0xvjnxDfpt7Kc6lruzkjXhrHeG\ng+CAtPfT8OPAH1XNYaQpsdJqtZcU4Ci5092d3fFGCyUGwd7UQ0VBIk2UK4IgoPCjQvy3138BMGtE\nKhuaODER0SOjy5Te0LsRS2fhR9fS8XsE93iko12VJf3MM8DChfKPDaspc7umAmD/azsxq9MsdcWx\nBg2YVfvLL+oD16oFg0YMimudQHVKFS/Svr7A/v2qyHS77NjBrG0t5s/H/97ej/HHAMyZA4wbx5Zn\nZQHDhsHQUD34cSkQUPX8ddy9eRXYuxepViK9YR2bJ+wawAqLpOamIiMvQxG8wpKDbaSBmSRoRr3a\nkpbWX713FctibK342Ydny/uandQiLQUcXh5/GcdGHEMr31ayoK9+dbVNwwfek+Nbxddu0Qp+MCkN\n5LRE2rraFcBEWlpew4V5VWYfmi1buDkFOfjmxDfwMnqhUTXbKnKSuxsAXtug3dKTt2al7miARaQL\n1SINsNrt2y9tR5VZVTB+53jM/W2u3eh33kKVtolPZ2V0/dz8NAfa816ch6qmqsgrzJPF8072HVUn\nLWuRBph17ax3hpdJmaOWLO7S3NklrZescUAdB1MelRcfBBJpotJQy1yrzG7qlS+vxK13b5Waf/so\nUKUPHT3KWmta4C1pAUBkgxfxfrv3bQ/SvTuby+bR6VSWtISNu9uPsyhHjGCFUCZPVnUAk/Hh3Lo3\nbjDXt1Z0+DvvsP8jI4F332UDj5495dXWgwdTciq8coA7Qg7EgwdwzwjUTQUSjrbFr98CHeOA2/2O\n453GLAc8LTkemSmJMOdwda//8x+gYUMgNRWIjwfyFNeDJCi8JV3brba8XgoUtCcav8b/iq9/Zy5/\ns8GMzvVYq8rm5mDob7KAKUEQIAgCOtTpIO/XyqeVTbSudeDihIgJ8py86p5wQiSJC+/ulkVao5DG\nnew7shVY08ymTxYcW4B/7WX9ybdc3IKEjAR80O4DVdlWiWqmaqpcbS34QMm6Hko6nEFnkC1p6wCw\njw+oYzjOJp/VPDb/e5CC16TqcD5VfGxatbo6uaJxtcbyd/PlYvniJ3xEulb3v2kdpuEfbf4hD6Ts\nVT6TKGk9L/rl7cJ+EEikiScSg96gCox5nJRULau6i9U5PMgoXK+Hvhjo6xqBSe0nyYtlkT56lAWA\nOXKuZ7NFQHQ6VrvcmoAA22Uffmhrxb/8MosOX6MU9pCC4QBlrlzCJeYcqmYDeXrg5O1TiKkJeOQA\nPj9Fo62UfpucDPdRLOUsdeNKZOoKYT7DzccOH87SyvbsYU1VBivlSE0HDgPgAsdupsDNQRHBnMIc\niKJY4ktX6lxkdjJjWNNhSBxzBTs++gMIVOeGj3tmHPqH9Eeb2m000/qsxWFI+BD88UM9dExWz1Wq\nLGnLQI5vw5mel47C4kLkFObI3/NaY2b1Spa0g+AgpxMBwKEbh/DB3g8w+zALtOpYt6NmLm99z/ry\ndy3921KcGXPGZhtepHlL2qA3yHO5fB1wQMnplrCXx83nJ0uCK1nlnkZPm0yK3o16Q+egg0FvQF5h\nnlyUSISoume8Ja3V0vKjyI8wu/NsWcx5S/nKvSs21cP458X6PvKWdGWznnlIpAmiFErq5y0IAnpW\nfxavPHjhKCAqCsK8eVgz4RCmdZgmL5YDV555BhgyRL1PFS6ojrPoZerXt93ujTeA6txg4sIFYMMG\noGNHdZlRTqRNVt0ETXfSUdViPHXtynKk66QDyOeigpOT4XGVWVOJSZdQoAOqFOpgw7Fj7P+1rDYy\njh+Hse8AAFzg2LRPITRvDgPYi7pYLEZ+TpbmHGMDq6ZZ0iCnVoEzvO9DXegFgKPOEStfWYlDww5B\nEATsHbhXrr8NgA2OrIPTdu2Ca6q685W/uz+Qng7s26c5kMvIy5Ct5U51O+HmxJv4vDOrwX4n+448\nZ89b7tE3ozHr0CxZHH2r+Mpiwrvh+cj3sBphmt/PCyVfgMOgY0IpiqKNJc3nvwP2RZq3pFecWqE6\nlruzuyr1KdAzEDM7zVS+u0gR6dtZt1Xu7rJatLIlzT0PgQsD0Wl5J1VgIS/iHkYPVQxLVaP9OInK\nBIk0QZSBZS8tw+4BuzXXbYlcjI22DbNKp3FjYMIEwMFB1efbXgoIAMWSBljw2Y4dqgpm8npXV2DV\nKmD9eha0xbvaGzQAHDT+9F0US9H00cc4zZW6dnFygVc+O8fbrkAjsSq+3Wq1f1wc3C16uDCUWVrm\nVA339GmrwhkpKaz4CxSRMBUAOHsW9z4txABDSwBA9vjRmpb02g1QpdGYpTzXVLWViJUrga3WJ82C\nw34exFlgERFs3t8KF6sspW6B3YCBA4HnnoPXJaWal1FkA4vk+8mySJidzKhprikL7t2cu8i8kwhz\neg5cizQGMmCCVdVUVQ5w4sW8nkc9WYSbeDeBp942bZEXaX6gadAbIEJEYXEhUnNT8UL9F7Co2yJ5\nfaBnIEKrh0KAoCrKIiGKoiqqes5vc7A3dq9slXs4e6hSui6NvyR3CnPSOTFL2lLeMzM/UxX9LvWc\nLo2S5qT5rlW8iJudzDgz5gzW9WZ/rEObDlXtt3vAbvwyxMrjVAmw9ScQBGHD8GbD7a80a+d1Pyxl\nFmkA6GrpADVyJPDNN0rkubc3S8+ScHdHqXCWNPr3R8iHSmME0wcfwX3/EaCQlQ39OGAUPDyigKQk\nNgi4cwcYMwbuVtqvmpOWOMlZZz/9BGzfDsdiQF8ESIa3JNqmAsAr7jZQE8hevxrpo1mBmPAk4GRN\nZZsgt3pIymKtKc0ZeYAL1CKdkMAEFbC1kqGRm3/8OPM0APLcuYuVfnQL7AbsHAMAqHdJMecbZRlx\n3JyJST9PUhUQAZiL3Kg3Mks69iJcAbhGq6uq6QQdisQiVDFUgYPgIAss75Kt61EX58aeQ0JGAnO7\nTxgHWHnFrV3OC7suRLFYjBNJJwAwMcvKz4KHs4dqLnhCxASMaTkGDb5soHJFS2ilPSVlJaks6Y7+\nHfFS0EuYEDFBtZ1Bb0BabppqsHUs8Zj8WSryUholzUln5mXKf0O8iEvL+jTug+JGxTYu7vIuUlJW\nyJImiD+Lawmi+jCHK0mktSxgAFiyhBUvGTeOzf1u3KheLwhsvvol+x2+VNfh78+KpVhwcXRBTR9W\nEMRQCHTrOEqJOm+qVI5ztNLkBK2U9xTOP/3888AiZsUZufgt3t1uusKihrMdgfRUJsTLvlfWu+QD\nwYKiUI5z5gH79gGfcGl4ixRLEcW2AwdXwWpKg/dOpDHxkSrBGXQGrH51Navo5ckCF/UX/oB3EQuW\napSm2D4Lj7FMAN4Kru5aHSdvnUSsByuA4wol5mDr37disiU+Ia+AiaFU4IOvf21yNKGeRz25rCqW\nLrW5JmuRHvfMOLzV6i15uZQC5mn0VM0FS1ZvddfqSlDX99+zwjlQXN28sC84ugAbLmwAwETaoDfg\n+37fo2PdjqrsLK//AAAdS0lEQVRzMOgMsrBK0fu80GoWVbl6lQ0COaT7mZ6XjgPXD6h6h3957Eu5\nZSl/bP7vqjLPQVtDIk0QfxazGXB2tq3T/YDEvhWL7f22lyzS9lKZBIH9M5lY32qt+erLl9nL1h68\nSAsCME2ZJzc5mvBivw+xwn0Ybvc7AWefOpoijenTVYesLRkyY8fa5nNbRbvzwqwl2NmOQPoZljvs\nlqdePzSXy0GfNw947jngxx+VZXv2KJ/r12c11E+cYHnk3bpB34prWFK9uqZI97QUv5rT4t+2nddO\nnIBPHptPdbhr5WaHej55UvtJSMpKQp6e5dab7ivC1CO4B3wSmLDk5TK3rRRYVt+jPmY8NwOzOs2y\nOT6q2QaX2ZTBtSDlAUvzzUFeQSqRlgIya7jWwJ3sO8y67dkT+//9OuLS4uSgMX7QcDzpOJLvJ8Oo\nN5YYw8Gv6xnc02a9n6Dh8QkIUGctgNVMMDuZcS31GiL/G4nwJUp53hm/zkDo16EQRVE1x26vkmFl\nh0SaIP4sOh2QkwPMty148SDU9aiL7kF2WjJu3szKg0ZGaq9/FJgs0cqctS5F2DrpnOBkMGHA29/C\nrZFFlCWRDgoCQkOZO3moMs+3Y6cnpu63/PDFF8ALnDsxMJDVMP/8c3mR0VMRbVMVL+D334EGDRSR\nNhuQcYEJixvncXUpACIuZWPeLmC6OrhX4ThXvSsuDujWjdVu376dCfapU7h4JhK33k4AGjVipVUl\nt7jFbd5RH4Dk2cCb+eFs3cyZQLKldOfp01gWUxv17gEfHLT9el4ghjUdJgdImfMBfbIlv9vBCBQV\noVomEz+/dPb9/UP7Y/bzs7G+z3p80P4D7RQ/jWIx1mlQEpJQHk9i96RxtcaalrSUv518PxlpzkDH\nIaz8qGRJaxWb4Y+j+d1coRDevbygywIs6LIAXwxdq95B+h3k21rYVQxVcC3tmnyOPJn5mUjMTERW\nfhaeq/scwmuE49Pnyq8H9KOERJogngR69QLOnHnk898qpBcil1ctuVo1GxV06cIE+tlngVOngP/9\nD6hWDecWAae+BroKQUxgmzRhx9TrmccBUKLQuZQxk1nJWzXWqM1EtFcvWaSn9/JEagZ7GfMlWPXF\nABYswIQjwIcHHvCaoy1VvcLCEPzNZlR392GlXO/fZwFuR48qbUE7d0a1bED47Tfg8GHg3/9WjpOZ\niWZHruPqAiD4LhC1Rf01srv7yBE4LF4iB5CZ84CQH09i0Q/AyS9ygO3b8ZLQAJN/AXZudgEOHoTD\nnr34R9t/lJxyWLUq5u0ChqXVg7HYqh73zp2sCI6lO5oklJJIN/FuoirHKaUVSt93K+sWjlkM2bs5\nd+VSpH0b98WPAzhvRRngLWk+oj7SPxLjW423mffnc+mtcXN2s4lG55FyvF9t+CpiRseo0tBKJTsb\nuGf/2OUJiTRBEIwCyxuSy8uWos412/M1aQL8weUhCwJgMKBRiqU/dyNLpSwuH1pOh5I6e3E5zEYn\nJe/YVNeyvF8/WaR3uSZhbRM2J24oAjpce8Dr0+omJuWP//Yb4GFJY5KKuqxfD4wZo2zbsSOzWHfv\nBpYvV5ZLA4/MTDbn//rrGHIS6JOpWJrmxBTWGrV1a2DsWLgncfPcJ09ibDRQLxVAbCx091Lx8T4g\nIMfIBkAvavRdj48HPv4YKLIMngwGTDgCfDv/GlpdZ8tk67JbNyAxUe6aJgnliaQT8HD2QA3XGioL\nWEoBlCzqRdGL8AvXQjk70xKB72hC5/qdVe07SysuwlvSfm5++Oz5z9DatzUr+ZnO9l23Dtj3Nza/\njRz7DTTs1fiXOHWLBeSFuNQtcTtNmjdn7WwrASTSBEEwDJYXqK9S4UrKr7VXT71EPv8c+PZblmZm\njWSdcnPnfClV47+nsA+hoTCNn6ja1S0XQO/e2Pc/QJzKreCj2f+wbWqBTp1sl8XFsVxxI+cabteO\nzUuvWSMHhgFgnzt3ZqVWly5lkfRTp6obmNSqJXs7fFMUK9A8Y45qTt7jHnMZO1gHmicmsoh5QF0p\nLtOqalmrViywb98+FjQopbWJIr7bCAw8BYxvNZ55XySysoBPPoFzPMtlz8rPQmPvxhAEQbM9oyTS\nUSejMMMSn1Y9C8gZyObjpd/XhTcv4PUQdu/tVYQDAPz8M5p4KN3ITI4m/LPtP3F4+GE2GLT0T+9z\nHuhgsgzwShBp637Z1uy+ylImm3ToW+J2mki93LNLuJ5yglKwCIJg9OvHXvZSHW+w3NFlJ5ahT6M+\nZT/OokXMTenpybpxadGKNZWA0cjEpnFjmPK/lVeb/BULO6NZYyBB2dUtD2wOfMMGZeEbb7BOYG+8\nwYKMfG1LaSI8nEUKm0ysO9jHlhKYda0sLZ2OWf+zZ6sj0T082PEvXmTu+mnTmLeA7+tdv748T+17\nPRWwjEHMJy+ovkLKJ0+XYrt692bXc/OmMv/KC/Mff7Ae5NeuAQsWALcsqVHbtrGfOWpmAcs3A9C5\nqu/RoUPAunWo3wCAJe6tiUcwUFwMgc8aSE8Hbt9GDXMNWJPpBOTcY9dnKmL7BFcNxuCwwfjuzHc2\n28ucPAl06oR327ZB4qS3VO51mQvcPbIE66lEOidHNZgqzZLeF7cP9e4BHim2ZVnLTEoKq45XgZAl\nTRAEw9GRNdvw95cX1XarjWkdp6mKrZTK2LHa1jPA3MorVgBu3Et62jTgtdfkQCcHwQGODorLvb1f\ne9UhPBqEM8GVuHiRiS4AtG/PrHPeCp0xg82HN2/O1tWooXZjc9erugYAyOCKZbi7M/fziRPMFS65\n8/na6h07ypZ097NKsJPZampVCnxL8zAyb8OqVcy9ylvSvEj36MEi51esUNzygDqC3Zrz55lISx6S\nTZsAAE251OfGM7+Vo/iP/G0Lzn9puc7gYITfdcRn95ojOq0PflwO9DsDZDtBbq5ivJnMUujOnUNd\ndzbQCaseZnseoihbprpDhzHfox+mr01mc+U8l7jCKVu2sIA9XqStitPw7W3t0VPDofJASIGBUVHK\noK6cIZEmCKL8iIgABgzQXCW51B0dHFV5rIFegSj+SEn3eS6oi7puee3a2jXTe/Zk88AffMACwXiL\nmS+TqjVXXaeOumQqYL8gDG+1N20qp7IF3wXevdcAQXcgl1SVaM9SvxHe9lXmbXByYq7ymzcVK5kn\nKQlo0UJJW5OuRXLLajF9OhPqUaOYBWpJ36vDVQJtnAw5ba5VuhkNuXRkh2+W4Z8LjqPF/61H51jA\ny3INiZYYOOOK1awYzYgRCPQKxN5Xt2JPbDtlkAEwyz84mHlpJFq3Zi764cNZHITkObAEtgFgXowu\nXdQinZbGBP+VV4Cvv0YjT8V1bo00T/7yBbublA3JkzJsmKpuQHlC7m6CICoFkiUtdWji4UV7YNhA\noKrS01s1n8yzhQuxtu4E5uDAOoE5O6vc+yr8/BTBHDdObcHyODsz4alfn7nKuQj8OYaX8LmxD4Qx\n94A332RpW88/j9GenvBvoEenutw8uY8Py+fW23ktr1nDrvXnn4EOHYCQEMUtDLDByg2lPCk2b2YD\nho8+YvPhsbEAAH440zgFzNJOT2dTATy7dql+9LToZULPDkD2fpgSLQJWUACkpKDTjovA54uAVZtZ\nfXaTiRWUuXxZ+3qSklh2QFwcmy+/ZhUJeOyYek44NZWl7W3eDJw6hYYbv9A+LoCEpa7YUy0T7eLt\nblI2UlKUgEqADfZcbOfvHyck0gRBVAqktp+NqzXWXL9v8D6cSDqh9FcOCACuXHmwzmM8X9h/yQNg\n1vSxY0CbNqoe4pocPqx85tPkfHwgvP228rMlKlwA0MX6GLVqsWjtIqt0t5AQ1mhFSluTPBHe3mqR\nDg5mIu3nx6K/ARaBX7UqE0GOvf8Djga7wLtbN+a61/ISXFLX7fayiHSicwGQDRhvsfxuODqqC9Pc\nvMm8C9Wr2/c+1K3LRFmyntetY5+dnNQ50bxIt2+vVNKLjUXDS7ZFYyRq3czE4Jvcgrw8xe0vMWUK\n+66ZM9XL+bKxKSnsGZM4e5Z5NHQPMP3zJyF3N0EQlYL32ryHk6NPYmf/nZrrO/h3wMTWXKT36dPq\nOeNHjVQH3aqLVqlYiXSZ4Su38WlXMTHAxIm227dpo/7ZxYWJGh/RLQU9SXOrvXsDADpdA/7tPxB4\n9dWSz6mxMmCSLWmRpUrJFeKOHNHe9/ZtFvBmPW0AsIEHz5IlzGvBxxoArG+6hCiqYh28Vm0q+dx5\n1qxhxXYkq1gUmZt/1iy1pQyof9/JyeqAtogIdX58OUAiTRBEpUAQBITVCENtt9pl28FofLzFXaRj\n379f8nb29gMUoS8Lw7kmLpIADx1q32qbP59Zn+0tgXUmE7snfJtSKagtKoq5yHmrvlEjFpDGC6E1\nUgMXKCIdncqKhJyvBti2KtGAD9KTaGzlLTl6lP3PD1QAFqTHc/268nnHDqxbB/QoYVpeZsgQ1gVt\n3z7mbeDjIs6eZRHy3bsz8ea9DikpbD3PV1+hPCF3N0EQhBZSmdQHzZV9WEvaaGSBYBkZzHXt6QmM\nGGF/+ypV2D9pDltrbl4S6SFD2D9RZOeUmMiE0mhkUcshIUBfjXziFi3kj4JVX5KhPYGPnwV2r7T0\nFrfH4MFsbjwsjEWDA+wcPDxs24nyIq21XqJ9e+DgQfQ5D2Q5Ad9bSrcnzgUcAxsAsKPcWoVhfvtN\nGbzwwWsAc93v26de5lZyfvajhixpgiAILaTKY5MmPdh+vEjXrGl/Oy2Cg4GWLZn4jhunVDMrCalC\nnEYLTlV6GMDm70+dYk1YOnRQlms1ZAFk97MIYMJIXzgIimQU6IGrVQV0GQiIn37K2qVaC9ro0czl\nPnGiupiMt7cyuOAHMr16KZ8/+0z5/PbbwN+5piacJdzWEiv3zfdArUygWvsX1Q1VSmPOHOXz8uXK\n7x1gKW7Xr6u7x5Wl7esjhCxpK/IK81TRpY4OjjA6GpFTkKNqSG7QGWDQG3A//76qrrGz3hlOOidk\n5WfZtJbTO+htmpS7OLrAQXBAZr464d7sZEaxWKxqYA6wBP7C4kJVZR8HwQGuTq7IL8pX9XrVCTq4\nOLnQNdE10TU9zDUFByA36y5zN+dllPmaBCfADED09obg5PTYr6lYJ8ARQH5uNnLzMlRFPu6bnVGU\nl6H+Pbk6AgP6AAVZyjXVrgWt3lU5dXxgBHDID7iWf1t1vgBQKIiIre6EQwOeRTs/q7naW7dsOp3J\neHsrAX89erBCNG3bsmCzWbPYNAHvGejXj5WQrVWLBZ116yavCroL5KeNh+MJS3Cfj0/Z2sd6ebFc\ndD6qfOpU5XPbtqwAjJsb8OmnrHAMULaB0yOERNqKmb/OxLRflBZ9w5sOx7IeyzB+53h8G6NURJoS\nOQVTO0zFK+tewY9XlYIC37z0DUY0G4FWy1rhfMp5efmu/rvwYsCL8J3nq/pjOzvmLGq71YbbLLUL\nJf1f6biRfgNNvm4iLzM7mZHxQQZ+iv0JXVYpsaGNqjXCubHnsPzUcozcNlJe/kL9F7B7wG66Jrom\nuqZyvCZjPpAN4JabA2oCj/2aLl3bjZcBbDq9FqPm7UDGBxk4O2MCnD//AmF7X0T2gTJc0/klmMod\nO8XbFdVWbMT4H9/BMgBXPIGCwgJN36uTgxOu3LuCdn7t2ILTp1nAVfUSGoLwrTUDA1mAmZRv/r6l\ny9dPPynbGI3M/T93rrIsLIx5Be7ehaOHB/B/FpH29dWOVdizB+jfn52bmxsLbGvWTCmpas3Agcyb\n0a6d2sVtzwX/mBBEUctH8uTg6+uLhISE0jcsI0/0yP+vaM3QNdE1PWnXJIowV/NF8Qudodu46fFb\n0n1eheOmrSh4uQdyVq946Gsq/GUf9Fu3wbDwK+Qv+QpOo8YgpyAHDlOmInrVbHQa4Yj8YqtIaLA2\npj8N+kkR6ZKQItBv32bz4MnJLBXunXdstz1xglWJA1g6GNeMBQDLl967F/jyS6WfOgAcOMDc/NaV\n5ESR9Rp/910m4hkZzIrfto3NfzduzILGTrJ2qFi1SqkHL4qqFq54+WVW/vZBpzPsUJKOkUgTBEE8\naqKjWepR7TJGqv8Z+vcHvvuOCc7WrX/uWKLIUo4aNlTln4uiiIaLGuLqvasoFJWOaHpBjwCvAJwf\ne15VcMYucXHMcu3RgzUr2buXnTtfkYzfVqqsduOGdj12Hun7Y2PZnD7fY9vFhQnw0qVsntzTE7h7\nFxg/nol8rVost7y4WCl8s3UrO0+Jy5dZ9bb9+9nP2dn2C+k8ICXpGAWOEQRBPGpatiwfgQYU1669\nSmUPgiCw1CwrwRUEAbsH7EZ9z/pw0jnB1dEVTjonBHgFYPeA3WUTaIBZt5LwrVnDBFIrqhxQV3h7\nEDHkOpEBYLnhUipX375AZKQymJFc8jod+8e1abWZ1w4MBBpYwsirVXtkAl0aNCdNEATxJPPpp6ys\n57x5j/Vr6rjXwYU3L+DQjUO4cu8KAjwD0LZ227ILtDVeXqxUqj14oS2LIP76K/MCWFcWmzVLqdbm\n5qZYwoASEe+gYa/m2ZanlXPWraPmHyNkSRMEQTzJeHkBq1c/svnRkhAEAe382mFI+BC082v38AJd\nFnjhLEtEddu22nnlUr67FgMHMlf44sXKslmz2P+hobbbSy7pchRpmpMmCIIgKifSIOBhZEraNy3t\nwQqQSN+lNQDZuZOlfx08yKK+HxEl6Ri5uwmCIIi/LiVZ0lqU5B3o2pU1QNFyjz8mSKQJgiCIysny\n5YqL+WHhg8EeBeUo0ACJNEEQBFFZGTiwos+gwiGRJgiCIP56xMezqPcnHBJpgiAI4q9H7drll6v+\nGHnszvXLly+jTZs2CAoKQsuWLXHu3Dmbbfbv3w+j0Yjw8HD5X05OzuM+NYIgCIKo1Dx2S3r06NEY\nNWoUhgwZgg0bNmDIkCGIjo622S44OBgnpZqpBEEQBEE8Xks6OTkZv//+OwZYen+++uqruHHjBq5c\nufI4v5YgCIIg/hI8VpG+ceMGatasCb2lpqwgCPDz80N8fLzNtlevXkWzZs3QsmVLfPXVV3aPOW/e\nPPj6+sr/srKyHtv5EwRBEERFUikCx5o1a4aEhAS4ubkhISEB3bp1Q9WqVfHaa6/ZbDtx4kRMnDhR\n/tm3tM4oBEEQBPGE8lgt6dq1ayMpKQmFhay1mSiKiI+Ph59V3dMqVarAzVK2zdfXF/369cPBgwcf\n56kRBEEQRKXnsYq0t7c3mjVrhpUrVwIANm7cCF9fXwQEBKi2S0pKQnExa36emZmJ7du3o2nTpo/z\n1AiCIAii0vPYU7CWLFmCJUuWICgoCLNmzUJUVBQAYMSIEfj+++8BMPEOCQlBWFgYIiIi0LlzZwwd\nOvRxnxpBEARBVGqoCxZBEARBVCAl6Rj1kyYIgiCISgqJNEEQBEFUUkikCYIgCKKSQiJNEARBEJUU\nEmmCIAiCqKSQSBMEQRBEJYVEmiAIgiAqKSTSBEEQBFFJeeKLmRgMBlSrVu2RHCsrKwuurq6P5FhP\nG3TvHh66dw8P3buHh+7dw/Oo711KSgry8vI01z3xIv0ooeplDw/du4eH7t3DQ/fu4aF79/CU570j\ndzdBEARBVFJIpAmCIAiikqKbOnXq1Io+icpE69atK/oUnljo3j08dO8eHrp3Dw/du4envO4dzUkT\nBEEQRCWF3N0EQRAEUUkhkSYIgiCISgqJNIDLly+jTZs2CAoKQsuWLXHu3LmKPqVKxVtvvQV/f38I\ngoCTJ0/Ky0u6b3RPgdzcXPTq1QtBQUEICwtD586dceXKFQBAcnIyunTpgsDAQDRp0gQHDhyQ9ytp\n3dPECy+8gNDQUISHh6N9+/aIiYkBQM/dgxAVFQVBELBlyxYA9NyVFX9/fwQHByM8PBzh4eFYu3Yt\ngAp69kRC7NixoxgVFSWKoiiuX79ebNGiRcWeUCXjl19+EW/cuCHWqVNHjImJkZeXdN/onopiTk6O\n+MMPP4jFxcWiKIriwoULxcjISFEURXHo0KHilClTRFEUxWPHjok+Pj5ifn5+qeueJlJTU+XPmzZt\nEkNDQ0VRpOeurFy7dk1s3bq1GBERIW7evFkURXruyor1u06iIp69p16kb9++LZrNZrGgoEAURVEs\nLi4Wq1evLl6+fLmCz6zywT+4Jd03uqfaREdHi3Xq1BFFURRdXFzEpKQkeV3Lli3FPXv2lLruaSUq\nKkoMCwuj566MFBUViZ06dRJ///13MTIyUhZpeu7KhpZIV9Sz99S7u2/cuIGaNWtCr9cDAARBgJ+f\nH+Lj4yv4zCo3Jd03uqfazJ8/Hz179sTdu3dRUFCAGjVqyOv8/f0RHx9f4rqnkUGDBqF27dr48MMP\nsWLFCnruysi8efPQtm1bNG/eXF5Gz92DMWjQIISEhGD48OFISUmpsGfvqRdpgigPZsyYgStXrmDm\nzJkVfSpPFMuXL8eNGzfwySef4P3336/o03kiOHv2LDZu3IjJkydX9Kk8sRw4cACnT5/GiRMnULVq\nVQwePLjCzuWpF+natWsjKSkJhYWFAABRFBEfHw8/P78KPrPKTUn3je6pmjlz5mDTpk3YuXMnTCYT\nvLy8oNfrcevWLXmbuLg4+Pn5lbjuaWbw4MHYt28ffH196bkrhYMHDyIuLg6BgYHw9/fHkSNHMGrU\nKKxbt46euzIiXbejoyPeeecdHDx4sMLeeU+9SHt7e6NZs2ZYuXIlAGDjxo3w9fVFQEBABZ9Z5aak\n+0b3VGHevHlYvXo19uzZA3d3d3l5nz59sHjxYgBAdHQ0EhMTERkZWeq6p4W0tDTcvHlT/nnLli3w\n8vKi564MjBkzBklJSYiLi0NcXBwiIiKwdOlSjBkzhp67MnD//n2kpaXJP69evRpNmzatuGfvT89q\n/wW4ePGiGBERIQYGBorNmzcXT58+XdGnVKkYNWqU6OPjI+p0OtHb21usX7++KIol3ze6p6J448YN\nEYBYr149MSwsTAwLCxOfeeYZURRF8datW2Lnzp3FgIAAsVGjRuLPP/8s71fSuqeFuLg4sWXLlmKT\nJk3E0NBQsVOnTnIgDz13DwYfOEbPXelcvXpVDA8PF0NCQsQmTZqIPXr0EK9duyaKYsU8e1QWlCAI\ngiAqKU+9u5sgCIIgKisk0gRBEARRSSGRJgiCIIhKCok0QRAEQVRSSKQJgiAIopJCIk0QBEEQlRR9\nRZ8AQRCPF39/fxgMBhiNRnnZihUrEBIS8si+Iy4uDuHh4aoiEARB/HlIpAniKWDt2rUIDw+v6NMg\nCOIBIXc3QTylCIKAyZMno2nTpggKCsKqVavkdbt370azZs0QGhqKyMhInD9/Xl4XFRWF8PBwhIWF\noUWLFoiLi5PXTZkyBc2bN0dAQAB27NhRnpdDEH9JyJImiKeAvn37qtzdv/32GwAm1DExMYiNjUWL\nFi3Qtm1bmEwmvP7669i/fz9CQkKwatUq9O7dG+fOncMvv/yC6dOn4/Dhw6hZsyays7MBAMnJyUhP\nT0doaCimTZuGXbt24e2330a3bt0q5HoJ4q8ClQUliL84/v7+2LJli427WxAExMXFoU6dOgCAXr16\n4ZVXXoGHhwfmzp2L/fv3y9u6u7vj7NmzmD9/PoxGI6ZPn646VlxcHBo2bIjs7GwIgoD09HR4eXnJ\nXYEIgng4yN1NEISMIAgPva/BYJD31+l0KCoqelSnRRBPLSTSBPEUExUVBYBZwgcPHkT79u0RERGB\nM2fO4OzZswCANWvWwMfHBz4+PnjppZewcuVKJCUlAQCys7NllzdBEI8empMmiKcA6znpL774AgBQ\nVFSEpk2b4v79+1iwYAH8/f0BAKtWrcKgQYNQWFgIDw8PrF+/HoIg4Nlnn8WUKVPw4osvQhAEODk5\nYcOGDRVxSQTxVEBz0gTxlCIIAlJTU+Hu7l7Rp0IQhB3I3U0QBEEQlRRydxPEUwo50Qii8kOWNEEQ\nBEFUUkikCYIgCKKSQiJNEARBEJUUEmmCIAiCqKSQSBMEQRBEJYVEmiAIgiAqKf8PtldtZVxX3pgA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 560x560 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}