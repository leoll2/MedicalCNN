{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Dual_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm7a-NPsSHGI",
        "colab_type": "text"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POc2zP_gR6Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfBvIGyoSOd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the dataset from Google Drive to local\n",
        "\n",
        "!cp \"/content/gdrive/My Drive/CBIS_DDSM.zip\" .\n",
        "!unzip -qq CBIS_DDSM.zip\n",
        "!rm CBIS_DDSM.zip\n",
        "cbis_path = 'CBIS_DDSM'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4kXIU2SO8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam, Nadam\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdf2u-HJSYi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_training():\n",
        "    \"\"\"\n",
        "    Load the training set\n",
        "    \"\"\"\n",
        "    abnorm_patches = np.load(os.path.join(cbis_path, 'numpy data', 'train_tensor.npy'))[1::2]\n",
        "    base_patches = np.load(os.path.join(cbis_path, 'numpy data', 'train_tensor.npy'))[0::2]\n",
        "    labels = np.load(os.path.join(cbis_path, 'numpy data', 'train_labels.npy'))[1::2]\n",
        "    return abnorm_patches, base_patches, labels\n",
        "\n",
        "\n",
        "def load_testing():\n",
        "    \"\"\"\n",
        "    Load the test set (abnormalities patches and labels, no baseline)\n",
        "    \"\"\"\n",
        "    abnorm_patches = np.load(os.path.join(cbis_path, 'numpy data', 'public_test_tensor.npy'))[1::2]\n",
        "    base_patches = np.load(os.path.join(cbis_path, 'numpy data', 'public_test_tensor.npy'))[0::2]\n",
        "    labels = np.load(os.path.join(cbis_path, 'numpy data', 'public_test_labels.npy'))[1::2]\n",
        "    return abnorm_patches, base_patches, labels\n",
        "\n",
        "\n",
        "def remap_label(l):\n",
        "    \"\"\"\n",
        "    Remap the labels to 0->mass 1->calcification\n",
        "    \"\"\"\n",
        "    if l == 1 or l == 2:\n",
        "        return 0\n",
        "    elif l == 3 or l == 4:\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"[WARN] Unrecognized label (%d)\" % l)\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmb9AI_SelP",
        "colab_type": "code",
        "outputId": "a9764335-bc1f-4608-e8e7-6a1c7fbc91a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load training and test images (abnormalities only, no baseline)\n",
        "train_abn_images, train_base_images, train_labels= load_training()\n",
        "test_abn_images, test_base_images, test_labels= load_testing()\n",
        "\n",
        "# Number of images\n",
        "n_train_img = train_abn_images.shape[0]\n",
        "n_test_img = test_abn_images.shape[0]\n",
        "print(\"Train size: %d \\t Test size: %d\" % (n_train_img, n_test_img))\n",
        "\n",
        "# Compute width and height of images\n",
        "img_w = train_abn_images.shape[1]\n",
        "img_h = train_abn_images.shape[2]\n",
        "print(\"Image size: %dx%d\" % (img_w, img_h))\n",
        "\n",
        "# Remap labels\n",
        "train_labels = np.array([remap_label(l) for l in train_labels])\n",
        "test_labels = np.array([remap_label(l) for l in test_labels])\n",
        "\n",
        "# Create a new dimension for color in the images arrays\n",
        "train_abn_images = train_abn_images.reshape((n_train_img, img_w, img_h, 1))\n",
        "train_base_images = train_base_images.reshape((n_train_img, img_w, img_h, 1))\n",
        "test_abn_images = test_abn_images.reshape((n_test_img, img_w, img_h, 1))\n",
        "test_base_images = test_base_images.reshape((n_test_img, img_w, img_h, 1))\n",
        "\n",
        "# Convert from 16-bit (0-65535) to float (0-1)\n",
        "train_abn_images = train_abn_images.astype('uint16') / 65535\n",
        "train_base_images = train_base_images.astype('uint16') / 65535\n",
        "test_abn_images = test_abn_images.astype('uint16') / 65535\n",
        "test_base_images = test_base_images.astype('uint16') / 65535\n",
        "\n",
        "# Shuffle the training set (originally sorted by label)\n",
        "perm = np.random.permutation(n_train_img)\n",
        "train_abn_images = train_abn_images[perm]\n",
        "train_base_images = train_base_images[perm]\n",
        "train_labels = train_labels[perm]\n",
        "\n",
        "def double_generator(train_abn_images, train_base_images, train_labels, subset, batch_size=128):\n",
        "\n",
        "    gen = ImageDataGenerator(\n",
        "        validation_split=0.2,\n",
        "        rotation_range=180,\n",
        "        shear_range=15,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='reflect'\n",
        "    )\n",
        "\n",
        "    gen.fit(train_abn_images)\n",
        "\n",
        "    gen_abn = gen.flow(train_abn_images, train_labels,  batch_size=batch_size, subset=subset, seed=1)\n",
        "    gen_base = gen.flow(train_base_images, train_labels, batch_size=batch_size, subset=subset, seed=1)\n",
        "\n",
        "    while True:\n",
        "        abn_img, abn_label = gen_abn.next()\n",
        "        base_img, _ = gen_base.next()\n",
        "        yield [abn_img, base_img], abn_label\n",
        "\n",
        "\n",
        "train_generator = double_generator(train_abn_images, train_base_images, train_labels, 'training', batch_size=128)\n",
        "validation_generator = double_generator(train_abn_images, train_base_images, train_labels, 'validation', batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: 2676 \t Test size: 336\n",
            "Image size: 150x150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1WX92c2ZRBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Create a model with double input '''\n",
        "\n",
        "def create_2ch_cnn():\n",
        "\n",
        "    # Two input channels\n",
        "    abnorm_input = layers.Input(shape=(150, 150, 1))\n",
        "    baseline_input = layers.Input(shape=(150, 150, 1))\n",
        "\n",
        "    # First branch (abnormality patches)\n",
        "    x = layers.Conv2D(32, (3,3), activation=\"relu\")(abnorm_input)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    x = layers.Conv2D(64, (3,3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    x = layers.Conv2D(128, (3,3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2,2))(x)\n",
        "    abnorm_model = models.Model(inputs=abnorm_input, outputs=x)\n",
        "\n",
        "    # Second branch (baseline patches)\n",
        "    y = layers.Conv2D(32, (3,3), activation=\"relu\")(baseline_input)\n",
        "    y = layers.MaxPooling2D((2,2))(y)\n",
        "    y = layers.Conv2D(64, (3,3), activation=\"relu\")(y)\n",
        "    y = layers.MaxPooling2D((2,2))(y)\n",
        "    y = layers.Conv2D(128, (3,3), activation=\"relu\")(y)\n",
        "    y = layers.MaxPooling2D((2,2))(y)\n",
        "    baseline_model = models.Model(inputs=baseline_input, outputs=y)\n",
        "\n",
        "    # Merge the two branches\n",
        "    combined = layers.concatenate([abnorm_model.output, baseline_model.output])\n",
        "\n",
        "    # FC layer\n",
        "    z = layers.Flatten()(combined)\n",
        "    z = layers.Dense(96, activation=\"relu\")(z)\n",
        "    z = layers.Dropout(0.5)(z)\n",
        "    z = layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = models.Model(inputs=[abnorm_model.input, baseline_model.input], outputs=z)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPlkBpF2u5g0",
        "colab_type": "code",
        "outputId": "529b1596-3f9a-4e75-a668-4f11ac30238f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "two_ch_cnn = create_2ch_cnn()\n",
        "two_ch_cnn.summary()\n",
        "plot_model(two_ch_cnn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 150, 150, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 150, 150, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 148, 148, 32) 320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 148, 148, 32) 320         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 74, 74, 32)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 74, 74, 32)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 72, 72, 64)   18496       max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 72, 72, 64)   18496       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 36, 36, 64)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 36, 36, 64)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 34, 34, 128)  73856       max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 34, 34, 128)  73856       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 17, 17, 128)  0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 17, 17, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 17, 17, 256)  0           max_pooling2d_8[0][0]            \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 73984)        0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 96)           7102560     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 96)           0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            97          dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,288,001\n",
            "Trainable params: 7,288,001\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnsAAARrCAYAAAD7MmA2AAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n4sP/B1mHYVXOYvB7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDPauoU2yIS7",
        "colab_type": "code",
        "outputId": "af40e84e-ff40-4f6d-a856-5d7f0d42aacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Early stopping (stop training after the validation loss reaches the minimum)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=80, verbose=1)\n",
        "\n",
        "# Callback for checkpointing\n",
        "checkpoint = ModelCheckpoint('two_ch_cnn_best.h5', \n",
        "        monitor='val_loss', mode='min', verbose=1, \n",
        "        save_best_only=True, save_freq='epoch'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "two_ch_cnn.compile(optimizer=RMSprop(learning_rate=0.001, decay=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history_two_ch_cnn = two_ch_cnn.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=n_train_img // 128,\n",
        "        epochs=500,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps = n_train_img // 128 // 5,\n",
        "        callbacks=[checkpoint, earlystopping],\n",
        "        shuffle=True,\n",
        "        verbose=1,\n",
        "        initial_epoch=0\n",
        ")\n",
        "\n",
        "# Save\n",
        "models.save_model(two_ch_cnn, 'two_ch_cnn_end.h5')\n",
        "!cp model* \"/content/gdrive/My Drive/models/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 1.2934 - acc: 0.5190Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 12s - loss: 0.6857 - acc: 0.5352\n",
            "Epoch 00001: val_loss improved from inf to 0.68567, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 14s 677ms/step - loss: 1.2634 - acc: 0.5208 - val_loss: 0.6857 - val_acc: 0.5352\n",
            "Epoch 2/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.7432 - acc: 0.5444Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6804 - acc: 0.5371\n",
            "Epoch 00002: val_loss improved from 0.68567 to 0.68042, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 540ms/step - loss: 0.7401 - acc: 0.5485 - val_loss: 0.6804 - val_acc: 0.5371\n",
            "Epoch 3/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6765 - acc: 0.5544Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 12s - loss: 0.6870 - acc: 0.5000\n",
            "Epoch 00003: val_loss did not improve from 0.68042\n",
            "20/20 [==============================] - 11s 559ms/step - loss: 0.6863 - acc: 0.5545 - val_loss: 0.6870 - val_acc: 0.5000\n",
            "Epoch 4/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6872 - acc: 0.5390Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 12s - loss: 0.6845 - acc: 0.5430\n",
            "Epoch 00004: val_loss did not improve from 0.68042\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.6860 - acc: 0.5430 - val_loss: 0.6845 - val_acc: 0.5430\n",
            "Epoch 5/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6769 - acc: 0.5574Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6940 - acc: 0.5254\n",
            "Epoch 00005: val_loss did not improve from 0.68042\n",
            "20/20 [==============================] - 11s 534ms/step - loss: 0.6765 - acc: 0.5580 - val_loss: 0.6940 - val_acc: 0.5254\n",
            "Epoch 6/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6805 - acc: 0.5533Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6763 - acc: 0.5293\n",
            "Epoch 00006: val_loss improved from 0.68042 to 0.67627, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 534ms/step - loss: 0.6803 - acc: 0.5574 - val_loss: 0.6763 - val_acc: 0.5293\n",
            "Epoch 7/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6743 - acc: 0.5682Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6794 - acc: 0.6484\n",
            "Epoch 00007: val_loss did not improve from 0.67627\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 0.6734 - acc: 0.5667 - val_loss: 0.6794 - val_acc: 0.6484\n",
            "Epoch 8/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6755 - acc: 0.5761Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6631 - acc: 0.5781\n",
            "Epoch 00008: val_loss improved from 0.67627 to 0.66315, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 543ms/step - loss: 0.6761 - acc: 0.5798 - val_loss: 0.6631 - val_acc: 0.5781\n",
            "Epoch 9/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6616 - acc: 0.6416Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6432 - acc: 0.6230\n",
            "Epoch 00009: val_loss improved from 0.66315 to 0.64322, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 545ms/step - loss: 0.6637 - acc: 0.6321 - val_loss: 0.6432 - val_acc: 0.6230\n",
            "Epoch 10/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.9086 - acc: 0.6533Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6766 - acc: 0.5391\n",
            "Epoch 00010: val_loss did not improve from 0.64322\n",
            "20/20 [==============================] - 11s 538ms/step - loss: 0.8956 - acc: 0.6495 - val_loss: 0.6766 - val_acc: 0.5391\n",
            "Epoch 11/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.6316 - acc: 0.6529Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6004 - acc: 0.6406\n",
            "Epoch 00011: val_loss improved from 0.64322 to 0.60037, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.6280 - acc: 0.6582 - val_loss: 0.6004 - val_acc: 0.6406\n",
            "Epoch 12/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5970 - acc: 0.6971Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.6177 - acc: 0.6738\n",
            "Epoch 00012: val_loss did not improve from 0.60037\n",
            "20/20 [==============================] - 11s 532ms/step - loss: 0.5979 - acc: 0.6998 - val_loss: 0.6177 - val_acc: 0.6738\n",
            "Epoch 13/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5746 - acc: 0.7071Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.5555 - acc: 0.7227\n",
            "Epoch 00013: val_loss improved from 0.60037 to 0.55553, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 534ms/step - loss: 0.5742 - acc: 0.7065 - val_loss: 0.5555 - val_acc: 0.7227\n",
            "Epoch 14/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5996 - acc: 0.7159Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.5859 - acc: 0.6738\n",
            "Epoch 00014: val_loss did not improve from 0.55553\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.6039 - acc: 0.7109 - val_loss: 0.5859 - val_acc: 0.6738\n",
            "Epoch 15/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5309 - acc: 0.7610Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.5109 - acc: 0.7617\n",
            "Epoch 00015: val_loss improved from 0.55553 to 0.51090, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 540ms/step - loss: 0.5274 - acc: 0.7640 - val_loss: 0.5109 - val_acc: 0.7617\n",
            "Epoch 16/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5392 - acc: 0.7388Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4870 - acc: 0.7988\n",
            "Epoch 00016: val_loss improved from 0.51090 to 0.48696, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 0.5371 - acc: 0.7394 - val_loss: 0.4870 - val_acc: 0.7988\n",
            "Epoch 17/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5029 - acc: 0.7671Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.5209 - acc: 0.7891\n",
            "Epoch 00017: val_loss did not improve from 0.48696\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.4994 - acc: 0.7695 - val_loss: 0.5209 - val_acc: 0.7891\n",
            "Epoch 18/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.5042 - acc: 0.7580Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4557 - acc: 0.8105\n",
            "Epoch 00018: val_loss improved from 0.48696 to 0.45574, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 538ms/step - loss: 0.5011 - acc: 0.7584 - val_loss: 0.4557 - val_acc: 0.8105\n",
            "Epoch 19/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4974 - acc: 0.7685Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4722 - acc: 0.7988\n",
            "Epoch 00019: val_loss did not improve from 0.45574\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.4969 - acc: 0.7679 - val_loss: 0.4722 - val_acc: 0.7988\n",
            "Epoch 20/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4879 - acc: 0.7639Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4409 - acc: 0.8047\n",
            "Epoch 00020: val_loss improved from 0.45574 to 0.44089, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 538ms/step - loss: 0.4872 - acc: 0.7644 - val_loss: 0.4409 - val_acc: 0.8047\n",
            "Epoch 21/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4676 - acc: 0.7826Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4277 - acc: 0.8203\n",
            "Epoch 00021: val_loss improved from 0.44089 to 0.42771, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 543ms/step - loss: 0.4657 - acc: 0.7846 - val_loss: 0.4277 - val_acc: 0.8203\n",
            "Epoch 22/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4515 - acc: 0.7881Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4165 - acc: 0.8242\n",
            "Epoch 00022: val_loss improved from 0.42771 to 0.41652, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.4510 - acc: 0.7877 - val_loss: 0.4165 - val_acc: 0.8242\n",
            "Epoch 23/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4474 - acc: 0.8044Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.9726 - acc: 0.5508\n",
            "Epoch 00023: val_loss did not improve from 0.41652\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.4461 - acc: 0.8020 - val_loss: 0.9726 - val_acc: 0.5508\n",
            "Epoch 24/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4924 - acc: 0.7764Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4053 - acc: 0.8125\n",
            "Epoch 00024: val_loss improved from 0.41652 to 0.40535, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 533ms/step - loss: 0.4853 - acc: 0.7806 - val_loss: 0.4053 - val_acc: 0.8125\n",
            "Epoch 25/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4408 - acc: 0.8110Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4683 - acc: 0.7910\n",
            "Epoch 00025: val_loss did not improve from 0.40535\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.4471 - acc: 0.8063 - val_loss: 0.4683 - val_acc: 0.7910\n",
            "Epoch 26/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4432 - acc: 0.7981Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4007 - acc: 0.8184\n",
            "Epoch 00026: val_loss improved from 0.40535 to 0.40071, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.4432 - acc: 0.7988 - val_loss: 0.4007 - val_acc: 0.8184\n",
            "Epoch 27/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4526 - acc: 0.8048Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4311 - acc: 0.7852\n",
            "Epoch 00027: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 533ms/step - loss: 0.4503 - acc: 0.8044 - val_loss: 0.4311 - val_acc: 0.7852\n",
            "Epoch 28/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4062 - acc: 0.8185Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4458 - acc: 0.7988\n",
            "Epoch 00028: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.4098 - acc: 0.8141 - val_loss: 0.4458 - val_acc: 0.7988\n",
            "Epoch 29/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4132 - acc: 0.8206Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4253 - acc: 0.8027\n",
            "Epoch 00029: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.4157 - acc: 0.8186 - val_loss: 0.4253 - val_acc: 0.8027\n",
            "Epoch 30/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4450 - acc: 0.8018Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4541 - acc: 0.7969\n",
            "Epoch 00030: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.4458 - acc: 0.7988 - val_loss: 0.4541 - val_acc: 0.7969\n",
            "Epoch 31/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4176 - acc: 0.8156Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.5962 - acc: 0.6738\n",
            "Epoch 00031: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.4158 - acc: 0.8162 - val_loss: 0.5962 - val_acc: 0.6738\n",
            "Epoch 32/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3994 - acc: 0.8139Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4198 - acc: 0.8203\n",
            "Epoch 00032: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.3973 - acc: 0.8162 - val_loss: 0.4198 - val_acc: 0.8203\n",
            "Epoch 33/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3947 - acc: 0.8214Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4146 - acc: 0.8066\n",
            "Epoch 00033: val_loss did not improve from 0.40071\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.4006 - acc: 0.8222 - val_loss: 0.4146 - val_acc: 0.8066\n",
            "Epoch 34/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4115 - acc: 0.8120Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3539 - acc: 0.8574\n",
            "Epoch 00034: val_loss improved from 0.40071 to 0.35386, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.4083 - acc: 0.8141 - val_loss: 0.3539 - val_acc: 0.8574\n",
            "Epoch 35/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4063 - acc: 0.8214Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3937 - acc: 0.8457\n",
            "Epoch 00035: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.4048 - acc: 0.8222 - val_loss: 0.3937 - val_acc: 0.8457\n",
            "Epoch 36/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3886 - acc: 0.8323Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3806 - acc: 0.8398\n",
            "Epoch 00036: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.3911 - acc: 0.8289 - val_loss: 0.3806 - val_acc: 0.8398\n",
            "Epoch 37/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4086 - acc: 0.8273Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4297 - acc: 0.8340\n",
            "Epoch 00037: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.4076 - acc: 0.8265 - val_loss: 0.4297 - val_acc: 0.8340\n",
            "Epoch 38/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4157 - acc: 0.8185Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3775 - acc: 0.8438\n",
            "Epoch 00038: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.4185 - acc: 0.8186 - val_loss: 0.3775 - val_acc: 0.8438\n",
            "Epoch 39/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3837 - acc: 0.8373Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3701 - acc: 0.8477\n",
            "Epoch 00039: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 0.3869 - acc: 0.8372 - val_loss: 0.3701 - val_acc: 0.8477\n",
            "Epoch 40/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4036 - acc: 0.8218Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4672 - acc: 0.7949\n",
            "Epoch 00040: val_loss did not improve from 0.35386\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.3983 - acc: 0.8249 - val_loss: 0.4672 - val_acc: 0.7949\n",
            "Epoch 41/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3834 - acc: 0.8373Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3422 - acc: 0.8496\n",
            "Epoch 00041: val_loss improved from 0.35386 to 0.34224, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.3806 - acc: 0.8384 - val_loss: 0.3422 - val_acc: 0.8496\n",
            "Epoch 42/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4351 - acc: 0.8148Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3793 - acc: 0.8379\n",
            "Epoch 00042: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.4305 - acc: 0.8166 - val_loss: 0.3793 - val_acc: 0.8379\n",
            "Epoch 43/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3603 - acc: 0.8515Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4238 - acc: 0.8320\n",
            "Epoch 00043: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3635 - acc: 0.8491 - val_loss: 0.4238 - val_acc: 0.8320\n",
            "Epoch 44/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3806 - acc: 0.8315Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4052 - acc: 0.8418\n",
            "Epoch 00044: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.3853 - acc: 0.8329 - val_loss: 0.4052 - val_acc: 0.8418\n",
            "Epoch 45/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3868 - acc: 0.8298Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4021 - acc: 0.7969\n",
            "Epoch 00045: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 10s 525ms/step - loss: 0.3852 - acc: 0.8313 - val_loss: 0.4021 - val_acc: 0.7969\n",
            "Epoch 46/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4014 - acc: 0.8327Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3881 - acc: 0.8262\n",
            "Epoch 00046: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.4012 - acc: 0.8301 - val_loss: 0.3881 - val_acc: 0.8262\n",
            "Epoch 47/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3655 - acc: 0.8431Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3798 - acc: 0.8457\n",
            "Epoch 00047: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.3723 - acc: 0.8440 - val_loss: 0.3798 - val_acc: 0.8457\n",
            "Epoch 48/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3569 - acc: 0.8452Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3662 - acc: 0.8457\n",
            "Epoch 00048: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.3587 - acc: 0.8440 - val_loss: 0.3662 - val_acc: 0.8457\n",
            "Epoch 49/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3811 - acc: 0.8331Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3671 - acc: 0.8457\n",
            "Epoch 00049: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.3771 - acc: 0.8376 - val_loss: 0.3671 - val_acc: 0.8457\n",
            "Epoch 50/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3715 - acc: 0.8415Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4272 - acc: 0.8203\n",
            "Epoch 00050: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.3784 - acc: 0.8380 - val_loss: 0.4272 - val_acc: 0.8203\n",
            "Epoch 51/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3542 - acc: 0.8501Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4033 - acc: 0.8281\n",
            "Epoch 00051: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 10s 512ms/step - loss: 0.3585 - acc: 0.8494 - val_loss: 0.4033 - val_acc: 0.8281\n",
            "Epoch 52/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3533 - acc: 0.8486Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4098 - acc: 0.8203\n",
            "Epoch 00052: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.3575 - acc: 0.8459 - val_loss: 0.4098 - val_acc: 0.8203\n",
            "Epoch 53/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3552 - acc: 0.8461Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3571 - acc: 0.8574\n",
            "Epoch 00053: val_loss did not improve from 0.34224\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3507 - acc: 0.8475 - val_loss: 0.3571 - val_acc: 0.8574\n",
            "Epoch 54/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3706 - acc: 0.8427Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3227 - acc: 0.8574\n",
            "Epoch 00054: val_loss improved from 0.34224 to 0.32275, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.3682 - acc: 0.8436 - val_loss: 0.3227 - val_acc: 0.8574\n",
            "Epoch 55/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3438 - acc: 0.8644Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3384 - acc: 0.8594\n",
            "Epoch 00055: val_loss did not improve from 0.32275\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.3414 - acc: 0.8650 - val_loss: 0.3384 - val_acc: 0.8594\n",
            "Epoch 56/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3663 - acc: 0.8465Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3484 - acc: 0.8574\n",
            "Epoch 00056: val_loss did not improve from 0.32275\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.3622 - acc: 0.8491 - val_loss: 0.3484 - val_acc: 0.8574\n",
            "Epoch 57/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3566 - acc: 0.8531Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3215 - acc: 0.8652\n",
            "Epoch 00057: val_loss improved from 0.32275 to 0.32154, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.3549 - acc: 0.8534 - val_loss: 0.3215 - val_acc: 0.8652\n",
            "Epoch 58/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3401 - acc: 0.8694Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3117 - acc: 0.8691\n",
            "Epoch 00058: val_loss improved from 0.32154 to 0.31169, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 534ms/step - loss: 0.3347 - acc: 0.8713 - val_loss: 0.3117 - val_acc: 0.8691\n",
            "Epoch 59/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.4289 - acc: 0.8319Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3222 - acc: 0.8652\n",
            "Epoch 00059: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.4209 - acc: 0.8341 - val_loss: 0.3222 - val_acc: 0.8652\n",
            "Epoch 60/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3218 - acc: 0.8719Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3588 - acc: 0.8574\n",
            "Epoch 00060: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.3231 - acc: 0.8701 - val_loss: 0.3588 - val_acc: 0.8574\n",
            "Epoch 61/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3306 - acc: 0.8627Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3246 - acc: 0.8672\n",
            "Epoch 00061: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.3290 - acc: 0.8634 - val_loss: 0.3246 - val_acc: 0.8672\n",
            "Epoch 62/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3160 - acc: 0.8636Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3770 - acc: 0.8379\n",
            "Epoch 00062: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 11s 532ms/step - loss: 0.3229 - acc: 0.8631 - val_loss: 0.3770 - val_acc: 0.8379\n",
            "Epoch 63/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3553 - acc: 0.8444Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3865 - acc: 0.8438\n",
            "Epoch 00063: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.3577 - acc: 0.8444 - val_loss: 0.3865 - val_acc: 0.8438\n",
            "Epoch 64/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3640 - acc: 0.8527Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3289 - acc: 0.8438\n",
            "Epoch 00064: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.3613 - acc: 0.8543 - val_loss: 0.3289 - val_acc: 0.8438\n",
            "Epoch 65/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3252 - acc: 0.8719Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3185 - acc: 0.8594\n",
            "Epoch 00065: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.3279 - acc: 0.8701 - val_loss: 0.3185 - val_acc: 0.8594\n",
            "Epoch 66/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3348 - acc: 0.8652Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3326 - acc: 0.8730\n",
            "Epoch 00066: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.3379 - acc: 0.8614 - val_loss: 0.3326 - val_acc: 0.8730\n",
            "Epoch 67/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3278 - acc: 0.8711Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3239 - acc: 0.8691\n",
            "Epoch 00067: val_loss did not improve from 0.31169\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.3291 - acc: 0.8705 - val_loss: 0.3239 - val_acc: 0.8691\n",
            "Epoch 68/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3270 - acc: 0.8764Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3081 - acc: 0.8828\n",
            "Epoch 00068: val_loss improved from 0.31169 to 0.30814, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3222 - acc: 0.8779 - val_loss: 0.3081 - val_acc: 0.8828\n",
            "Epoch 69/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3296 - acc: 0.8682Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3645 - acc: 0.8457\n",
            "Epoch 00069: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.3290 - acc: 0.8693 - val_loss: 0.3645 - val_acc: 0.8457\n",
            "Epoch 70/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3209 - acc: 0.8723Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4104 - acc: 0.8418\n",
            "Epoch 00070: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3239 - acc: 0.8697 - val_loss: 0.4104 - val_acc: 0.8418\n",
            "Epoch 71/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3459 - acc: 0.8552Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3098 - acc: 0.8711\n",
            "Epoch 00071: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.3400 - acc: 0.8594 - val_loss: 0.3098 - val_acc: 0.8711\n",
            "Epoch 72/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3213 - acc: 0.8594Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3663 - acc: 0.8516\n",
            "Epoch 00072: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.3213 - acc: 0.8618 - val_loss: 0.3663 - val_acc: 0.8516\n",
            "Epoch 73/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3198 - acc: 0.8669Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3159 - acc: 0.8633\n",
            "Epoch 00073: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3238 - acc: 0.8673 - val_loss: 0.3159 - val_acc: 0.8633\n",
            "Epoch 74/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3004 - acc: 0.8776Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3608 - acc: 0.8379\n",
            "Epoch 00074: val_loss did not improve from 0.30814\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.3019 - acc: 0.8771 - val_loss: 0.3608 - val_acc: 0.8379\n",
            "Epoch 75/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3157 - acc: 0.8790Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3027 - acc: 0.8789\n",
            "Epoch 00075: val_loss improved from 0.30814 to 0.30270, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 0.3166 - acc: 0.8768 - val_loss: 0.3027 - val_acc: 0.8789\n",
            "Epoch 76/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3151 - acc: 0.8778Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3190 - acc: 0.8672\n",
            "Epoch 00076: val_loss did not improve from 0.30270\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.3153 - acc: 0.8776 - val_loss: 0.3190 - val_acc: 0.8672\n",
            "Epoch 77/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3129 - acc: 0.8711Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3312 - acc: 0.8672\n",
            "Epoch 00077: val_loss did not improve from 0.30270\n",
            "20/20 [==============================] - 10s 525ms/step - loss: 0.3165 - acc: 0.8713 - val_loss: 0.3312 - val_acc: 0.8672\n",
            "Epoch 78/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3320 - acc: 0.8761Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2921 - acc: 0.8711\n",
            "Epoch 00078: val_loss improved from 0.30270 to 0.29208, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 542ms/step - loss: 0.3288 - acc: 0.8768 - val_loss: 0.2921 - val_acc: 0.8711\n",
            "Epoch 79/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2923 - acc: 0.8840Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3536 - acc: 0.8691\n",
            "Epoch 00079: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 10s 513ms/step - loss: 0.2910 - acc: 0.8855 - val_loss: 0.3536 - val_acc: 0.8691\n",
            "Epoch 80/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3004 - acc: 0.8782Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3183 - acc: 0.8691\n",
            "Epoch 00080: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2993 - acc: 0.8780 - val_loss: 0.3183 - val_acc: 0.8691\n",
            "Epoch 81/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3050 - acc: 0.8740Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3275 - acc: 0.8613\n",
            "Epoch 00081: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.3027 - acc: 0.8764 - val_loss: 0.3275 - val_acc: 0.8613\n",
            "Epoch 82/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3146 - acc: 0.8794Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3030 - acc: 0.8730\n",
            "Epoch 00082: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.3107 - acc: 0.8820 - val_loss: 0.3030 - val_acc: 0.8730\n",
            "Epoch 83/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3190 - acc: 0.8723Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3213 - acc: 0.8848\n",
            "Epoch 00083: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3158 - acc: 0.8745 - val_loss: 0.3213 - val_acc: 0.8848\n",
            "Epoch 84/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3110 - acc: 0.8828Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3221 - acc: 0.8594\n",
            "Epoch 00084: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.3088 - acc: 0.8828 - val_loss: 0.3221 - val_acc: 0.8594\n",
            "Epoch 85/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2917 - acc: 0.8831Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3131 - acc: 0.8730\n",
            "Epoch 00085: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2938 - acc: 0.8819 - val_loss: 0.3131 - val_acc: 0.8730\n",
            "Epoch 86/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2955 - acc: 0.8815Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4765 - acc: 0.7949\n",
            "Epoch 00086: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2969 - acc: 0.8792 - val_loss: 0.4765 - val_acc: 0.7949\n",
            "Epoch 87/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3042 - acc: 0.8844Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3054 - acc: 0.8730\n",
            "Epoch 00087: val_loss did not improve from 0.29208\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.3045 - acc: 0.8851 - val_loss: 0.3054 - val_acc: 0.8730\n",
            "Epoch 88/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3084 - acc: 0.8698Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2880 - acc: 0.8789\n",
            "Epoch 00088: val_loss improved from 0.29208 to 0.28800, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.3082 - acc: 0.8713 - val_loss: 0.2880 - val_acc: 0.8789\n",
            "Epoch 89/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2918 - acc: 0.8861Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3179 - acc: 0.8750\n",
            "Epoch 00089: val_loss did not improve from 0.28800\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2925 - acc: 0.8867 - val_loss: 0.3179 - val_acc: 0.8750\n",
            "Epoch 90/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2798 - acc: 0.8936Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3093 - acc: 0.8730\n",
            "Epoch 00090: val_loss did not improve from 0.28800\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2823 - acc: 0.8911 - val_loss: 0.3093 - val_acc: 0.8730\n",
            "Epoch 91/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2998 - acc: 0.8696Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3214 - acc: 0.8672\n",
            "Epoch 00091: val_loss did not improve from 0.28800\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2962 - acc: 0.8715 - val_loss: 0.3214 - val_acc: 0.8672\n",
            "Epoch 92/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2743 - acc: 0.8940Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3177 - acc: 0.8672\n",
            "Epoch 00092: val_loss did not improve from 0.28800\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2728 - acc: 0.8966 - val_loss: 0.3177 - val_acc: 0.8672\n",
            "Epoch 93/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3039 - acc: 0.8773Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2996 - acc: 0.8750\n",
            "Epoch 00093: val_loss did not improve from 0.28800\n",
            "20/20 [==============================] - 11s 532ms/step - loss: 0.3035 - acc: 0.8776 - val_loss: 0.2996 - val_acc: 0.8750\n",
            "Epoch 94/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2885 - acc: 0.8794Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2746 - acc: 0.8789\n",
            "Epoch 00094: val_loss improved from 0.28800 to 0.27462, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2915 - acc: 0.8800 - val_loss: 0.2746 - val_acc: 0.8789\n",
            "Epoch 95/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2825 - acc: 0.8919Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3524 - acc: 0.8672\n",
            "Epoch 00095: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2772 - acc: 0.8950 - val_loss: 0.3524 - val_acc: 0.8672\n",
            "Epoch 96/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3008 - acc: 0.8803Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2989 - acc: 0.8770\n",
            "Epoch 00096: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 511ms/step - loss: 0.2948 - acc: 0.8823 - val_loss: 0.2989 - val_acc: 0.8770\n",
            "Epoch 97/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2969 - acc: 0.8798Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2889 - acc: 0.8828\n",
            "Epoch 00097: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 532ms/step - loss: 0.2935 - acc: 0.8804 - val_loss: 0.2889 - val_acc: 0.8828\n",
            "Epoch 98/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2776 - acc: 0.8986Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2834 - acc: 0.8809\n",
            "Epoch 00098: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2766 - acc: 0.8978 - val_loss: 0.2834 - val_acc: 0.8809\n",
            "Epoch 99/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2886 - acc: 0.8836Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2814 - acc: 0.8828\n",
            "Epoch 00099: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 525ms/step - loss: 0.2854 - acc: 0.8848 - val_loss: 0.2814 - val_acc: 0.8828\n",
            "Epoch 100/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2760 - acc: 0.8945Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2783 - acc: 0.8672\n",
            "Epoch 00100: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2771 - acc: 0.8931 - val_loss: 0.2783 - val_acc: 0.8672\n",
            "Epoch 101/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2568 - acc: 0.8990Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3608 - acc: 0.8730\n",
            "Epoch 00101: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.2591 - acc: 0.8974 - val_loss: 0.3608 - val_acc: 0.8730\n",
            "Epoch 102/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2839 - acc: 0.8903Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3361 - acc: 0.8711\n",
            "Epoch 00102: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2843 - acc: 0.8896 - val_loss: 0.3361 - val_acc: 0.8711\n",
            "Epoch 103/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2857 - acc: 0.8819Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3114 - acc: 0.8750\n",
            "Epoch 00103: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2794 - acc: 0.8844 - val_loss: 0.3114 - val_acc: 0.8750\n",
            "Epoch 104/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2783 - acc: 0.8957Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2983 - acc: 0.8594\n",
            "Epoch 00104: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2779 - acc: 0.8962 - val_loss: 0.2983 - val_acc: 0.8594\n",
            "Epoch 105/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2897 - acc: 0.8807Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2881 - acc: 0.8809\n",
            "Epoch 00105: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.2894 - acc: 0.8816 - val_loss: 0.2881 - val_acc: 0.8809\n",
            "Epoch 106/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2751 - acc: 0.8890Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2955 - acc: 0.8809\n",
            "Epoch 00106: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2758 - acc: 0.8895 - val_loss: 0.2955 - val_acc: 0.8809\n",
            "Epoch 107/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2774 - acc: 0.8882Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2996 - acc: 0.8730\n",
            "Epoch 00107: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2731 - acc: 0.8895 - val_loss: 0.2996 - val_acc: 0.8730\n",
            "Epoch 108/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2876 - acc: 0.8865Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3314 - acc: 0.8750\n",
            "Epoch 00108: val_loss did not improve from 0.27462\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2848 - acc: 0.8867 - val_loss: 0.3314 - val_acc: 0.8750\n",
            "Epoch 109/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2655 - acc: 0.8924Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2740 - acc: 0.8750\n",
            "Epoch 00109: val_loss improved from 0.27462 to 0.27399, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 535ms/step - loss: 0.2690 - acc: 0.8919 - val_loss: 0.2740 - val_acc: 0.8750\n",
            "Epoch 110/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2705 - acc: 0.8878Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3319 - acc: 0.8789\n",
            "Epoch 00110: val_loss did not improve from 0.27399\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2660 - acc: 0.8891 - val_loss: 0.3319 - val_acc: 0.8789\n",
            "Epoch 111/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2743 - acc: 0.8936Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2946 - acc: 0.8906\n",
            "Epoch 00111: val_loss did not improve from 0.27399\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2726 - acc: 0.8943 - val_loss: 0.2946 - val_acc: 0.8906\n",
            "Epoch 112/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2917 - acc: 0.8882Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2705 - acc: 0.8789\n",
            "Epoch 00112: val_loss improved from 0.27399 to 0.27054, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 540ms/step - loss: 0.2887 - acc: 0.8899 - val_loss: 0.2705 - val_acc: 0.8789\n",
            "Epoch 113/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.3375 - acc: 0.8803Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3078 - acc: 0.8633\n",
            "Epoch 00113: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.3366 - acc: 0.8803 - val_loss: 0.3078 - val_acc: 0.8633\n",
            "Epoch 114/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2805 - acc: 0.8861Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3058 - acc: 0.8750\n",
            "Epoch 00114: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2788 - acc: 0.8871 - val_loss: 0.3058 - val_acc: 0.8750\n",
            "Epoch 115/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2614 - acc: 0.8999Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2843 - acc: 0.8887\n",
            "Epoch 00115: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2635 - acc: 0.8986 - val_loss: 0.2843 - val_acc: 0.8887\n",
            "Epoch 116/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2593 - acc: 0.8974Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2952 - acc: 0.8750\n",
            "Epoch 00116: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2609 - acc: 0.8974 - val_loss: 0.2952 - val_acc: 0.8750\n",
            "Epoch 117/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2805 - acc: 0.8924Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2909 - acc: 0.8750\n",
            "Epoch 00117: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2783 - acc: 0.8939 - val_loss: 0.2909 - val_acc: 0.8750\n",
            "Epoch 118/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2571 - acc: 0.8961Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2960 - acc: 0.8711\n",
            "Epoch 00118: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.2591 - acc: 0.8954 - val_loss: 0.2960 - val_acc: 0.8711\n",
            "Epoch 119/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2734 - acc: 0.8903Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2752 - acc: 0.8887\n",
            "Epoch 00119: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 513ms/step - loss: 0.2739 - acc: 0.8900 - val_loss: 0.2752 - val_acc: 0.8887\n",
            "Epoch 120/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2545 - acc: 0.8961Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2816 - acc: 0.8809\n",
            "Epoch 00120: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2560 - acc: 0.8954 - val_loss: 0.2816 - val_acc: 0.8809\n",
            "Epoch 121/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2701 - acc: 0.8899Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2835 - acc: 0.8848\n",
            "Epoch 00121: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2697 - acc: 0.8899 - val_loss: 0.2835 - val_acc: 0.8848\n",
            "Epoch 122/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2570 - acc: 0.9028Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2937 - acc: 0.8711\n",
            "Epoch 00122: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2613 - acc: 0.9010 - val_loss: 0.2937 - val_acc: 0.8711\n",
            "Epoch 123/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2748 - acc: 0.8990Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2861 - acc: 0.8770\n",
            "Epoch 00123: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2764 - acc: 0.8998 - val_loss: 0.2861 - val_acc: 0.8770\n",
            "Epoch 124/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2610 - acc: 0.8982Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2960 - acc: 0.8809\n",
            "Epoch 00124: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2609 - acc: 0.8978 - val_loss: 0.2960 - val_acc: 0.8809\n",
            "Epoch 125/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2519 - acc: 0.9026Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2967 - acc: 0.8848\n",
            "Epoch 00125: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2485 - acc: 0.9048 - val_loss: 0.2967 - val_acc: 0.8848\n",
            "Epoch 126/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2702 - acc: 0.9011Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2732 - acc: 0.8906\n",
            "Epoch 00126: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2700 - acc: 0.9006 - val_loss: 0.2732 - val_acc: 0.8906\n",
            "Epoch 127/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2611 - acc: 0.8995Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2764 - acc: 0.8828\n",
            "Epoch 00127: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2590 - acc: 0.9006 - val_loss: 0.2764 - val_acc: 0.8828\n",
            "Epoch 128/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2625 - acc: 0.8961Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2727 - acc: 0.8887\n",
            "Epoch 00128: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2643 - acc: 0.8947 - val_loss: 0.2727 - val_acc: 0.8887\n",
            "Epoch 129/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2563 - acc: 0.8919Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3031 - acc: 0.8770\n",
            "Epoch 00129: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2595 - acc: 0.8919 - val_loss: 0.3031 - val_acc: 0.8770\n",
            "Epoch 130/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2664 - acc: 0.9011Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2896 - acc: 0.8789\n",
            "Epoch 00130: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2669 - acc: 0.9012 - val_loss: 0.2896 - val_acc: 0.8789\n",
            "Epoch 131/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2398 - acc: 0.9091Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2801 - acc: 0.8867\n",
            "Epoch 00131: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2416 - acc: 0.9089 - val_loss: 0.2801 - val_acc: 0.8867\n",
            "Epoch 132/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2631 - acc: 0.8919Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2792 - acc: 0.8984\n",
            "Epoch 00132: val_loss did not improve from 0.27054\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2635 - acc: 0.8919 - val_loss: 0.2792 - val_acc: 0.8984\n",
            "Epoch 133/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2581 - acc: 0.9061Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2585 - acc: 0.8867\n",
            "Epoch 00133: val_loss improved from 0.27054 to 0.25845, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2557 - acc: 0.9077 - val_loss: 0.2585 - val_acc: 0.8867\n",
            "Epoch 134/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2737 - acc: 0.8932Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2725 - acc: 0.8887\n",
            "Epoch 00134: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2738 - acc: 0.8931 - val_loss: 0.2725 - val_acc: 0.8887\n",
            "Epoch 135/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2337 - acc: 0.9107Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4475 - acc: 0.8145\n",
            "Epoch 00135: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2387 - acc: 0.9081 - val_loss: 0.4475 - val_acc: 0.8145\n",
            "Epoch 136/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2654 - acc: 0.8980Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2832 - acc: 0.8984\n",
            "Epoch 00136: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 506ms/step - loss: 0.2630 - acc: 0.8992 - val_loss: 0.2832 - val_acc: 0.8984\n",
            "Epoch 137/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2505 - acc: 0.9040Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3047 - acc: 0.8789\n",
            "Epoch 00137: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2512 - acc: 0.9034 - val_loss: 0.3047 - val_acc: 0.8789\n",
            "Epoch 138/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2629 - acc: 0.8990Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2888 - acc: 0.8867\n",
            "Epoch 00138: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2627 - acc: 0.9002 - val_loss: 0.2888 - val_acc: 0.8867\n",
            "Epoch 139/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2522 - acc: 0.9015Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2678 - acc: 0.8867\n",
            "Epoch 00139: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2476 - acc: 0.9046 - val_loss: 0.2678 - val_acc: 0.8867\n",
            "Epoch 140/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2614 - acc: 0.9003Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2739 - acc: 0.8887\n",
            "Epoch 00140: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2556 - acc: 0.9030 - val_loss: 0.2739 - val_acc: 0.8887\n",
            "Epoch 141/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2391 - acc: 0.9028Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2819 - acc: 0.8867\n",
            "Epoch 00141: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 525ms/step - loss: 0.2414 - acc: 0.9018 - val_loss: 0.2819 - val_acc: 0.8867\n",
            "Epoch 142/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2572 - acc: 0.8992Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2788 - acc: 0.8789\n",
            "Epoch 00142: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 503ms/step - loss: 0.2553 - acc: 0.8992 - val_loss: 0.2788 - val_acc: 0.8789\n",
            "Epoch 143/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2569 - acc: 0.8961Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2964 - acc: 0.8906\n",
            "Epoch 00143: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2539 - acc: 0.8986 - val_loss: 0.2964 - val_acc: 0.8906\n",
            "Epoch 144/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2346 - acc: 0.9065Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2800 - acc: 0.8652\n",
            "Epoch 00144: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2365 - acc: 0.9053 - val_loss: 0.2800 - val_acc: 0.8652\n",
            "Epoch 145/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2583 - acc: 0.9032Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2787 - acc: 0.8984\n",
            "Epoch 00145: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2618 - acc: 0.9010 - val_loss: 0.2787 - val_acc: 0.8984\n",
            "Epoch 146/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2440 - acc: 0.9070Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2978 - acc: 0.8730\n",
            "Epoch 00146: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2442 - acc: 0.9061 - val_loss: 0.2978 - val_acc: 0.8730\n",
            "Epoch 147/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2479 - acc: 0.8995Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2818 - acc: 0.8926\n",
            "Epoch 00147: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2497 - acc: 0.9004 - val_loss: 0.2818 - val_acc: 0.8926\n",
            "Epoch 148/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2551 - acc: 0.9020Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2837 - acc: 0.8789\n",
            "Epoch 00148: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2547 - acc: 0.9030 - val_loss: 0.2837 - val_acc: 0.8789\n",
            "Epoch 149/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2570 - acc: 0.8974Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2707 - acc: 0.8984\n",
            "Epoch 00149: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2562 - acc: 0.8978 - val_loss: 0.2707 - val_acc: 0.8984\n",
            "Epoch 150/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2269 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2747 - acc: 0.8730\n",
            "Epoch 00150: val_loss did not improve from 0.25845\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2332 - acc: 0.9156 - val_loss: 0.2747 - val_acc: 0.8730\n",
            "Epoch 151/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2454 - acc: 0.9024Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2524 - acc: 0.8965\n",
            "Epoch 00151: val_loss improved from 0.25845 to 0.25236, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2448 - acc: 0.9030 - val_loss: 0.2524 - val_acc: 0.8965\n",
            "Epoch 152/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2584 - acc: 0.9028Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2844 - acc: 0.8867\n",
            "Epoch 00152: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2572 - acc: 0.9030 - val_loss: 0.2844 - val_acc: 0.8867\n",
            "Epoch 153/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2521 - acc: 0.8975Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2711 - acc: 0.8926\n",
            "Epoch 00153: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2470 - acc: 0.9000 - val_loss: 0.2711 - val_acc: 0.8926\n",
            "Epoch 154/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2439 - acc: 0.9111Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2884 - acc: 0.8906\n",
            "Epoch 00154: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2424 - acc: 0.9105 - val_loss: 0.2884 - val_acc: 0.8906\n",
            "Epoch 155/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2447 - acc: 0.9036Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2746 - acc: 0.8965\n",
            "Epoch 00155: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2445 - acc: 0.9030 - val_loss: 0.2746 - val_acc: 0.8965\n",
            "Epoch 156/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2308 - acc: 0.9141Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2528 - acc: 0.8906\n",
            "Epoch 00156: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2351 - acc: 0.9121 - val_loss: 0.2528 - val_acc: 0.8906\n",
            "Epoch 157/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2527 - acc: 0.8990Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3073 - acc: 0.8887\n",
            "Epoch 00157: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2510 - acc: 0.9014 - val_loss: 0.3073 - val_acc: 0.8887\n",
            "Epoch 158/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2339 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3089 - acc: 0.8711\n",
            "Epoch 00158: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2378 - acc: 0.9113 - val_loss: 0.3089 - val_acc: 0.8711\n",
            "Epoch 159/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2421 - acc: 0.9030Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2682 - acc: 0.9102\n",
            "Epoch 00159: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2388 - acc: 0.9040 - val_loss: 0.2682 - val_acc: 0.9102\n",
            "Epoch 160/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2424 - acc: 0.9011Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2590 - acc: 0.8945\n",
            "Epoch 00160: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2444 - acc: 0.9002 - val_loss: 0.2590 - val_acc: 0.8945\n",
            "Epoch 161/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2422 - acc: 0.9049Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2952 - acc: 0.8828\n",
            "Epoch 00161: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2398 - acc: 0.9053 - val_loss: 0.2952 - val_acc: 0.8828\n",
            "Epoch 162/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2445 - acc: 0.9091Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2647 - acc: 0.8906\n",
            "Epoch 00162: val_loss did not improve from 0.25236\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2417 - acc: 0.9097 - val_loss: 0.2647 - val_acc: 0.8906\n",
            "Epoch 163/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2505 - acc: 0.9061Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2475 - acc: 0.9004\n",
            "Epoch 00163: val_loss improved from 0.25236 to 0.24755, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2493 - acc: 0.9053 - val_loss: 0.2475 - val_acc: 0.9004\n",
            "Epoch 164/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2423 - acc: 0.9065Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2777 - acc: 0.8926\n",
            "Epoch 00164: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2429 - acc: 0.9064 - val_loss: 0.2777 - val_acc: 0.8926\n",
            "Epoch 165/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2413 - acc: 0.9049Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2758 - acc: 0.8965\n",
            "Epoch 00165: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2407 - acc: 0.9050 - val_loss: 0.2758 - val_acc: 0.8965\n",
            "Epoch 166/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2344 - acc: 0.9078Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2601 - acc: 0.8926\n",
            "Epoch 00166: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2344 - acc: 0.9089 - val_loss: 0.2601 - val_acc: 0.8926\n",
            "Epoch 167/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2348 - acc: 0.9153Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2929 - acc: 0.8789\n",
            "Epoch 00167: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2339 - acc: 0.9152 - val_loss: 0.2929 - val_acc: 0.8789\n",
            "Epoch 168/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2487 - acc: 0.9074Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2626 - acc: 0.9004\n",
            "Epoch 00168: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2462 - acc: 0.9093 - val_loss: 0.2626 - val_acc: 0.9004\n",
            "Epoch 169/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2486 - acc: 0.9049Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2584 - acc: 0.9023\n",
            "Epoch 00169: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 11s 532ms/step - loss: 0.2448 - acc: 0.9061 - val_loss: 0.2584 - val_acc: 0.9023\n",
            "Epoch 170/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2333 - acc: 0.9098Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2687 - acc: 0.8984\n",
            "Epoch 00170: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2321 - acc: 0.9084 - val_loss: 0.2687 - val_acc: 0.8984\n",
            "Epoch 171/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2496 - acc: 0.9011Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2612 - acc: 0.9043\n",
            "Epoch 00171: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2454 - acc: 0.9046 - val_loss: 0.2612 - val_acc: 0.9043\n",
            "Epoch 172/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2391 - acc: 0.9070Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2544 - acc: 0.9023\n",
            "Epoch 00172: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2370 - acc: 0.9081 - val_loss: 0.2544 - val_acc: 0.9023\n",
            "Epoch 173/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2395 - acc: 0.9103Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2666 - acc: 0.8965\n",
            "Epoch 00173: val_loss did not improve from 0.24755\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2347 - acc: 0.9117 - val_loss: 0.2666 - val_acc: 0.8965\n",
            "Epoch 174/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2475 - acc: 0.9049Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2411 - acc: 0.9082\n",
            "Epoch 00174: val_loss improved from 0.24755 to 0.24113, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2441 - acc: 0.9069 - val_loss: 0.2411 - val_acc: 0.9082\n",
            "Epoch 175/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2428 - acc: 0.9032Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2731 - acc: 0.8887\n",
            "Epoch 00175: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 525ms/step - loss: 0.2436 - acc: 0.9022 - val_loss: 0.2731 - val_acc: 0.8887\n",
            "Epoch 176/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2344 - acc: 0.9047Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.4362 - acc: 0.8789\n",
            "Epoch 00176: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2356 - acc: 0.9040 - val_loss: 0.4362 - val_acc: 0.8789\n",
            "Epoch 177/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2226 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2717 - acc: 0.9004\n",
            "Epoch 00177: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2216 - acc: 0.9184 - val_loss: 0.2717 - val_acc: 0.9004\n",
            "Epoch 178/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2390 - acc: 0.9078Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2698 - acc: 0.8848\n",
            "Epoch 00178: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2406 - acc: 0.9077 - val_loss: 0.2698 - val_acc: 0.8848\n",
            "Epoch 179/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2230 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2667 - acc: 0.9004\n",
            "Epoch 00179: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2238 - acc: 0.9137 - val_loss: 0.2667 - val_acc: 0.9004\n",
            "Epoch 180/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2398 - acc: 0.9086Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2772 - acc: 0.8867\n",
            "Epoch 00180: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 534ms/step - loss: 0.2432 - acc: 0.9073 - val_loss: 0.2772 - val_acc: 0.8867\n",
            "Epoch 181/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2243 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2592 - acc: 0.8926\n",
            "Epoch 00181: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 511ms/step - loss: 0.2250 - acc: 0.9149 - val_loss: 0.2592 - val_acc: 0.8926\n",
            "Epoch 182/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2301 - acc: 0.9091Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2468 - acc: 0.8867\n",
            "Epoch 00182: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2320 - acc: 0.9081 - val_loss: 0.2468 - val_acc: 0.8867\n",
            "Epoch 183/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2307 - acc: 0.9111Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2999 - acc: 0.8926\n",
            "Epoch 00183: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2274 - acc: 0.9117 - val_loss: 0.2999 - val_acc: 0.8926\n",
            "Epoch 184/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2339 - acc: 0.9103Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2555 - acc: 0.8906\n",
            "Epoch 00184: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2332 - acc: 0.9105 - val_loss: 0.2555 - val_acc: 0.8906\n",
            "Epoch 185/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2360 - acc: 0.9107Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2661 - acc: 0.8965\n",
            "Epoch 00185: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.2350 - acc: 0.9109 - val_loss: 0.2661 - val_acc: 0.8965\n",
            "Epoch 186/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2276 - acc: 0.9124Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2509 - acc: 0.9043\n",
            "Epoch 00186: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2298 - acc: 0.9105 - val_loss: 0.2509 - val_acc: 0.9043\n",
            "Epoch 187/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2313 - acc: 0.9094Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2630 - acc: 0.8867\n",
            "Epoch 00187: val_loss did not improve from 0.24113\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2341 - acc: 0.9064 - val_loss: 0.2630 - val_acc: 0.8867\n",
            "Epoch 188/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2289 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2320 - acc: 0.9062\n",
            "Epoch 00188: val_loss improved from 0.24113 to 0.23199, saving model to two_ch_cnn_best.h5\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2281 - acc: 0.9125 - val_loss: 0.2320 - val_acc: 0.9062\n",
            "Epoch 189/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2338 - acc: 0.9103Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2740 - acc: 0.8828\n",
            "Epoch 00189: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2364 - acc: 0.9093 - val_loss: 0.2740 - val_acc: 0.8828\n",
            "Epoch 190/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2309 - acc: 0.9149Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2585 - acc: 0.9082\n",
            "Epoch 00190: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 525ms/step - loss: 0.2331 - acc: 0.9137 - val_loss: 0.2585 - val_acc: 0.9082\n",
            "Epoch 191/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2287 - acc: 0.9149Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2457 - acc: 0.8965\n",
            "Epoch 00191: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2308 - acc: 0.9141 - val_loss: 0.2457 - val_acc: 0.8965\n",
            "Epoch 192/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2249 - acc: 0.9120Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2829 - acc: 0.8926\n",
            "Epoch 00192: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2265 - acc: 0.9109 - val_loss: 0.2829 - val_acc: 0.8926\n",
            "Epoch 193/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2377 - acc: 0.9047Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2592 - acc: 0.8965\n",
            "Epoch 00193: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2401 - acc: 0.9032 - val_loss: 0.2592 - val_acc: 0.8965\n",
            "Epoch 194/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2220 - acc: 0.9170Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2466 - acc: 0.8945\n",
            "Epoch 00194: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2213 - acc: 0.9172 - val_loss: 0.2466 - val_acc: 0.8945\n",
            "Epoch 195/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2207 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2767 - acc: 0.8945\n",
            "Epoch 00195: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 538ms/step - loss: 0.2222 - acc: 0.9141 - val_loss: 0.2767 - val_acc: 0.8945\n",
            "Epoch 196/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2329 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2781 - acc: 0.8965\n",
            "Epoch 00196: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2314 - acc: 0.9137 - val_loss: 0.2781 - val_acc: 0.8965\n",
            "Epoch 197/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2305 - acc: 0.9082Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2637 - acc: 0.8984\n",
            "Epoch 00197: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2322 - acc: 0.9081 - val_loss: 0.2637 - val_acc: 0.8984\n",
            "Epoch 198/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2384 - acc: 0.9095Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2676 - acc: 0.9004\n",
            "Epoch 00198: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2349 - acc: 0.9104 - val_loss: 0.2676 - val_acc: 0.9004\n",
            "Epoch 199/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2221 - acc: 0.9078Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2686 - acc: 0.8945\n",
            "Epoch 00199: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2233 - acc: 0.9081 - val_loss: 0.2686 - val_acc: 0.8945\n",
            "Epoch 200/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2234 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2611 - acc: 0.8945\n",
            "Epoch 00200: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2238 - acc: 0.9137 - val_loss: 0.2611 - val_acc: 0.8945\n",
            "Epoch 201/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2230 - acc: 0.9136Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2612 - acc: 0.8770\n",
            "Epoch 00201: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.2216 - acc: 0.9133 - val_loss: 0.2612 - val_acc: 0.8770\n",
            "Epoch 202/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2288 - acc: 0.9074Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2632 - acc: 0.8984\n",
            "Epoch 00202: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 512ms/step - loss: 0.2265 - acc: 0.9077 - val_loss: 0.2632 - val_acc: 0.8984\n",
            "Epoch 203/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2306 - acc: 0.9124Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2411 - acc: 0.9043\n",
            "Epoch 00203: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2315 - acc: 0.9125 - val_loss: 0.2411 - val_acc: 0.9043\n",
            "Epoch 204/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2232 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2393 - acc: 0.9004\n",
            "Epoch 00204: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 512ms/step - loss: 0.2219 - acc: 0.9181 - val_loss: 0.2393 - val_acc: 0.9004\n",
            "Epoch 205/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2239 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2561 - acc: 0.9043\n",
            "Epoch 00205: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2226 - acc: 0.9152 - val_loss: 0.2561 - val_acc: 0.9043\n",
            "Epoch 206/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2202 - acc: 0.9182Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2573 - acc: 0.8984\n",
            "Epoch 00206: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2179 - acc: 0.9192 - val_loss: 0.2573 - val_acc: 0.8984\n",
            "Epoch 207/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2306 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2584 - acc: 0.8906\n",
            "Epoch 00207: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2312 - acc: 0.9152 - val_loss: 0.2584 - val_acc: 0.8906\n",
            "Epoch 208/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2118 - acc: 0.9149Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2690 - acc: 0.8965\n",
            "Epoch 00208: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2147 - acc: 0.9149 - val_loss: 0.2690 - val_acc: 0.8965\n",
            "Epoch 209/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2241 - acc: 0.9099Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2784 - acc: 0.8887\n",
            "Epoch 00209: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2231 - acc: 0.9109 - val_loss: 0.2784 - val_acc: 0.8887\n",
            "Epoch 210/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2293 - acc: 0.9102Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2813 - acc: 0.8906\n",
            "Epoch 00210: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 530ms/step - loss: 0.2266 - acc: 0.9120 - val_loss: 0.2813 - val_acc: 0.8906\n",
            "Epoch 211/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2099 - acc: 0.9141Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2750 - acc: 0.9004\n",
            "Epoch 00211: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 512ms/step - loss: 0.2071 - acc: 0.9156 - val_loss: 0.2750 - val_acc: 0.9004\n",
            "Epoch 212/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2372 - acc: 0.9095Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2639 - acc: 0.8984\n",
            "Epoch 00212: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2360 - acc: 0.9097 - val_loss: 0.2639 - val_acc: 0.8984\n",
            "Epoch 213/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2232 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2855 - acc: 0.8984\n",
            "Epoch 00213: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2249 - acc: 0.9117 - val_loss: 0.2855 - val_acc: 0.8984\n",
            "Epoch 214/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2227 - acc: 0.9195Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2500 - acc: 0.8945\n",
            "Epoch 00214: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2233 - acc: 0.9176 - val_loss: 0.2500 - val_acc: 0.8945\n",
            "Epoch 215/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2189 - acc: 0.9124Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2533 - acc: 0.8867\n",
            "Epoch 00215: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2200 - acc: 0.9112 - val_loss: 0.2533 - val_acc: 0.8867\n",
            "Epoch 216/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2213 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2640 - acc: 0.9004\n",
            "Epoch 00216: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 537ms/step - loss: 0.2187 - acc: 0.9192 - val_loss: 0.2640 - val_acc: 0.9004\n",
            "Epoch 217/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2219 - acc: 0.9070Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2573 - acc: 0.9004\n",
            "Epoch 00217: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2232 - acc: 0.9061 - val_loss: 0.2573 - val_acc: 0.9004\n",
            "Epoch 218/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2148 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2573 - acc: 0.8887\n",
            "Epoch 00218: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2183 - acc: 0.9149 - val_loss: 0.2573 - val_acc: 0.8887\n",
            "Epoch 219/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2332 - acc: 0.9099Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2511 - acc: 0.8867\n",
            "Epoch 00219: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2366 - acc: 0.9085 - val_loss: 0.2511 - val_acc: 0.8867\n",
            "Epoch 220/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2293 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2832 - acc: 0.8887\n",
            "Epoch 00220: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.2298 - acc: 0.9145 - val_loss: 0.2832 - val_acc: 0.8887\n",
            "Epoch 221/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2232 - acc: 0.9153Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2667 - acc: 0.8926\n",
            "Epoch 00221: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2218 - acc: 0.9165 - val_loss: 0.2667 - val_acc: 0.8926\n",
            "Epoch 222/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2215 - acc: 0.9207Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2984 - acc: 0.8809\n",
            "Epoch 00222: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2184 - acc: 0.9216 - val_loss: 0.2984 - val_acc: 0.8809\n",
            "Epoch 223/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2217 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2712 - acc: 0.8926\n",
            "Epoch 00223: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2207 - acc: 0.9145 - val_loss: 0.2712 - val_acc: 0.8926\n",
            "Epoch 224/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2183 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2592 - acc: 0.9004\n",
            "Epoch 00224: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 538ms/step - loss: 0.2194 - acc: 0.9129 - val_loss: 0.2592 - val_acc: 0.9004\n",
            "Epoch 225/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2207 - acc: 0.9120Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2744 - acc: 0.9043\n",
            "Epoch 00225: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2230 - acc: 0.9117 - val_loss: 0.2744 - val_acc: 0.9043\n",
            "Epoch 226/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2257 - acc: 0.9170Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2489 - acc: 0.9023\n",
            "Epoch 00226: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2255 - acc: 0.9168 - val_loss: 0.2489 - val_acc: 0.9023\n",
            "Epoch 227/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2192 - acc: 0.9162Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2574 - acc: 0.8945\n",
            "Epoch 00227: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2185 - acc: 0.9169 - val_loss: 0.2574 - val_acc: 0.8945\n",
            "Epoch 228/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2201 - acc: 0.9149Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2492 - acc: 0.9102\n",
            "Epoch 00228: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2183 - acc: 0.9149 - val_loss: 0.2492 - val_acc: 0.9102\n",
            "Epoch 229/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2238 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2404 - acc: 0.8867\n",
            "Epoch 00229: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2252 - acc: 0.9133 - val_loss: 0.2404 - val_acc: 0.8867\n",
            "Epoch 230/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2202 - acc: 0.9111Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2368 - acc: 0.9023\n",
            "Epoch 00230: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2183 - acc: 0.9117 - val_loss: 0.2368 - val_acc: 0.9023\n",
            "Epoch 231/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2191 - acc: 0.9116Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2595 - acc: 0.9023\n",
            "Epoch 00231: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2200 - acc: 0.9117 - val_loss: 0.2595 - val_acc: 0.9023\n",
            "Epoch 232/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2059 - acc: 0.9212Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2484 - acc: 0.9043\n",
            "Epoch 00232: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.2121 - acc: 0.9197 - val_loss: 0.2484 - val_acc: 0.9043\n",
            "Epoch 233/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2222 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2815 - acc: 0.8926\n",
            "Epoch 00233: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2220 - acc: 0.9133 - val_loss: 0.2815 - val_acc: 0.8926\n",
            "Epoch 234/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2216 - acc: 0.9145Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2436 - acc: 0.9121\n",
            "Epoch 00234: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2208 - acc: 0.9141 - val_loss: 0.2436 - val_acc: 0.9121\n",
            "Epoch 235/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2124 - acc: 0.9166Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2589 - acc: 0.8945\n",
            "Epoch 00235: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2141 - acc: 0.9156 - val_loss: 0.2589 - val_acc: 0.8945\n",
            "Epoch 236/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2133 - acc: 0.9186Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2581 - acc: 0.9102\n",
            "Epoch 00236: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2150 - acc: 0.9188 - val_loss: 0.2581 - val_acc: 0.9102\n",
            "Epoch 237/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2196 - acc: 0.9182Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2530 - acc: 0.8867\n",
            "Epoch 00237: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2200 - acc: 0.9180 - val_loss: 0.2530 - val_acc: 0.8867\n",
            "Epoch 238/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2203 - acc: 0.9145Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2709 - acc: 0.8867\n",
            "Epoch 00238: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2171 - acc: 0.9165 - val_loss: 0.2709 - val_acc: 0.8867\n",
            "Epoch 239/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2224 - acc: 0.9128Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2355 - acc: 0.9062\n",
            "Epoch 00239: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2214 - acc: 0.9133 - val_loss: 0.2355 - val_acc: 0.9062\n",
            "Epoch 240/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2060 - acc: 0.9153Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2601 - acc: 0.8984\n",
            "Epoch 00240: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 527ms/step - loss: 0.2071 - acc: 0.9160 - val_loss: 0.2601 - val_acc: 0.8984\n",
            "Epoch 241/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2093 - acc: 0.9195Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2730 - acc: 0.8965\n",
            "Epoch 00241: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2085 - acc: 0.9204 - val_loss: 0.2730 - val_acc: 0.8965\n",
            "Epoch 242/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2088 - acc: 0.9207Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2883 - acc: 0.8867\n",
            "Epoch 00242: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2065 - acc: 0.9212 - val_loss: 0.2883 - val_acc: 0.8867\n",
            "Epoch 243/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2268 - acc: 0.9116Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2447 - acc: 0.8965\n",
            "Epoch 00243: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2300 - acc: 0.9113 - val_loss: 0.2447 - val_acc: 0.8965\n",
            "Epoch 244/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2149 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2501 - acc: 0.8945\n",
            "Epoch 00244: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2122 - acc: 0.9173 - val_loss: 0.2501 - val_acc: 0.8945\n",
            "Epoch 245/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2187 - acc: 0.9091Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 10s - loss: 0.2754 - acc: 0.8984\n",
            "Epoch 00245: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 515ms/step - loss: 0.2154 - acc: 0.9093 - val_loss: 0.2754 - val_acc: 0.8984\n",
            "Epoch 246/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2061 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2830 - acc: 0.8926\n",
            "Epoch 00246: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2085 - acc: 0.9160 - val_loss: 0.2830 - val_acc: 0.8926\n",
            "Epoch 247/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2139 - acc: 0.9107Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2751 - acc: 0.8984\n",
            "Epoch 00247: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2135 - acc: 0.9113 - val_loss: 0.2751 - val_acc: 0.8984\n",
            "Epoch 248/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2146 - acc: 0.9220Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2929 - acc: 0.8750\n",
            "Epoch 00248: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 519ms/step - loss: 0.2161 - acc: 0.9204 - val_loss: 0.2929 - val_acc: 0.8750\n",
            "Epoch 249/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2131 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2821 - acc: 0.8984\n",
            "Epoch 00249: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2093 - acc: 0.9189 - val_loss: 0.2821 - val_acc: 0.8984\n",
            "Epoch 250/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2122 - acc: 0.9186Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2413 - acc: 0.8984\n",
            "Epoch 00250: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2132 - acc: 0.9184 - val_loss: 0.2413 - val_acc: 0.8984\n",
            "Epoch 251/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2116 - acc: 0.9253Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2453 - acc: 0.8965\n",
            "Epoch 00251: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.2112 - acc: 0.9248 - val_loss: 0.2453 - val_acc: 0.8965\n",
            "Epoch 252/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2124 - acc: 0.9182Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2633 - acc: 0.8984\n",
            "Epoch 00252: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 521ms/step - loss: 0.2141 - acc: 0.9180 - val_loss: 0.2633 - val_acc: 0.8984\n",
            "Epoch 253/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2058 - acc: 0.9174Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2677 - acc: 0.8984\n",
            "Epoch 00253: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 531ms/step - loss: 0.2034 - acc: 0.9180 - val_loss: 0.2677 - val_acc: 0.8984\n",
            "Epoch 254/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2023 - acc: 0.9253Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2655 - acc: 0.8828\n",
            "Epoch 00254: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 536ms/step - loss: 0.2024 - acc: 0.9255 - val_loss: 0.2655 - val_acc: 0.8828\n",
            "Epoch 255/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2179 - acc: 0.9179Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2629 - acc: 0.8906\n",
            "Epoch 00255: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 520ms/step - loss: 0.2190 - acc: 0.9157 - val_loss: 0.2629 - val_acc: 0.8906\n",
            "Epoch 256/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2061 - acc: 0.9216Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.3883 - acc: 0.8789\n",
            "Epoch 00256: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 529ms/step - loss: 0.2097 - acc: 0.9200 - val_loss: 0.3883 - val_acc: 0.8789\n",
            "Epoch 257/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2148 - acc: 0.9199Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2491 - acc: 0.8906\n",
            "Epoch 00257: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 516ms/step - loss: 0.2160 - acc: 0.9176 - val_loss: 0.2491 - val_acc: 0.8906\n",
            "Epoch 258/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2132 - acc: 0.9132Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2602 - acc: 0.8945\n",
            "Epoch 00258: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 526ms/step - loss: 0.2143 - acc: 0.9121 - val_loss: 0.2602 - val_acc: 0.8945\n",
            "Epoch 259/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2122 - acc: 0.9203Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2552 - acc: 0.8906\n",
            "Epoch 00259: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2129 - acc: 0.9204 - val_loss: 0.2552 - val_acc: 0.8906\n",
            "Epoch 260/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2192 - acc: 0.9149Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2827 - acc: 0.9023\n",
            "Epoch 00260: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.2166 - acc: 0.9160 - val_loss: 0.2827 - val_acc: 0.9023\n",
            "Epoch 261/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2026 - acc: 0.9225Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2456 - acc: 0.9082\n",
            "Epoch 00261: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 518ms/step - loss: 0.1999 - acc: 0.9233 - val_loss: 0.2456 - val_acc: 0.9082\n",
            "Epoch 262/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2098 - acc: 0.9224Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2469 - acc: 0.8984\n",
            "Epoch 00262: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 11s 528ms/step - loss: 0.2133 - acc: 0.9208 - val_loss: 0.2469 - val_acc: 0.8984\n",
            "Epoch 263/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2063 - acc: 0.9207Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2632 - acc: 0.8926\n",
            "Epoch 00263: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 523ms/step - loss: 0.2062 - acc: 0.9204 - val_loss: 0.2632 - val_acc: 0.8926\n",
            "Epoch 264/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2095 - acc: 0.9224Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2634 - acc: 0.8926\n",
            "Epoch 00264: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 509ms/step - loss: 0.2117 - acc: 0.9220 - val_loss: 0.2634 - val_acc: 0.8926\n",
            "Epoch 265/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2243 - acc: 0.9145Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2444 - acc: 0.9004\n",
            "Epoch 00265: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 517ms/step - loss: 0.2221 - acc: 0.9152 - val_loss: 0.2444 - val_acc: 0.9004\n",
            "Epoch 266/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2207 - acc: 0.9157Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2649 - acc: 0.8965\n",
            "Epoch 00266: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 514ms/step - loss: 0.2162 - acc: 0.9173 - val_loss: 0.2649 - val_acc: 0.8965\n",
            "Epoch 267/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.2189 - acc: 0.9170Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2630 - acc: 0.8965\n",
            "Epoch 00267: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 524ms/step - loss: 0.2175 - acc: 0.9184 - val_loss: 0.2630 - val_acc: 0.8965\n",
            "Epoch 268/500\n",
            "19/20 [===========================>..] - ETA: 0s - loss: 0.1982 - acc: 0.9228Epoch 1/500\n",
            " 4/20 [=====>........................] - ETA: 11s - loss: 0.2877 - acc: 0.8945\n",
            "Epoch 00268: val_loss did not improve from 0.23199\n",
            "20/20 [==============================] - 10s 522ms/step - loss: 0.1956 - acc: 0.9244 - val_loss: 0.2877 - val_acc: 0.8945\n",
            "Epoch 00268: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B52sa8XUZ7Uw",
        "colab_type": "code",
        "outputId": "a36230ba-5dd6-4b2b-a363-a867d7041b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# History of accuracy and loss\n",
        "tra_loss = history_two_ch_cnn.history['loss']\n",
        "tra_acc = history_two_ch_cnn.history['acc']\n",
        "val_loss = history_two_ch_cnn.history['val_loss']\n",
        "val_acc = history_two_ch_cnn.history['val_acc']\n",
        "\n",
        "# Total number of epochs training\n",
        "epochs = range(1, len(tra_acc)+1)\n",
        "end_epoch = len(tra_acc)\n",
        "\n",
        "# Epoch when reached the validation loss minimum\n",
        "opt_epoch = val_loss.index(min(val_loss)) + 1\n",
        "\n",
        "# Loss and accuracy on the validation set\n",
        "end_val_loss = val_loss[-1]\n",
        "end_val_acc = val_acc[-1]\n",
        "opt_val_loss = val_loss[opt_epoch-1]\n",
        "opt_val_acc = val_acc[opt_epoch-1]\n",
        "\n",
        "# Loss and accuracy on the test set\n",
        "opt_two_ch_cnn = models.load_model('two_ch_cnn_best.h5')\n",
        "test_loss, test_acc = two_ch_cnn.evaluate([test_abn_images, test_base_images], test_labels, verbose=False)\n",
        "opt_test_loss, opt_test_acc = opt_two_ch_cnn.evaluate([test_abn_images, test_base_images], test_labels, verbose=False)\n",
        "\n",
        "print(\"Dual input CNN\\n\")\n",
        "\n",
        "print(\"Epoch [end]: %d\" % end_epoch)\n",
        "print(\"Epoch [opt]: %d\" % opt_epoch)\n",
        "print(\"Valid accuracy [end]: %.4f\" % end_val_acc)\n",
        "print(\"Valid accuracy [opt]: %.4f\" % opt_val_acc)\n",
        "print(\"Test accuracy [end]:  %.4f\" % test_acc)\n",
        "print(\"Test accuracy [opt]:  %.4f\" % opt_test_acc)\n",
        "print(\"Valid loss [end]: %.4f\" % end_val_loss)\n",
        "print(\"Valid loss [opt]: %.4f\" % opt_val_loss)\n",
        "print(\"Test loss [end]:  %.4f\" % test_loss)\n",
        "print(\"Test loss [opt]:  %.4f\" % opt_test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dual input CNN\n",
            "\n",
            "Epoch [end]: 268\n",
            "Epoch [opt]: 188\n",
            "Valid accuracy [end]: 0.8945\n",
            "Valid accuracy [opt]: 0.9062\n",
            "Test accuracy [end]:  0.8542\n",
            "Test accuracy [opt]:  0.8720\n",
            "Valid loss [end]: 0.2877\n",
            "Valid loss [opt]: 0.2320\n",
            "Test loss [end]:  0.6290\n",
            "Test loss [opt]:  0.4116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHHdix2yoZCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNs3slDEs6Zu",
        "colab_type": "code",
        "outputId": "dbc60034-df8b-4f59-e08a-9474cbf199b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "pred = opt_two_ch_cnn.predict([test_abn_images, test_base_images])\n",
        "\n",
        "pred_classes = np.rint(pred)\n",
        "confusion_mtx = confusion_matrix(test_labels, pred_classes) \n",
        "plot_confusion_matrix(confusion_mtx, classes=range(2), normalize=False, title='Dual input CNN confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEmCAYAAADr3bIaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJL4-PGzt6At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_roc(preds, names, titles='ROC curve'):\n",
        "    plt.figure(figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
        "    for pred, name in zip(preds, names):\n",
        "        fpr, tpr, _ = roc_curve(test_labels, pred)\n",
        "        auc_keras = auc(fpr, tpr)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(fpr, tpr, label=(name +' (area = {:.3f})'.format(auc_keras)))\n",
        "    plt.xlabel('False positive rate')\n",
        "    plt.ylabel('True positive rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuPfqfgut9kz",
        "colab_type": "code",
        "outputId": "17f91d72-6131-49f4-e6aa-99cacdaa200b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "plot_roc([pred], names=['Dual input CNN'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqMAAAIkCAYAAAAqKiIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfyiIEXPei9itAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 800x640 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}